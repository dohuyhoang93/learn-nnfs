{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":""},{"location":"#welcome-to-the-digital-cityscape","title":"\ud83c\udf0c Welcome to the Digital Cityscape \ud83c\udf03","text":"Text Only<pre><code>       \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n       \u2551  NEURAL NETWORKS FROM SCRATCH      \u2551\n       \u2551  A Journey Built from Ground Zero  \u2551\n       \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n</code></pre> <p>Hey there! \ud83d\ude0a I'm just a curious coder diving into the fascinating world of artificial intelligence through Neural Networks from Scratch in Python. This website is my digital journal, capturing every step of my learning journey\u2014from the first lines of Python code to building complex neural networks. It's not a fancy project, just a humble corner of the internet where I share what I've learned, the mistakes I've made, and the excitement of discovering something new.</p>"},{"location":"#why-this-journey-matters","title":"\ud83d\udcbe Why This Journey Matters","text":"<p>In my own cyberpunk-inspired world, I see each chapter of the book as a node in a vast network of knowledge. Every Python file, every documentation folder, is a piece of a digital puzzle I'm piecing together. I'm no expert\u2014just someone passionate about learning\u2014but I believe that starting from scratch, one small step at a time, can lead to incredible discoveries.</p> <ul> <li>One chapter, one challenge: Each folder corresponds to a book chapter, with Python code and notes from basic to advanced.</li> <li>Learning by doing: I write, test, and document the code to truly understand how neural networks work.</li> <li>Open to collaboration: I\u2019d love to hear your thoughts, comments, or contributions to make this journey richer!</li> </ul>"},{"location":"#explore-the-neuron-city","title":"\ud83c\udf06 Explore the Neuron City","text":"<p>Imagine stepping into a futuristic city where buildings are neural layers, neon lights are weights, and data flows like endless streets. This website is your guide through that city:</p> <ul> <li>Book chapters: Each folder holds a chapter\u2019s Python code and detailed explanations.</li> <li>Personal notes: My thoughts, lessons learned, and even the bugs I\u2019ve wrestled with.</li> <li>Interactive space: I hope you\u2019ll leave a comment, suggestion, or even a snippet of code to help shape this project!</li> </ul> Text Only<pre><code>   \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n   \u2551  // STARTING POINT          \u2551\n   \u2551  &gt; Browse the folders       \u2551\n   \u2551  &gt; Run the Python code      \u2551\n   \u2551  &gt; Leave your mark!         \u2551\n   \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n</code></pre>"},{"location":"#join-me-on-this-journey","title":"\ud83d\udca1 Join Me on This Journey!","text":"<p>This project isn\u2019t perfect, but it\u2019s fueled by curiosity and passion. If you\u2019re excited about AI, curious about how neural networks tick, or just love coding, I invite you to:</p> <ul> <li>Dive into the chapters: Start with the first folder and build neural networks from the ground up with me.</li> <li>Share your thoughts: Drop a comment, question, or idea in the section below.</li> <li>Contribute code: Spot a better way to code or explain something? Submit a pull request on the repo!</li> </ul> <p>Thanks for stopping by this little corner of the digital universe. Let\u2019s light up the neon trails of knowledge together! \ud83d\ude80</p> <p>CONNECT &amp; CONTRIBUTE GitHub Email</p> <p>The journey begins with a single line of code. Are you ready?</p>"},{"location":"Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-foward-diagram/","title":"\u2514\u2500> First Forward Diagram","text":""},{"location":"Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-foward-diagram/#detailed-diagram-calculation-for-a-single-row-in-the-forward-pass","title":"Detailed Diagram: Calculation for a Single Row in the Forward Pass","text":"<p>This diagram \"zooms in\" on the calculation process to generate the first row of <code>dense1.output</code>.</p> Text Only<pre><code>======================================================================================================\n               DETAILED FORWARD PASS DIAGRAM - CALCULATION FOR A SINGLE INPUT SAMPLE (FIRST ROW)\n======================================================================================================\n\n    (INPUT - FIRST ROW OF X)                                (ENTIRE WEIGHT MATRIX W)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 [x1, y1]  \u2502                                      \u2502 [[w11, w12, w13],     \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                                      \u2502  [w21, w22, w23]]     \u2502\n          \u2502                                            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                      \u2502\n                                      \u25bc\n                                (1) DOT PRODUCT\n                      \"Multiply the row by each column\"\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u2502                                                       \u2502\n          \u25bc (To calculate the FIRST value of the output row)      \u25bc (To calculate the SECOND value of the output row)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 [x1, y1]  \u25cf  [w11, w21]  \u2502                          \u2502 [x1, y1]  \u25cf  [w12, w22]  \u2502\n\u2502 (Row 1 of X \u25cf Col 1 of W)  \u2502                          \u2502 (Row 1 of X \u25cf Col 2 of W)  \u2502\n\u2502                          \u2502                          \u2502                          \u2502\n\u2502   x1 * w11               \u2502                          \u2502   x1 * w12               \u2502\n\u2502      +                   \u2502                          \u2502      +                   \u2502\n\u2502   y1 * w21               \u2502                          \u2502   y1 * w22               \u2502\n\u2502      =                   \u2502                          \u2502      =                   \u2502\n\u2502     o11                  \u2502                          \u2502     o12                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502                                                       \u2502\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                           \u2502\n                                       \u2502                           \u2502\n                                       \u25bc                           \u25bc\n                                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                \u2502   dot_product (FIRST ROW)                       \u2502\n                                \u2502   [o11, o12, o13]                               \u2502  &lt;-- o13 is calculated similarly with col 3 of W\n                                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                       \u2502\n                                                       \u25bc\n                                         (2) ADD BIAS\n                                                       \u2502\n                                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                      \u2502                                 \u2502\n                                      \u25bc                                 \u25bc\n                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                          \u2502  [o11, o12, o13]  \u2502               \u2502    [b1, b2, b3]   \u2502\n                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502                                 \u2502\n                                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba    +    \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                    \u2502\n                                                    \u25bc\n                                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                        \u2502    output (FIRST ROW)     \u2502\n                                        \u2502 [o11+b1, o12+b2, o13+b3]   \u2502\n                                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n------------------------------------------------------------------------------------------------------\n* IMPORTANT: This process is repeated for ALL 100 rows of the input matrix X to create the 100 rows\n* of the output matrix `dense1.output`. NumPy performs all these calculations in parallel very\n* efficiently.\n------------------------------------------------------------------------------------------------------\n</code></pre>"},{"location":"Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/","title":"\u2514\u2500\u2500\u2500> First Layer","text":""},{"location":"Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#detailed-code-explanation","title":"Detailed Code Explanation","text":"<p>The <code>chapter01.py</code> program</p> <p>Overview: What is the goal of this program?</p> <p>Imagine we want to build a simple artificial \"brain\". This program performs the first and most fundamental step:</p> <ol> <li>Build a \"neuron\": Create a basic information processing unit.</li> <li>Prepare the data: Generate a sample dataset for the \"brain\" to process.</li> <li>Perform a calculation: Pass the data through the \"brain\" and see what the output is.</li> </ol> <p>This is the foundation of every neural network. Understanding each line of code here will help you grasp more complex concepts later on.</p>"},{"location":"Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#part-1-preparing-tools-and-materials-imports-data","title":"Part 1: Preparing Tools and Materials (Imports &amp; Data)","text":"<p>This is the step where we gather the necessary libraries and data before we start \"building\".</p> Python<pre><code>import numpy as np\nimport nnfs\nimport matplotlib.pyplot as plt\n\nfrom nnfs.datasets import spiral_data\nnnfs.init()\n</code></pre>"},{"location":"Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#detailed-line-by-line-explanation","title":"Detailed Line-by-Line Explanation:","text":"<ul> <li> <p><code>import numpy as np</code>:</p> <ul> <li>What is it?: <code>NumPy</code> (Numerical Python) is the most fundamental and important library for data science in Python. It provides an extremely efficient data structure called an array and tools to perform operations on these arrays, especially matrix math.</li> <li>Why do we need it?: A neural network is, in essence, a series of matrix operations. NumPy helps us perform these matrix multiplications and additions much more quickly and efficiently than using Python's standard lists. <code>as np</code> is a common convention for aliasing the library.</li> </ul> </li> <li> <p><code>import nnfs</code>:</p> <ul> <li>What is it?: <code>nnfs</code> (Neural Networks from Scratch) is a helper library written specifically for the book of the same name. Its purpose is to help learners focus on the concepts of neural networks rather than getting bogged down in minor details.</li> <li>Why do we need it?: It provides utility functions, such as creating sample data (<code>spiral_data</code>) and initializing the environment (<code>init</code>), to ensure that everyone's results are the same, making it easier to learn and debug.</li> </ul> </li> <li> <p><code>import matplotlib.pyplot as plt</code>:</p> <ul> <li>What is it?: <code>Matplotlib</code> is the most popular data visualization (plotting) library in Python. <code>pyplot</code> is a module within Matplotlib that provides an interface similar to MATLAB.</li> <li>Why do we need it?: \"A picture is worth a thousand words.\" This library allows us to plot the data on a graph to see what it looks like. Seeing the spiral data visually helps us better understand the problem the neural network is trying to solve.</li> </ul> </li> <li> <p><code>from nnfs.datasets import spiral_data</code>:</p> <ul> <li>What is it?: This is a specific <code>import</code> statement. Instead of importing the entire <code>nnfs.datasets</code> library, we only import the <code>spiral_data</code> function from it.</li> <li>Why do we need it?: <code>spiral_data</code> is a function that helps create the famous spiral dataset, a classic problem for testing classification models.</li> </ul> </li> <li> <p><code>nnfs.init()</code>:</p> <ul> <li>What is it?: This command calls the <code>init</code> function from the <code>nnfs</code> library.</li> <li>Why do we need it?: This function performs some background setup, most importantly setting the seed for NumPy's random number generation and establishing a default data type. This ensures that every time you run the code, the \"random weights\" and \"data\" generated will be exactly the same, making learning and reproducing results consistent.</li> </ul> </li> </ul>"},{"location":"Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#part-2-building-the-blueprint-for-a-judge-class-layer_dense","title":"Part 2: Building the \"Blueprint for a Judge\" (<code>class Layer_Dense</code>)","text":"<p>This is the heart of the program. We aren't building a single neuron, but a \"blueprint\" (<code>class</code>) so that we can easily create an entire layer/panel of judges.</p> Python<pre><code>class Layer_Dense:\n        def __init__(self, n_inputs, n_neurons):\n            self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n            self.biases = np.zeros((1, n_neurons))\n        def forward(self, inputs):\n            self.output = np.dot(inputs, self.weights) + self.biases\n</code></pre>"},{"location":"Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#detailed-section-by-section-explanation","title":"Detailed Section-by-Section Explanation:","text":"<ul> <li> <p><code>class Layer_Dense:</code>: Declares a \"blueprint\" named <code>Layer_Dense</code>. Everything inside it will define the properties and behaviors of a dense neural network layer.</p> </li> <li> <p><code>def __init__(self, n_inputs, n_neurons):</code>: The initializer (constructor).</p> <ul> <li>What does it do?: This function is automatically called every time a new object is created from this blueprint (e.g., <code>dense1 = Layer_Dense(...)</code>). It is used to set up initial properties.</li> <li><code>self</code>: Represents the object that will be created. When you call <code>dense1.weights</code>, <code>self</code> is <code>dense1</code>.</li> <li><code>n_inputs</code>: The number of input features this layer will receive (e.g., 2 features like \"Redness\" and \"Roundness\" of fruits).</li> <li><code>n_neurons</code>: The number of neurons in this layer (e.g., 3 judges, one for each type of fruit).</li> <li><code>self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)</code>: This is an extremely important line.<ul> <li><code>np.random.randn(n_inputs, n_neurons)</code>: Creates a matrix of size <code>(number_of_inputs, number_of_neurons)</code> filled with random numbers from a standard normal distribution (Gaussian distribution, with a mean of 0 and variance of 1). This represents the initial, completely random \"preferences\" of the judges.</li> <li><code>* 0.01</code>: Multiplies all the random weights by a very small number. This is a common technique to prevent the initial output values from being too large, which helps stabilize the training process later on.</li> </ul> </li> <li><code>self.biases = np.zeros((1, n_neurons))</code>:<ul> <li><code>np.zeros((1, n_neurons))</code>: Creates a row matrix (vector) of size <code>(1, number_of_neurons)</code> filled with all zeros. This represents the initial \"bias\" or \"mood\" of the judges. Initializing with zeros means that, initially, they have no predisposition.</li> </ul> </li> </ul> </li> <li> <p><code>def forward(self, inputs):</code>: The action method.</p> <ul> <li>What does it do?: Defines the main behavior of the layer: receiving input data and calculating an output. This process is called the forward pass.</li> <li><code>inputs</code>: The input data that will be fed into the layer (e.g., a list of features for all fruits).</li> <li><code>self.output = np.dot(inputs, self.weights) + self.biases</code>: The core mathematical formula.<ul> <li><code>np.dot(inputs, self.weights)</code>: Matrix multiplication. This is where each judge \"looks\" at the features of the fruits and multiplies them by their \"preferences\" (weights) to come up with a preliminary score.</li> <li><code>+ self.biases</code>: Adds the \"bias\" (prejudice) of each judge to their score.</li> <li><code>self.output = ...</code>: The final result is stored in the <code>output</code> attribute of the layer.</li> </ul> </li> </ul> </li> </ul>"},{"location":"Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#part-3-the-competition-begins-using-the-class-and-data","title":"Part 3: The Competition Begins! (Using the Class and Data)","text":"<p>Now we will use the \"blueprint\" and \"materials\" prepared above to conduct a real competition.</p> Python<pre><code># Create dataset\nX, y = spiral_data(samples=100, classes=3)\n# Visualize dataset\nplt.scatter(X[:,0], X[:,1], c=y, cmap='brg')\nplt.show()\n</code></pre> <ul> <li><code>X, y = spiral_data(samples=100, classes=3)</code>: Calls the imported function to create data.<ul> <li><code>X</code>: Will be a NumPy array of size <code>(300, 2)</code>. It's 300 because there are 3 <code>classes</code>, each with 100 <code>samples</code>. It's 2 because each sample has 2 features (x, y coordinates). This is the \"list of fruit contestants and their characteristics.\"</li> <li><code>y</code>: Will be a NumPy array of size <code>(300,)</code> containing the labels <code>0, 1, 2</code>. This is the \"correct answer\" for each contestant (Apple, Orange, or Banana).</li> </ul> </li> <li><code>plt.scatter(X[:,0], X[:,1], c=y, cmap='brg')</code>: Prepares to plot the graph.<ul> <li><code>X[:,0]</code>: Gets all rows, first column (all x-coordinates).</li> <li><code>X[:,1]</code>: Gets all rows, second column (all y-coordinates).</li> <li><code>c=y</code>: <code>c</code> is short for color. This command tells Matplotlib to color each point <code>(x, y)</code> based on the corresponding value in the <code>y</code> array. Points with <code>y=0</code> will have one color, <code>y=1</code> another, and so on.</li> <li><code>cmap='brg'</code>: Color map. Selects the Blue-Red-Green color palette.</li> </ul> </li> <li><code>plt.show()</code>: Displays the prepared plot on the screen.</li> </ul> Python<pre><code># Create Dense layer with 2 input features and 3 output values\ndense1 = Layer_Dense(2, 3)\n</code></pre> <ul> <li>This is where we create an object from the <code>Layer_Dense</code> blueprint. We are \"hiring a panel of judges.\"</li> <li><code>dense1 = ...</code>: Creates a specific panel of judges named <code>dense1</code>.</li> <li><code>Layer_Dense(2, 3)</code>: Calls the <code>__init__</code> function.<ul> <li><code>n_inputs=2</code>: Because each \"fruit contestant\" (<code>X</code>) has 2 features (x, y coordinates).</li> <li><code>n_neurons=3</code>: Because we need to classify into 3 types of fruit (3 classes in <code>y</code>). We need 3 judges, each specializing in one type.</li> </ul> </li> </ul> Python<pre><code># Let's see initial weights and biases\nprint(\"&gt;&gt;&gt; Initial weights and biases of the first layer:\")\nprint(dense1.weights)\nprint(dense1.biases)\n</code></pre> <ul> <li>Prints the <code>weights</code> and <code>biases</code> attributes of the newly created <code>dense1</code> object. This shows us the initial, completely random \"preferences\" and \"biases\" of the judges before they've scored any contestants.</li> </ul> Python<pre><code># Perform a forward pass of our training data through this layer\ndense1.forward(X)\n</code></pre> <ul> <li>This is the moment of action. We call the <code>forward</code> method of <code>dense1</code> and pass the entire \"list of contestants\" (<code>X</code>) into it. The calculation <code>np.dot(X, dense1.weights) + dense1.biases</code> is executed. The judges begin scoring.</li> </ul> Python<pre><code># Let's see output of the first few samples:\nprint(\"&gt;&gt;&gt; Output of the first few samples:\")\nprint(dense1.output[:5])\n</code></pre> <ul> <li>After <code>forward()</code> finishes running, the result is stored in <code>dense1.output</code>.</li> <li><code>dense1.output[:5]</code>: We print the scoring results for the first 5 \"fruit contestants\" to see a sample. Each row is a contestant, each column is a score from a judge. These values are called logits.</li> </ul>"},{"location":"Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#part-4-abstract-interpretation-the-fruit-classification-competition","title":"Part 4: Abstract Interpretation - The Fruit Classification Competition","text":"<p>Let's retell the whole story seamlessly:</p> <ol> <li> <p>The Scene: We are organizing a competition to classify 3 types of fruit: Apples, Oranges, and Bananas.</p> </li> <li> <p>The Contestants (<code>X</code>, <code>y</code>): 300 fruits are participating. For each fruit, we use a machine to measure 2 characteristics: \"Redness\" and \"Roundness\" (these are the 2 columns of <code>X</code>). We also know the correct answer for each fruit (this is <code>y</code>).</p> </li> <li> <p>Hiring the Judges (<code>dense1 = Layer_Dense(2, 3)</code>): We hire a panel of 3 judges:</p> <ul> <li>Judge 1: An expert on Apples.</li> <li>Judge 2: An expert on Oranges.</li> <li>Judge 3: An expert on Bananas. They are new to the job, so their \"knowledge\" is initially random.</li> </ul> </li> <li> <p>The Judges' Knowledge (<code>weights</code> and <code>biases</code>):</p> <ul> <li>Preferences (<code>weights</code>): Each judge has their own set of \"preferences\" for the 2 characteristics \"Redness\" and \"Roundness\". For example, an ideal Apple expert would have a high preference for \"Redness\" and \"Roundness\". A Banana expert would have a negative preference for \"Roundness\" (since bananas are long). But because they are new, these preferences are assigned randomly (e.g., the Apple expert might prefer non-red fruits, the Banana expert might prefer round ones).</li> <li>Mood (<code>biases</code>): Initially, all 3 judges have a neutral mood (equal to 0).</li> </ul> </li> <li> <p>The Scoring Process (<code>dense1.forward(X)</code>):</p> <ul> <li>One by one, each fruit is presented to the panel.</li> <li>Each judge calculates their score using the formula:     <code>Score = (Redness * Preference for Redness) + (Roundness * Preference for Roundness) + Mood</code></li> <li>This process happens for all 300 fruits.</li> </ul> </li> <li> <p>The Scoreboard (<code>dense1.output</code>):</p> <ul> <li>The final result is a large scoreboard. Each row is a fruit, each column is a score from a judge.</li> <li>For example, the first row might be <code>[0.0012, -0.0045, 0.0031]</code>. This means that with their current random knowledge, the Apple Judge gives this fruit 0.0012 points, the Orange Judge gives it -0.0045 points, and the Banana Judge gives it 0.0031 points.</li> </ul> </li> </ol> <p>The Key Takeaway: Because the judges' \"knowledge\" (weights) is random, this \"scoreboard\" (output) is completely meaningless. The process of \"training\", which is not in this code, is the act of showing the judges the correct answers (<code>y</code>), pointing out their mistakes, and helping them adjust their \"preferences\" (<code>weights</code>) and \"moods\" (<code>biases</code>) over thousands of iterations, so that eventually their scoreboard accurately reflects the type of fruit.</p>"},{"location":"Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#part-5-ascii-illustration","title":"Part 5: ASCII Illustration","text":"<p>Diagram for a single fruit passing through the panel of judges:  Text Only<pre><code>           INPUT (1 fruit)\n          (2 features)\n        +----------------------+\n        | Redness, Roundness   |\n        +----------------------+\n               |\n               |                               JUDGING PANEL (dense1)\n               |                                (3 Judges/Neurons)\n               |\n               |       Preferences (w11, w21)  +--------------------+   (Score from Apple Judge)\n               +-----------------------------&gt;|    APPLE Judge   + b1|-----&gt; output_1\n               |                             +--------------------+\n               |\n               |       Preferences (w12, w22)  +--------------------+   (Score from Orange Judge)\n               +-----------------------------&gt;|   ORANGE Judge   + b2|-----&gt; output_2\n               |                             +--------------------+\n               |\n               |       Preferences (w13, w23)  +--------------------+   (Score from Banana Judge)\n               +-----------------------------&gt;|   BANANA Judge   + b3|-----&gt; output_3\n                                             +--------------------+\n\n\nScoring formula for the APPLE Judge:\noutput_1 = (Redness * w11) + (Roundness * w21) + b1\n\nThe final result for 1 fruit is a set of 3 scores: [output_1, output_2, output_3]\n</code></pre></p>"},{"location":"Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#appendix-explaining-spiral-data","title":"Appendix - Explaining Spiral Data","text":"<p>This is an explanation of \"Spiral Data\" - it might sound abstract, but it's one of the most classic and important sample datasets when you start learning about neural networks.</p> <p>Let's break it down.</p>"},{"location":"Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#1-simple-definition","title":"1. Simple Definition","text":"<p>Spiral Data is a synthetically generated dataset where data points belonging to different classes are arranged in interlocking spirals.</p> <p>Take another look at the very plot you generated:</p> <ul> <li>You have 3 classes, corresponding to 3 colors: Red, Green, and Blue.</li> <li>Each point has a position (x, y coordinates).</li> <li>Points of the same color form a spiral \"arm\".</li> <li>These arms are intertwined, wrapping around each other.</li> </ul>"},{"location":"Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#2-why-is-it-so-important-and-famous","title":"2. Why is it so important and famous?","text":"<p>The reason this dataset is used so widely is because it's a perfect challenge to demonstrate the power of neural networks.</p>"},{"location":"Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#a-it-defeats-simple-linear-models","title":"A. It \"defeats\" simple (Linear) models","text":"<p>Imagine you only have a ruler. Your task is to draw one or more straight lines to separate these 3 color groups, such that each region contains only one color.</p> <p>You will quickly find that this is impossible.</p> Text Only<pre><code>       /\n      /    &lt;-- You cannot draw any single straight line\n     /         to separate Red (R) from Green (G) and Blue (B)\n    /\n   RRRRR\n  G B R G\n B G R B G\nB B G G B B\n R R B R R\n  R G B R\n   BBBBB\n</code></pre> <p>A model that can only draw straight lines for classification is called a linear model. Spiral data is a classic example of non-linear data, where the boundary between classes is not a straight line but a complex curve.</p> <p>In other words, spiral data is intentionally designed to be difficult for simple classification algorithms.</p>"},{"location":"Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#b-it-demonstrates-the-necessity-of-neural-networks","title":"B. It demonstrates the necessity of Neural Networks","text":"<p>Neural networks, especially those with hidden layers and non-linear activation functions (which we will learn about later), are capable of learning extremely complex and winding decision boundaries.</p> <p>A well-trained neural network can create a boundary that looks something like this:</p> <p>It doesn't use a \"ruler\"; it learns to \"draw\" smooth curves to perfectly enclose each group of data.</p> <p>Conclusion: Spiral data is a \"graduation\" test for a classification model. If your model can solve this problem, it proves that it is capable of handling complex, non-linear relationships in data, something that simple models cannot do.</p>"},{"location":"Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#3-what-does-the-spiral_data-function-create","title":"3. What does the <code>spiral_data</code> function create?","text":"<p>When you call <code>X, y = spiral_data(samples=100, classes=3)</code>, the function calculates and returns two things:</p> <ol> <li> <p><code>X</code> (The Features):</p> <ul> <li>This is a NumPy array containing the <code>[x, y]</code> coordinates of all the points.</li> <li>With <code>samples=100</code> and <code>classes=3</code>, it will create <code>100 * 3 = 300</code> points.</li> <li>Therefore, <code>X</code> will have the shape <code>(300, 2)</code>.</li> <li>In our \"fruit judges\" story, <code>X</code> is equivalent to a list of 300 fruits, each with 2 features: \"Redness\" and \"Roundness\".</li> </ul> </li> <li> <p><code>y</code> (The Labels):</p> <ul> <li>This is a NumPy array containing the class label for each corresponding point in <code>X</code>.</li> <li>It will contain 300 numbers, consisting of 100 <code>0</code>s, 100 <code>1</code>s, and 100 <code>2</code>s.</li> <li><code>y[i]</code> is the label (the correct answer) for the point <code>X[i]</code>.</li> <li>In our story, <code>y</code> is the list of correct answers: which fruit is an \"Apple\" (class 0), which is an \"Orange\" (class 1), and which is a \"Banana\" (class 2).</li> </ul> </li> </ol> <p>So, <code>spiral_data</code> isn't just a dataset; it's a classic non-linear classification problem packaged and ready for you to quickly test your models.</p>"},{"location":"Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu-diagram/","title":"ReLU Diagram","text":""},{"location":"Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu-diagram/#consolidated-ascii-diagram-from-data-point-to-relu-activation","title":"Consolidated ASCII Diagram: From Data Point to ReLU Activation","text":"<p>This diagram tracks a single data point <code>O(0.5, 1.0)</code> through the entire process: Dense Layer 1 -&gt; ReLU Activation 1.</p> Text Only<pre><code>\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551             COMPLETE FLOW DIAGRAM: LINEAR TRANSFORMATION &amp; NON-LINEAR (RELU) ACTIVATION                      \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\n    (STEP 1: INPUT)\n    A single data point in 2D space\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502     Input: O(x, y)      \u2502\n    \u2502      [0.5, 1.0]         \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n                \u25bc\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551   (STEP 2: LINEAR TRANSFORMATION - DENSE LAYER 1) - `output = v \u00b7 W + b`                                     \u2551\n\u2551   Goal: Project the point O from 2D space into a new 3D space.                                               \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                                                                                    \u2502\n    \u25bc (2a: Dot Product Operation)                                                        \u25bc (Layer's Parameters)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 [0.5, 1.0]\u2502         \u2502      Weights: W         \u2502                            \u2502       Biases: b         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518         \u2502     (2x3 Matrix)        \u2502                            \u2502     (1x3 Vector)        \u2502\n      \u2502               \u2502 [[ 0.2, 0.8,-0.5],     \u2502                            \u2502     [[2.0, 3.0, 0.5]]   \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502  [-0.9, 0.2, 0.4]]      \u2502                            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                          \u2502\n                  \u25bc                                                                        \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502          DETAILED CALCULATION FOR EACH NEURON (Each neuron creates a new dimension)    \u2502 \u2502\n\u2502                                                                                        \u2502 \u2502\n\u2502  Neuron 0: (0.5 * 0.2) + (1.0 * -0.9) = -0.8  &lt;\u2500\u2500\u2500\u2510                                    \u2502 \u2502\n\u2502  Neuron 1: (0.5 * 0.8) + (1.0 *  0.2) =  0.6  &lt;\u2500\u2500\u2500\u253c\u2500\u2500\u2510                                 \u2502 \u2502\n\u2502  Neuron 2: (0.5 *-0.5) + (1.0 *  0.4) =  0.15 &lt;\u2500\u2500\u2500\u253c\u2500\u2500\u253c\u2500\u2500\u2510                              \u2502 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502  \u2502  \u2502                              \u2502\n                                                    \u2502  \u2502  \u2502                              \u2502\n    (Result of v \u00b7 W)                               \u25bc  \u25bc  \u25bc                              \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                          \u2502\n    \u2502  [-0.8,  0.6,   0.15]    \u2502               \u2502  (2b: Add Bias) \u2502                          \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba   +   \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                    \u2502\n                                                    \u25bc\n                                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                          \u2502     dense1.output       \u2502 (Coordinates in the new space)\n                                          \u2502        (Logits)         \u2502\n                                          \u2502 [1.2,  3.6,  0.65]      \u2502\n                                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                      \u2502\n                                                      \u25bc\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551   (STEP 3: NON-LINEAR TRANSFORMATION - RELU ACTIVATION) - `output = max(0, x)`                             \u2551\n\u2551   Goal: \"Bend\" the space, zeroing out negative values to enable non-linear learning.                       \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                                                                                    \u2502\n    \u25bc (Element-wise operation)                                                           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     dense1.output       \u2502                                                    \u2502 activation1.output        \u2502\n\u2502   [1.2, 3.6, 0.65]      \u2502                                                    \u2502      (Final Output)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n            \u2502                                                                  \u2502   [1.2, 3.6, 0.65]        \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           max(0, 1.2)  --&gt;  1.2                 \u2502\n\u2502           max(0, 3.6)  --&gt;  3.6                 \u2502\n\u2502           max(0, 0.65) --&gt;  0.65                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n* NOTE: If the input to ReLU were [-0.8, 0.6, 0.15], the output would be [0.0, 0.6, 0.15].\n* This transformation to 0 is the \"bending\" action on the space.\n\n==============================================================================================================\nFLOW SUMMARY:\nInput(2D) \u2500\u2500(Linear Transformation)\u2500\u2500&gt; Logits(3D) \u2500\u2500(\"Bending\" with ReLU)\u2500\u2500&gt; Hidden Layer Output(3D)\n==============================================================================================================\n</code></pre>"},{"location":"Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/","title":"Transformation & ReLU activation function","text":""},{"location":"Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#part-1-the-relu-activation-function","title":"Part 1: The ReLU Activation Function","text":"<p>This is one of the best illustrations of the power of the ReLU activation function, a fundamental concept in modern neural networks.</p> <p>The main goal of this chapter is to answer the question: \"How can a simple function like ReLU, which is just a straight line 'broken' at point 0, help a neural network learn incredibly complex non-linear relationships (like a sine wave)?\"</p> <p>Let's dissect it step by step.</p>"},{"location":"Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#the-big-idea","title":"The Big Idea","text":"<p>Imagine you only have straight LEGO bricks. How would you use them to build a curved wall?</p> <p>The answer is: You can't create a smooth curve, but you can approximate it by piecing together many short, straight bricks. The more short bricks you use, the more curved and smooth your wall will look.</p> <p>In a neural network:</p> <ul> <li>The ReLU function is our straight brick.</li> <li>Each neuron (or pair of neurons) is a builder, tasked with placing one brick.</li> <li>The neural network is the entire construction, assembling these bricks to create the final complex shape (like a sine curve).</li> </ul> <p>Now let's dive into the details of how the neuron \"builder\" works.</p>"},{"location":"Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#the-basic-building-block-a-single-relu-neuron","title":"The Basic Building Block - A Single ReLU Neuron","text":"<p>A neuron receives an input, multiplies it by a weight, adds a bias, and then passes it through the ReLU activation function.</p> <p>Formula: <code>output = ReLU(weight * input + bias)</code> ReLU function: <code>ReLU(x) = max(0, x)</code>. This means: *   If <code>x &lt;= 0</code>, the result is <code>0</code>. *   If <code>x &gt; 0</code>, the result is <code>x</code>.</p> <p>It's like a gate: *   Calculated input is negative -&gt; Gate closes -&gt; Output = 0. *   Calculated input is positive -&gt; Gate opens -&gt; Output = the calculated value.</p>"},{"location":"Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#the-role-of-weight-and-bias","title":"The Role of Weight and Bias:","text":"<ol> <li> <p>Weight: Changes the slope.</p> <ul> <li>A large weight -&gt; The line slopes up faster.</li> <li>A small weight -&gt; The line slopes up more gently.</li> <li>A negative weight -&gt; The line is flipped, sloping down.</li> </ul> </li> <li> <p>Bias: Shifts the \"break\" point.</p> <ul> <li>The bias determines at which input value the neuron starts to \"activate\" (i.e., produce an output &gt; 0). It shifts the graph left or right.</li> </ul> </li> </ol> <p>ASCII Illustration:</p> <p>Let's look at the graph of a ReLU neuron. The horizontal axis is <code>input</code>, the vertical axis is <code>output</code>.</p> Text Only<pre><code>        Basic (w=1, b=0)          Increase Weight (w=2)           Add Bias (w=1, b=-1)\n           /                          //                              /\n          /                          / /                             /\n         /                          / /                             /\n        +------- (input)          +------- (input)                +------- (input)\n                                                                 |\n                                                               (break point shifted right)\n</code></pre> <p>In summary: With one neuron, we can create a \"ramp\" that starts at an arbitrary point and has an arbitrary slope. But it's still just a straight line.</p>"},{"location":"Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#the-magic-begins-combining-two-neurons","title":"The Magic Begins - Combining Two Neurons","text":"<p>This is the most crucial and magical part. The book illustrates how by using a pair of neurons (one in the first hidden layer, one in the second), we can create something called an \"area of effect.\"</p> <p>Imagine this:</p> <ol> <li> <p>Neuron 1 (The \"Activator\" Neuron): Creates an upward-sloping line.</p> <ul> <li>Example: <code>y1 = ReLU(1.0 * x - 0.2)</code></li> <li>It will produce an upward slope, starting from <code>x = 0.2</code>.</li> </ul> </li> <li> <p>Neuron 2 (The \"Deactivator\" Neuron): Creates a downward-sloping line.</p> <ul> <li>Example: <code>y2 = ReLU(-1.0 * x + 0.8)</code></li> <li>It will produce a downward slope, starting from <code>x = 0.8</code>.</li> </ul> </li> </ol> <p>When you add the results of these two neurons together (which is what happens in the next layer), something interesting occurs:</p> <ul> <li>When x &lt; 0.2: Both neurons output 0. The sum is 0.</li> <li>When 0.2 &lt; x &lt; 0.8: Neuron 1 is active (sloping up), Neuron 2 is still 0. The sum is an upward-sloping line.</li> <li>When x &gt; 0.8: Both neurons are active. The upward slope of Neuron 1 is cancelled out by the downward slope of Neuron 2. The sum is a horizontal line.</li> </ul> <p>The result is a \"tent\" or \"hat\" shape!</p> <p>ASCII Illustration:</p> Text Only<pre><code>   Output of Neuron 1         +    Output of Neuron 2         =       Final Result\n  (slope up from x=0.2)            (slope down from x=0.8)             (a tent shape)\n            /                                 |                               /\\\n           /                                  |                              /  \\\n          /                                   |                             /    \\\n---------+---------- (input)    --------------+---------- (input)   ---------+----+------ (input)\n       0.2                                  0.8                             0.2  0.8\n                                              \\\n                                               \\\n</code></pre> <p>This is our triangular LEGO brick! By adjusting the weights and biases of this pair of neurons, we can control: *   The position of the tent (shift left/right). *   The height of the tent. *   The slope of the tent's sides.</p>"},{"location":"Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#building-a-sine-wave-assembling-many-bricks","title":"Building a Sine Wave - Assembling Many \"Bricks\"","text":"<p>Now that you have these \"tent-shaped bricks,\" approximating a sine curve becomes simple:</p> <p>You just use many pairs of neurons, with each pair creating one \"tent\" to simulate a segment of the sine wave.</p> <ul> <li>Neuron pair 1: Creates the first upward slope of the sine wave.</li> <li>Neuron pair 2: Creates the next downward slope.</li> <li>Neuron pair 3: Creates the upward slope of the negative part.</li> <li>... and so on.</li> </ul> <p>The \"hand-tuning\" process in the book (from Fig 4.20 to 4.33) is just to demonstrate to you: *   \"Ah, if I adjust this <code>weight</code>, this slope gets steeper.\" *   \"If I adjust that <code>bias</code>, this tent shifts to the right.\" *   \"If I use a negative <code>weight</code> on the output, the tent gets flipped upside down (creating the trough of the sine wave).\"</p> <p>Abstract ASCII Illustration:</p> Text Only<pre><code>   Target Sine Wave:\n       __/ \\__\n      /     \\\n     /       \\\n            / \\\n    _______/   \\______\n\n   Neural Network approximates by adding \"tents\":\n\n     Tent 1      Tent 2 (inverted)      Tent 3...\n       /\\               _                /\\\n      /  \\             / \\              /  \\\n     /    \\           /   \\            /    \\\n    -------  +      \\/     +  ...   =   Result approximates a sine wave\n</code></pre>"},{"location":"Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#summary","title":"Summary","text":"<ol> <li> <p>The Power of ReLU: ReLU itself is very simple, but when combined in a multi-layer network, its units are capable of creating incredibly complex piecewise linear functions. These functions can approximate any continuous function (this is the idea behind the Universal Approximation Theorem).</p> </li> <li> <p>Why Do We Need Hidden Layers? The first hidden layer creates the \"ramps.\" The second hidden layer combines those \"ramps\" to form \"tents.\" Subsequent layers can combine these \"tents\" into even more complex shapes.</p> </li> <li> <p>From \"Hand-Tuning\" to \"Self-Learning\": Manually tuning the parameters in the book is tedious and for illustrative purposes only. In reality, the process of training a neural network is the process of the computer automatically finding the best <code>weight</code> and <code>bias</code> values for all neurons, so that the network's output matches the target data as closely as possible. An optimizer like Gradient Descent is the \"chief architect\" that does this job.</p> </li> <li> <p>More Neurons = Better: By increasing the number of neurons to, for example, 64, the network has more \"bricks\" to build with. The result is an approximated curve that looks much smoother and more accurate.</p> </li> </ol> <p>ReLU, despite its simplicity, is the foundation for the extraordinary power of modern neural networks.</p>"},{"location":"Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#part-2-a-deep-dive-into-the-transformation-of-a-single-data-point","title":"Part 2: A Deep Dive into the Transformation of a Single Data Point.","text":"<p>Let's assume we have:</p> <ul> <li>A single input data point <code>O</code> with coordinates <code>(0.5, 1.0)</code>.</li> <li>A <code>Layer_Dense</code> layer with 2 inputs and 3 neurons.</li> </ul>"},{"location":"Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#1-initialization","title":"1. Initialization","text":"<p>The <code>dense1</code> layer is initialized with <code>weights</code> and <code>biases</code>. Let's assume that after random initialization, we have these specific values:</p>"},{"location":"Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#weight-matrix-w-selfweights-size-2-3","title":"Weight Matrix <code>W</code> (self.weights) - Size (2, 3)","text":"<ul> <li>Row 0: Weights for the first input (<code>i1 = 0.5</code>).</li> <li>Row 1: Weights for the second input (<code>i2 = 1.0</code>).</li> <li>Cols 0, 1, 2: Correspond to Neuron 0, 1, and 2.</li> </ul> Text Only<pre><code>          Neuron 0   Neuron 1   Neuron 2\n         +----------+----------+----------+\nInput 0  |   0.2    |   0.8    |  -0.5    |\n(i1=0.5) +----------+----------+----------+\nInput 1  |  -0.9    |   0.2    |   0.4    |\n(i2=1.0) +----------+----------+----------+\n</code></pre>"},{"location":"Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#bias-vector-b-selfbiases-size-1-3","title":"Bias Vector <code>b</code> (self.biases) - Size (1, 3)","text":"<ul> <li>Each value corresponds to the bias of one neuron.</li> </ul> Text Only<pre><code>         +----------+----------+----------+\n         |   2.0    |   3.0    |   0.5    |\n         +----------+----------+----------+\n           Neuron 0   Neuron 1   Neuron 2\n</code></pre>"},{"location":"Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#2-the-transformation-process-dense1forwardo","title":"2. The Transformation Process - <code>dense1.forward(O)</code>","text":"<p>We perform the operation: <code>v' = v \u00b7 W + b</code></p>"},{"location":"Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#step-21-dot-product-v-w","title":"Step 2.1: Dot Product <code>v \u00b7 W</code>","text":"<ul> <li><code>v</code> is the input vector: <code>[0.5, 1.0]</code> (Size 1x2)</li> <li><code>W</code> is the weight matrix (Size 2x3)</li> <li>The result will be a vector of size 1x3.</li> </ul> Text Only<pre><code>                                       +-------+-------+-------+\n                                       |  0.2  |  0.8  | -0.5  |\n                                       | -0.9  |  0.2  |  0.4  |\n                                       +-------+-------+-------+\n                                                 ^\n                                                 |\n                                                 \u00b7 (Dot Product)\n+-------+-------+\n|  0.5  |  1.0  |\n+-------+-------+\n      |\n      +-------------------------------------------------------------+\n      |                                                             |\n      v                                                             v\n    Calculation for Neuron 0:                                     Calculation for Neuron 1:\n    (0.5 * 0.2) + (1.0 * -0.9)                                    (0.5 * 0.8) + (1.0 * 0.2)\n    = 0.1 - 0.9                                                   = 0.4 + 0.2\n    = -0.8                                                        = 0.6\n\n                                                                     Calculation for Neuron 2:\n                                                                     (0.5 * -0.5) + (1.0 * 0.4)\n                                                                     = -0.25 + 0.4\n                                                                     = 0.15\n</code></pre> <p>The result of the dot product is the vector <code>[-0.8, 0.6, 0.15]</code>.</p>"},{"location":"Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#step-22-add-bias-vector-b","title":"Step 2.2: Add Bias Vector <code>+ b</code>","text":"<p>Now, we take the result from above and add the bias vector.</p> Text Only<pre><code>      Result from v \u00b7 W                 Bias Vector b                  Output Vector v'\n+--------+-------+--------+     +     +-------+-------+-------+     =     +-------+-------+-------+\n|  -0.8  |  0.6  |  0.15  |           |  2.0  |  3.0  |  0.5  |           |  1.2  |  3.6  |  0.65 |\n+--------+-------+--------+           +-------+-------+-------+           +-------+-------+-------+\n     |        |        |                 |        |        |                 |        |        |\n     |        |        +-----------------|--------|--------|-----------------+        |\n     |        +--------------------------|--------|--------+--------------------------+\n     +-----------------------------------|--------+-----------------------------------+\n\n     -0.8 + 2.0 = 1.2\n           0.6 + 3.0 = 3.6\n                 0.15 + 0.5 = 0.65\n</code></pre> <p>Result: The vector <code>v'</code> (the output of <code>dense1</code>) is <code>[1.2, 3.6, 0.65]</code>. This is the coordinate of point <code>O</code> in the new 3-dimensional space after the linear transformation.</p>"},{"location":"Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#3-relu-activation-activation1forwardv","title":"3. ReLU Activation - <code>activation1.forward(v')</code>","text":"<p>Now, we pass the vector <code>v'</code> through the ReLU function. This function operates element-wise.</p> Text Only<pre><code>     Input vector v' to ReLU             Action of ReLU              Final vector v''\n+-------+-------+-------+        max(0, x)       +-------+-------+-------+\n|  1.2  |  3.6  |  0.65 |  ----------------&gt;     |  1.2  |  3.6  |  0.65 |\n+-------+-------+-------+                        +-------+-------+-------+\n     |        |        |\n     |        |        +-----&gt; max(0, 0.65) = 0.65\n     |        +--------------&gt; max(0, 3.6)  = 3.6\n     +-----------------------&gt; max(0, 1.2)  = 1.2\n</code></pre> <p>In this example, because all components of <code>v'</code> are positive, the output of ReLU <code>v''</code> is identical to <code>v'</code>.</p> <p>If <code>v'</code> were <code>[-0.8, 0.6, 0.15]</code> (before adding the bias), the result would be different:</p> Text Only<pre><code>     Input vector v' to ReLU             Action of ReLU              Final vector v''\n+--------+-------+--------+        max(0, x)       +-------+-------+--------+\n|  -0.8  |  0.6  |  0.15  |  ----------------&gt;     |  0.0  |  0.6  |  0.15  |\n+--------+-------+--------+                        +-------+-------+--------+\n     |        |        |\n     |        |        +-----&gt; max(0, 0.15) = 0.15\n     |        +--------------&gt; max(0, 0.6)  = 0.6\n     +-----------------------&gt; max(0, -0.8) = 0.0\n</code></pre> <p>This diagram describes the entire mathematical process from an input vector <code>v</code> to the final vector <code>v''</code> after passing through a dense layer and a ReLU activation layer.</p>"},{"location":"Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#abstraction","title":"Abstraction","text":"<p>How does each neuron contribute to creating the final \"signature\"?</p>"},{"location":"Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#analysis","title":"Analysis","text":"<ol> <li> <p>\"Neuron 1 contributes a part\" to creating the final signature.     This is like a musician in an orchestra. The violinist doesn't \"carry\" the symphony; they just play their violin part. The symphony (the signature) is the combination of all the musicians.     A more accurate phrasing: \"Neuron 1 has its own set of criteria (its weights and bias).\"</p> </li> <li> <p>\"transforms O(x,y) --&gt; O'(x,y,z)\": The entire layer (composed of all 3 neurons) collectively performs this transformation.</p> </li> </ol>"},{"location":"Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#interpretation","title":"Interpretation","text":"<ol> <li> <p>Each Neuron is a \"Feature Detector\":</p> <ul> <li>Neuron 1 is equipped with a set of criteria <code>(w1, b1)</code>. It measures how well the point <code>O(x,y)</code> fits these criteria and outputs a score <code>x'</code>.</li> <li>Neuron 2 is equipped with a set of criteria <code>(w2, b2)</code>. It measures how well the point <code>O(x,y)</code> fits these criteria and outputs a score <code>y'</code>.</li> <li>Neuron 3 is equipped with a set of criteria <code>(w3, b3)</code>. It measures how well the point <code>O(x,y)</code> fits these criteria and outputs a score <code>z'</code>.</li> </ul> </li> <li> <p>Creating the \"Signature\":</p> <ul> <li>The \"signature\" of point <code>O</code> is not created by a single neuron. The \"signature\" is the resulting vector <code>O'(x', y', z')</code>. It is the collection of scores that all the \"detectors\" have produced.</li> </ul> </li> <li> <p>The Goal of Training:</p> <ul> <li>The training process will adjust the criteria <code>(w, b)</code> of each neuron such that:<ul> <li>All points <code>O</code> belonging to the \"Blue\" class, when passed through these 3 \"detectors,\" will produce <code>O'</code> vectors (signatures) that lie close to each other in one region of space.</li> <li>All points <code>O</code> belonging to the \"Red\" class will produce signatures that lie close to each other in a different region of space.</li> <li>And similarly for the \"Green\" class.</li> </ul> </li> </ul> </li> </ol> <p>ASCII Diagram Reflecting This Idea</p> Text Only<pre><code>  Input Point O(x,y)\n          |\n          |\n+---------+---------+\n|                   |\nv                   v\nDetector 1          Detector 2          Detector 3\n(Criteria w1, b1)   (Criteria w2, b2)   (Criteria w3, b3)\n|                   |                   |\nv                   v                   v\nScore x'            Score y'            Score z'\n|                   |                   |\n+---------+---------+-------------------+\n          |\n          v\n\"Signature\" = O'(x', y', z')\n(Resulting vector in a new space)\n</code></pre> <p>Example: After training, it might be the case that:</p> <ul> <li>Criterion 1 (of Neuron 1) becomes \"detect upward curves.\"</li> <li>Criterion 2 (of Neuron 2) becomes \"detect proximity to the origin.\"</li> <li>A point <code>O</code> from the \"Blue\" class might be both curving up and close to the origin. Its signature would be <code>O'(HIGH, HIGH, ...)</code>.</li> <li>A point <code>O</code> from the \"Red\" class might be curving up but far from the origin. Its signature would be <code>O'(HIGH, LOW, ...)</code>.</li> </ul> <p>Conclusion: Each neuron has a specific role. That role is to \"measure a feature.\" The final \"signature\" of a data point is the combination of results from all of those feature measurements.</p>"},{"location":"Explain%20Documents/Ch4pt3r.03_Softmax/03-softmax-diagram/","title":"Softmax Diagram","text":""},{"location":"Explain%20Documents/Ch4pt3r.03_Softmax/03-softmax-diagram/#detailed-diagram-from-hidden-layer-to-probability-distribution-for-a-single-sample","title":"Detailed Diagram: From Hidden Layer to Probability Distribution (for a single sample)","text":"Text Only<pre><code>\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551             DETAILED STEP-BY-STEP DIAGRAM: OUTPUT LAYER (DENSE 2) &amp; SOFTMAX FOR A SINGLE SAMPLE                          \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\n    (STEP 3: INPUT TO THE OUTPUT LAYER)\n    Output of Hidden Layer 1 for a single sample\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 act1.output: [0.8, 0.2, 0.0] \u2502 (Vector 1xH, H=3)\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n                \u25bc\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551                                        (STEP 4a: LINEAR TRANSFORMATION - DENSE LAYER 2)                                  \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                                                                                    \u2502\n    \u25bc (Dot Product: v' \u00b7 W2)                                                             \u25bc (Parameters of the Output Layer)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502[0.8,0.2,0.0]\u2502     \u2502     Weights W2 (HxC)    \u2502                                          \u2502      Biases b2 (1xC)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518     \u2502 [[ 0.1,  0.4, -0.3],    \u2502 (C=3)                                    \u2502      [[2.0, 1.5, 0.9]]  \u2502\n      \u2502           \u2502  [ 0.6, -0.9,  0.2],    \u2502                                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502  [-0.2,  0.5,  0.7]]    \u2502                                                        \u2502\n              \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                        \u2502\n              \u25bc                                                                                    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502          DETAILED CALCULATION FOR EACH OUTPUT NEURON                                             \u2502 \u2502\n\u2502                                                                                                \u2502 \u2502\n\u2502 Output Neuron 0: (0.8*0.1) + (0.2*0.6) + (0.0*-0.2) = 0.08 + 0.12 + 0.0 = 0.2 &lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u2502 \u2502\n\u2502 Output Neuron 1: (0.8*0.4) + (0.2*-0.9) + (0.0*0.5) = 0.32 - 0.18 + 0.0 = 0.14 &lt;\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2510       \u2502 \u2502\n\u2502 Output Neuron 2: (0.8*-0.3) + (0.2*0.2) + (0.0*0.7) = -0.24 + 0.04 + 0.0 = -0.2 &lt;\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2510   \u2502 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502   \u2502   \u2502   \u2502\n                                                                                       \u2502   \u2502   \u2502   \u2502\n(Result of v'\u00b7W2) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                    \u25bc   \u25bc   \u25bc   \u2502\n                 \u2502 [0.2, 0.14, -0.2] \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba         +         \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                       (Add Bias)\n                                                               \u2502\n                                                               \u25bc\n                                               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                               \u2502   dense2.output (Logits)         \u2502\n                                               \u2502 [0.2+2.0, 0.14+1.5, -0.2+0.9]    \u2502\n                                               \u2502      [2.2, 1.64, 0.7]            \u2502\n                                               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                              \u2502\n                                                              \u25bc\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551                                        (STEP 4b: SOFTMAX ACTIVATION FUNCTION)                                            \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n                                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                  \u2502      (i) SUBTRACT MAX FOR STABILITY           \u2502\n                                  \u2502   max([2.2, 1.64, 0.7]) = 2.2                 \u2502\n                                  \u2502   [2.2-2.2, 1.64-2.2, 0.7-2.2]                \u2502\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                          \u2502\n                                                          \u25bc\n                                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                  \u2502       Shifted Logits: [0.0, -0.56, -1.5]      \u2502\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                          \u2502\n                                                          \u25bc\n                                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                  \u2502       (ii) EXPONENTIATE (e^x)                 \u2502\n                                  \u2502   e^0.0   = 1.0                               \u2502\n                                  \u2502   e^-0.56 = 0.571                             \u2502\n                                  \u2502   e^-1.5  = 0.223                             \u2502\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                          \u2502\n                                                          \u25bc\n                                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                  \u2502   Exponentiated: [1.0, 0.571, 0.223]          \u2502\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                          \u2502\n                                                          \u25bc\n                                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                  \u2502       (iii) NORMALIZE (Divide by sum)         \u2502\n                                  \u2502   Sum = 1.0 + 0.571 + 0.223 = 1.794           \u2502\n                                  \u2502                                               \u2502\n                                  \u2502   1.0   / 1.794 = 0.557                       \u2502\n                                  \u2502   0.571 / 1.794 = 0.318                       \u2502\n                                  \u2502   0.223 / 1.794 = 0.124                       \u2502\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                          \u2502\n                                                          \u25bc\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551                                              (STEP 5: FINAL RESULT)                                                      \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n                                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                  \u2502      Final Probabilities (Confidence Scores)  \u2502\n                                  \u2502         [0.557, 0.318, 0.124]                 \u2502\n                                  \u2502 (Sum = 0.557 + 0.318 + 0.124 = 0.999 \u2248 1.0)   \u2502\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"Explain%20Documents/Ch4pt3r.03_Softmax/03-softmax/","title":"Softmax","text":""},{"location":"Explain%20Documents/Ch4pt3r.03_Softmax/03-softmax/#explanation-of-softmax-in-neural-networks","title":"Explanation of Softmax in Neural Networks","text":""},{"location":"Explain%20Documents/Ch4pt3r.03_Softmax/03-softmax/#part-1-why-do-we-need-softmax-whats-the-problem","title":"Part 1: Why Do We Need Softmax? What's the Problem?","text":"<p>In previous chapters, you may have become familiar with the ReLU (Rectified Linear Unit) activation function. ReLU is excellent for hidden layers but has a few problems if used for the final output layer of a classification network:</p> <ol> <li>Unbounded: The output of ReLU can be any positive number (e.g., <code>[12, 99, 318]</code>). These numbers don't mean much on their own. 318 is larger than 99, but \"how much\" larger? Is it significantly more \"certain\"? We don't have a \"context\" for comparison.</li> <li>Not Normalized: The output values have no overall relationship. Their sum doesn't equal any fixed number.</li> <li>Exclusive: The output of each neuron is independent of the other neurons.</li> </ol> <p>Our Goal: For a classification problem, we want the neural network to \"tell\" us which class it \"thinks\" the input belongs to, with a clear level of confidence. For example, with 3 classes (dog, cat, bird), we want an output that looks like <code>[0.05, 0.9, 0.05]</code>, which means: \"I am 90% certain this is a cat, 5% it's a dog, and 5% it's a bird.\"</p> <p>=&gt; Softmax was created to solve this problem. It takes any real numbers (positive, negative, large, or small) and transforms them into a probability distribution. The characteristics of this probability distribution are: *   All output values are in the range <code>[0, 1]</code>. *   The sum of all output values always equals 1.</p> <p>These values are precisely the confidence scores we need.</p>"},{"location":"Explain%20Documents/Ch4pt3r.03_Softmax/03-softmax/#section-2-anatomy-of-the-softmax-formula","title":"Section 2: \"Anatomy\" of the Softmax Formula","text":"<p>The formula in the book might look intimidating:</p> \\[ S_{i,j} = \\frac{e^{z_{i,j}}}{\\sum_{l=1}^{L} e^{z_{i,l}}} \\] <p>Don't worry, let's break it down into two extremely simple steps:</p> <p>Step 1: Exponentiation - The Numerator <code>e^z</code></p> <ul> <li><code>z</code> are the output values from the previous layer (e.g., <code>layer_outputs = [4.8, 1.21, 2.385]</code>).</li> <li><code>e</code> is Euler's number (approximately 2.71828), the base of the natural logarithm.</li> <li>\"Exponentiation\" simply means raising <code>e</code> to the power of those <code>z</code> values. In Python, we use <code>E ** output</code> or <code>math.exp(output)</code>.</li> </ul> Python<pre><code># Example from the book\nlayer_outputs = [4.8, 1.21, 2.385]\nE = 2.71828182846\n\n# Calculate e^z for each value\nexp_values = [E**4.8, E**1.21, E**2.385]\n# Result: [121.51, 3.35, 10.86]\n</code></pre> <p>Why do this step? 1.  Eliminate Negative Numbers: <code>e</code> raised to any power always results in a positive number. This is crucial because probabilities cannot be negative. 2.  Amplify Differences: The exponential function makes large values overwhelmingly larger than small values. The value <code>4.8</code> is only about 2 times larger than <code>2.385</code>, but after exponentiation, <code>121.51</code> is more than 11 times larger than <code>10.86</code>! This helps the network become more \"confident\" in the prediction with the highest score.</p> <p>Step 2: Normalization - The Division</p> <p>After getting the exponentiated values (<code>exp_values</code>), we just need to do one thing:</p> <ol> <li>Calculate the sum of all those values (the denominator \\(\\sum_{l=1}^{L} e^{z_{i,l}}\\)).</li> <li>Divide each value by the sum you just calculated.</li> </ol> Python<pre><code># Continuing the example above\nexp_values = [121.51, 3.35, 10.86]\n\n# 1. Calculate the sum\nnorm_base = sum(exp_values) # 121.51 + 3.35 + 10.86 = 135.72\n\n# 2. Divide each value by the sum\nnorm_values = [\n    121.51 / norm_base, # ~0.895\n    3.35 / norm_base,   # ~0.025\n    10.86 / norm_base   # ~0.080\n]\n\n# Result: [0.895, 0.025, 0.080]\n# Check: 0.895 + 0.025 + 0.080 = 1.0\n</code></pre> <p>And that's it! We have transformed <code>[4.8, 1.21, 2.385]</code> into a probability distribution <code>[0.895, 0.025, 0.080]</code>.</p>"},{"location":"Explain%20Documents/Ch4pt3r.03_Softmax/03-softmax/#section-3-optimization-with-numpy-and-batch-processing","title":"Section 3: Optimization with NumPy and Batch Processing","text":"<p>In practice, we don't process data samples one by one but in an entire batch to speed things up. A batch of data will be in the form of a matrix, where each row is the output for one sample.</p> Python<pre><code># A batch of 3 samples, each with 3 outputs\nlayer_outputs = np.array([[4.8, 1.21, 2.385],\n                          [8.9, -1.81, 0.2],\n                          [1.41, 1.051, 0.026]])\n</code></pre> <p>Now, we need to calculate the Softmax for each row individually. This is where NumPy's <code>axis</code> and <code>keepdims</code> parameters shine.</p> <ul> <li><code>np.exp(layer_outputs)</code>: NumPy is smart and will automatically calculate the exponential for every element in the matrix.</li> <li><code>np.sum(..., axis=1)</code>: We need to calculate the sum of values along each row.<ul> <li><code>axis=0</code>: calculates the sum down the columns.</li> <li><code>axis=1</code>: calculates the sum across the rows. This is what we need.</li> </ul> </li> <li><code>keepdims=True</code>: When summing along <code>axis=1</code>, the result would be a row vector <code>[8.395, 7.29, 2.487]</code>. If we try to divide a <code>(3, 3)</code> matrix by a <code>(3,)</code> vector, NumPy might throw an error or not perform the division row-wise as intended. <code>keepdims=True</code> preserves the dimensions, turning the result into a column vector <code>[[8.395], [7.29], [2.487]]</code> with a shape of <code>(3, 1)</code>. Now, NumPy can correctly perform the division of a <code>(3, 3)</code> matrix by a <code>(3, 1)</code> column vector (each row of the matrix is divided by the corresponding value in the column vector).</li> </ul>"},{"location":"Explain%20Documents/Ch4pt3r.03_Softmax/03-softmax/#section-4-the-secret-trick-for-overflow-prevention","title":"Section 4: The \"Secret Trick\" for Overflow Prevention","text":"<p>The exponential function <code>e^x</code> grows very rapidly. If the input <code>z</code> is a large number (e.g., <code>1000</code>), <code>np.exp(1000)</code> will return <code>inf</code> (infinity), causing an overflow error and breaking the entire calculation.</p> <p>The Solution: We can subtract any arbitrary number from all input values <code>z</code> without changing the final result of the Softmax. Why? Because of the properties of exponents and division: $$ \\frac{e<sup>{z_1}}{e</sup>{z_1} + e^{z_2}} = \\frac{e^{z_1} \\cdot e<sup>{-C}}{e</sup>{z_1} \\cdot e^{-C} + e^{z_2} \\cdot e^{-C}} = \\frac{e^{z_1 - C}}{e^{z_1 - C} + e^{z_2 - C}} $$</p> <p>So, what number should we subtract? The largest number (max) among the input values of that row.</p> Python<pre><code>inputs = [1, 2, 3]\nmax_value = 3\nshifted_inputs = [1-3, 2-3, 3-3] # -&gt; [-2, -1, 0]\n</code></pre> <p>The benefits of this are: 1.  The largest value after subtraction will be <code>0</code>. (<code>e^0 = 1</code>) 2.  All other values will be negative. (<code>e</code> to a negative power is always a number less than 1). 3.  This ensures that the input to the <code>exp</code> function will never be a large positive number, thereby completely preventing overflow errors.</p> <p>This is exactly why in the final code snippet of the book, you will see this line: Python<pre><code>exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n</code></pre> This is the complete, safe, and efficient version of Softmax.</p>"},{"location":"Explain%20Documents/Ch4pt3r.03_Softmax/03-softmax/#part-2-abstract-easy-to-understand-explanation","title":"Part 2: Abstract &amp; Easy-to-Understand Explanation","text":"<p>Forget the math formulas and code. Imagine Softmax is a \"Confidence Distribution Machine\" for a talent competition.</p> <p>1. The Preliminary Round - Raw Scores</p> <p>Suppose there are 3 contestants (Dog, Cat, Bird) in a competition. The judges (the preceding neural network layer) give them raw scores. These scores can be messy: <code>Raw Scores = [4.8, 1.21, 2.385]</code></p> <p>Looking at these scores, we know the Dog contestant has the highest score, but how much \"higher\"? Is the lead \"overwhelming\"? It's hard to say.</p> <p>2. Step 1: The \"Hype\" Machine - Exponentiation</p> <p>To make the results clearer, the MC puts these scores into a \"Hype\" Machine. This machine has two functions: *   No Negative Scores: It turns all scores into \"enthusiasm\" points (always positive). *   Hypes Up the Best: This machine is extremely \"biased.\" Whoever already has a high score gets hyped up to the moon, while those with low scores only get a slight boost.</p> <p>After passing through the \"Hype\" Machine (i.e., <code>e^x</code>): <code>Hype Scores = [121.5, 3.4, 10.9]</code></p> <p>Now the difference is crystal clear! The Dog contestant not only has a higher score but is completely dominating the rest.</p> <p>3. Step 2: Slicing the \"Confidence Pie\" - Normalization</p> <p>Now, to make it easy for the audience to understand, the MC decides to stop using hype scores and instead divide a 100% \"confidence pie\" among the 3 contestants, based on the ratio of their hype scores.</p> <ul> <li>Total Hype Score = 121.5 + 3.4 + 10.9 = 135.8</li> <li>Dog's Slice of the Pie: <code>121.5 / 135.8 \u2248 89.5%</code></li> <li>Cat's Slice of the Pie: <code>3.4 / 135.8 \u2248 2.5%</code></li> <li>Bird's Slice of the Pie: <code>10.9 / 135.8 \u2248 8.0%</code></li> </ul> <p>The Final Result: <code>Confidence Levels = [0.895, 0.025, 0.080]</code></p> <p>This is the output of Softmax. It gives us a very clear conclusion: \"Based on the performance, I am 89.5% confident that the winner is the Dog.\"</p> <p>Regarding the \"overflow prevention trick\": Imagine a judge gets overly excited and gives a score of <code>1000</code>. The \"Hype\" Machine would \"burn out\" (overflow). A clever MC realizes that what matters is the difference between scores, not the absolute scores themselves. So, before putting them in the machine, he finds the highest score (1000) and subtracts it from everyone's score. The final result after slicing the pie remains exactly the same, but the hype machine is saved!</p> <p>In summary, Softmax does two things:</p> <ol> <li>It uses the exponential function <code>e^x</code> to amplify the highest score, making it a clear \"front-runner.\"</li> <li>It normalizes those amplified scores into a percentage (or probability), so that they all add up to 1.</li> </ol>"},{"location":"Explain%20Documents/Ch4pt3r.04_L0SS/04-loss-diagram/","title":"LOSS Diagram","text":"Text Only<pre><code>================================================================================\n          LOSS &amp; ACCURACY CALCULATION DIAGRAM (MATRIX OPERATIONS)\n================================================================================\n\n                                    (INPUTS)\n            +---------------------------------+      +---------------------+\n            |      softmax_outputs (\u0177)        |      |  class_targets (y)  |\n            |   (N x C Prediction Matrix)     |      |    (Ground Truth)   |\n            | [[0.7, 0.2, 0.1],               |      | [0, 1, 1]           | &lt;-- Sparse format\n            |  [0.5, 0.1, 0.4],               |      |      OR             |\n            |  [0.02, 0.9, 0.08]]             |      | [[1, 0, 0],         |\n            +---------------------------------+      |  [0, 1, 0],         | &lt;-- One-Hot format\n                                                     |  [0, 1, 0]]         |\n                                                     +---------------------+\n                                     |\n                                     v\n+-------------------------------------------------------------------------------+\n|                        FLOW 1: LOSS CALCULATION                               |\n+-------------------------------------------------------------------------------+\n                                     |\n                                     v\n                          (ACTION: Clip to prevent log(0))\n                           np.clip(y_pred, 1e-7, 1-1e-7)\n                                     |\n                                     v\n                      +-------------------------------+\n                      |      y_pred_clipped           |\n                      | [[0.7, 0.2, 0.1],             |\n                      |  [0.5, 0.1, 0.4],             |\n                      |  [0.02, 0.9, 0.08]]           |\n                      +-------------------------------+\n                                     |\n           +-------------------------+-------------------------+\n           |                                                   |\n           v (IF TARGETS ARE SPARSE)                         v (IF TARGETS ARE ONE-HOT)\n+------------------------------------+             +--------------------------------------+\n|       Get values by index          |             |   Element-wise multiplication        |\n|      y_pred[range(N), y_true]      |             |      y_pred_clipped * y_true         |\n|                                    |             |                                      |\n|  [0.7, 0.2, 0.1] ---&lt;--- [0]       |             |  [[0.7, 0.2, 0.1],   * [[1, 0, 0],   |\n|  [0.5, 0.1, 0.4] -------&lt;--- [1]   |             |   [0.5, 0.1, 0.4],     [0, 1, 0],   |\n|  [0.02, 0.9, 0.08] -----^---&lt;--- [1]   |             |   [0.02, 0.9, 0.08]]    [0, 1, 0]]   |\n|                      |           |             |            =                         |\n|                      v           |             |  [[0.7, 0.0, 0.0],                   |\n| (Vector of correct probabilities)|             |   [0.0, 0.1, 0.0],                   |\n|      [0.7, 0.5, 0.9]             |             |   [0.0, 0.9, 0.0]]                   |\n+------------------------------------+             |            |                         |\n                                                   |            v (np.sum(axis=1))        |\n                                                   +--------------------------------------+\n                                     |                         |\n                                     +-----------&gt;-------------+\n                                                 |\n                                                 v\n                                    +--------------------------+\n                                    |  correct_confidences     |\n                                    | (Vector of correct probs)|\n                                    |    [0.7, 0.5, 0.9]       |\n                                    +--------------------------+\n                                                 |\n                                                 v (ACTION: Take Negative Log)\n                                                    -np.log()\n                                                 |\n                                                 v\n                                    +--------------------------+\n                                    |      sample_losses       |\n                                    | [0.356, 0.693, 0.105]    |\n                                    +--------------------------+\n                                                 |\n                                                 v (ACTION: Take the mean)\n                                                     np.mean()\n                                                 |\n                                                 v\n                                     +-----------------------+\n                                     |     average_loss      |\n                                     |       (Scalar)        |\n                                     |       0.385           |\n                                     +-----------------------+\n\n\n+-------------------------------------------------------------------------------+\n|                  FLOW 2: ACCURACY CALCULATION - CORRECTED                     |\n+-------------------------------------------------------------------------------+\n                                     |\n         (Using original `softmax_outputs`) |\n+---------------------------------+  |  +---------------------------------------+\n|      softmax_outputs (\u0177)        |  |  |            class_targets (y)          |\n| [[0.7, 0.2, 0.1],               |  |  |                                       |\n|  [0.5, 0.1, 0.4],               |  |  | If One-Hot:                           |\n|  [0.02, 0.9, 0.08]]             |  |  | [[1,0,0], [0,1,0], [0,1,0]]            |\n+---------------------------------+  |  |      |                                |\n                 |                   |  |      v np.argmax(axis=1)              |\n                 v np.argmax(axis=1)   |  |                                       |\n                 |                   |  | If Sparse (no action needed):         |\n+----------------+                   |  | [0, 1, 1]                             |\n|  predictions   |                   |  +-----------------|---------------------+\n|   [0, 0, 1]    |                   |                    |\n+----------------+                   +-----------&gt;&lt;-------+\n                 |                              |\n                 +---------------&gt;&lt;-------------+\n                                |\n                                v (ACTION: Compare for equality)\n                                       predictions == y\n                                |\n                                v\n                   +----------------------------+\n                   |   [0, 0, 1] == [0, 1, 1]   |\n                   +----------------------------+\n                                |\n                                v\n                   +----------------------------+\n                   |   [True, False, True]      |\n                   +----------------------------+\n                                |\n                                v (ACTION: Take the mean, True=1, False=0)\n                                    np.mean()\n                                |\n                                v\n                         +-----------------------+\n                         |       accuracy        |\n                         |       (Scalar)        |\n                         |    0.666666...        |\n                         +-----------------------+\n\n\n================================================================================\n                                FINAL RESULTS\n                +------------------+     +------------------+\n                |   average_loss   |     |     accuracy     |\n                |      0.385       |     |   0.666666...    |\n                +------------------+     +------------------+\n================================================================================\n</code></pre>"},{"location":"Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/","title":"LOSS & ACCURACY","text":""},{"location":"Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#calculating-network-error-with-a-loss-function","title":"Calculating Network Error with a Loss Function","text":"<p>Welcome to the lesson on how we \"measure the error\" of a neural network. In previous chapters, we built a network capable of taking an input and making a prediction. But how do we know if that prediction is \"good\" or \"bad\"? And how much \"worse\" is it compared to another prediction? This is where the Loss Function comes into play.</p>"},{"location":"Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#1-why-isnt-accuracy-enough","title":"1. Why Isn't Accuracy Enough?","text":"<p>The first question you might ask is: \"Why not just use accuracy for measurement? We can just see what percentage of predictions the network gets right.\"</p> <p>Let's consider the classic example from the book: Assume the correct class is the middle one (index 1). We have two predictions from the model:</p> <ul> <li>Prediction A: <code>[0.22, 0.6, 0.18]</code></li> <li>Prediction B: <code>[0.32, 0.36, 0.32]</code></li> </ul> <p>If we only use accuracy, we would do the following: 1.  Use the <code>argmax</code> function to find the index of the largest value. 2.  The <code>argmax</code> for both A and B is <code>1</code>. 3.  Compare this to the correct result, which is <code>1</code>. Both are correct. The accuracy is 100% for both cases.</p> <p>But let's look closer. The output of a neural network (after the Softmax layer) represents its confidence level. *   In Prediction A, the model is very confident (60%) that it's class 1. *   In Prediction B, the model is only slightly more confident (36%) and is quite indecisive among the three classes.</p> <p>Clearly, Prediction A is much \"better\" than Prediction B. We need a metric that can reflect this difference in confidence. Accuracy is a discrete metric (right or wrong), whereas we need a more continuous and detailed metric.</p> <p>This is the purpose of Loss. Loss is a single number that indicates how much the model was wrong. The goal of training is to adjust the weights and biases to bring this Loss value as close to 0 as possible.</p>"},{"location":"Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#2-categorical-cross-entropy-the-loss-function-for-classification","title":"2. Categorical Cross-Entropy: The Loss Function for Classification","text":"<p>In our problem (classifying spiral data), the output layer uses the Softmax activation function, which turns numbers (logits) into a probability distribution (the sum of values is 1). To compare two probability distributions (one from the model's prediction, one from the ground truth), a mathematical tool called Cross-Entropy is commonly used.</p> <p>Info</p> <p>Side Note: What is Cross-Entropy? In information theory, cross-entropy measures the difference between two probability distributions. If you use an optimal encoding for distribution P to encode data from distribution Q, the cross-entropy H(Q, P) tells you the average number of bits required. The more similar P and Q are, the smaller this value becomes. In Machine Learning, we consider the \"ground-truth\" as distribution P and the model's \"predictions\" as distribution Q. The goal is to make Q as similar to P as possible, i.e., to minimize the cross-entropy. (Source: Deep Learning Book, Chapter 3.13)</p> <p>When applied to a multi-class classification problem, it is called Categorical Cross-Entropy.</p>"},{"location":"Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#mathematical-formula","title":"Mathematical Formula","text":"<p>The full formula for calculating the loss for a single sample <code>i</code> is:</p> <p><code>L_i = - \u03a3_j ( y_i,j * log(\u0177_i,j) )</code></p> <p>Where: *   <code>L_i</code>: The loss value for the <code>i</code>-th sample. *   <code>j</code>: The index for each output class (e.g., dog, cat, human). *   <code>\u03a3_j</code>: The summation symbol, summing over all classes <code>j</code>. *   <code>y_i,j</code>: The ground-truth. It is 1 if sample <code>i</code> truly belongs to class <code>j</code>, and 0 for all other classes. *   <code>\u0177_i,j</code> (read as \"y-hat\"): The model's prediction. It is the probability that the model predicts sample <code>i</code> belongs to class <code>j</code> (the value from Softmax). *   <code>log</code>: Is the natural logarithm (base e), which is <code>math.log()</code> or <code>np.log()</code> in Python.</p>"},{"location":"Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#the-magic-of-one-hot-encoding","title":"The Magic of One-Hot Encoding","text":"<p>Let's see how this formula works in practice. Assume we have 3 classes and the ground truth is the first class (index 0). *   Model's prediction (\u0177): <code>[0.7, 0.1, 0.2]</code> *   Ground truth (y): <code>[1, 0, 0]</code></p> <p>The vector <code>[1, 0, 0]</code> is called one-hot encoding. \"Hot\" is 1, \"cold\" is 0.</p> <p>Applying the formula: <code>L = - ( y_0*log(\u0177_0) + y_1*log(\u0177_1) + y_2*log(\u0177_2) )</code> <code>L = - ( 1*log(0.7) + 0*log(0.1) + 0*log(0.2) )</code> <code>L = - ( 1*log(0.7) + 0 + 0 )</code> <code>L = -log(0.7)</code></p> <p>The initially imposing formula has been simplified to a very simple operation: just take the natural logarithm of the predicted probability for the correct class, and then negate it.</p> <p>This is a critically important insight! It makes the calculation much more efficient.</p>"},{"location":"Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#3-implementation-in-python-step-by-step","title":"3. Implementation in Python (Step-by-Step)","text":""},{"location":"Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#31-calculating-loss-for-a-batch","title":"3.1. Calculating Loss for a Batch","text":"<p>In practice, we don't process samples one by one but in batches to speed things up.</p> Python<pre><code>import numpy as np\n\n# Softmax output for a batch of 3 samples, each with 3 classes\nsoftmax_outputs = np.array([[0.7, 0.1, 0.2],  # Sample 1\n                            [0.1, 0.5, 0.4],  # Sample 2\n                            [0.02, 0.9, 0.08]]) # Sample 3\n\n# True labels (target labels). This is in \"sparse\" format.\n# Sample 1 is class 0, sample 2 is class 1, sample 3 is class 1\nclass_targets = [0, 1, 1]\n</code></pre> <p>How do we extract the correct probabilities from <code>softmax_outputs</code>? We need: *   From row 0, get the element at column 0 (<code>0.7</code>) *   From row 1, get the element at column 1 (<code>0.5</code>) *   From row 2, get the element at column 1 (<code>0.9</code>)</p> <p>NumPy provides a very elegant way to do this, called advanced indexing:</p> Python<pre><code># Extract the probabilities of the correct classes\ncorrect_confidences = softmax_outputs[\n    range(len(softmax_outputs)), # Row indices: [0, 1, 2]\n    class_targets              # Column indices: [0, 1, 1]\n]\n\nprint(correct_confidences)\n# Result: [0.7 0.5 0.9]\n</code></pre> <p>Now, we just need to apply the <code>-log</code> formula to each of these values:</p> Python<pre><code># Calculate the loss for each sample\nsample_losses = -np.log(correct_confidences)\nprint(sample_losses)\n# Result: [0.35667494 0.69314718 0.10536052]\n</code></pre> <p>Finally, the loss for the entire batch is usually calculated as the arithmetic mean of the individual losses:</p> Python<pre><code># Calculate the average loss for the batch\naverage_loss = np.mean(sample_losses)\nprint(average_loss)\n# Result: 0.38506088...\n</code></pre>"},{"location":"Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#32-the-tricky-problem-log0","title":"3.2. The Tricky Problem: <code>log(0)</code>","text":"<p>The function <code>log(x)</code> is only defined for <code>x &gt; 0</code>. What happens if the model predicts a probability of 0 for the correct class?</p> <p><code>log(0)</code> is negative infinity. This will produce a loss value of <code>inf</code> (infinity) in Python, and once <code>inf</code> appears, it will \"infect\" subsequent calculations (e.g., the mean of a list containing <code>inf</code> is also <code>inf</code>). This will break the entire training process.</p> <p>Even worse, if the model is overconfident and predicts a probability of 1.0 for the correct class, then <code>log(1.0) = 0</code>, and the loss will be 0. But due to floating-point rounding errors in computers, the value might be <code>1.0000001</code>. <code>log(1.0000001)</code> is a very small positive number, causing the loss to become a very small negative number. A negative loss is nonsensical.</p> <p>Solution: Clipping</p> <p>To solve this definitively, we will \"clip\" the predicted values so they never reach 0 or 1. We'll force them into a very small range, for example, <code>[1e-7, 1 - 1e-7]</code>.</p> <ul> <li><code>1e-7</code> is scientific notation for <code>0.0000001</code>.</li> <li>Any value smaller than <code>1e-7</code> will be set to <code>1e-7</code>.</li> <li>Any value larger than <code>1 - 1e-7</code> will be set to <code>1 - 1e-7</code>.</li> </ul> <p>In NumPy, we use the <code>np.clip()</code> function:</p> <p>Python<pre><code># y_pred is the array of predicted probabilities\ny_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n</code></pre> This operation ensures that we will never have to calculate the <code>log</code> of 0 or of a number greater than 1, making the calculation stable.</p>"},{"location":"Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#33-handling-both-label-formats-one-hot-and-sparse","title":"3.3. Handling Both Label Formats (One-Hot and Sparse)","text":"<p>The true labels (targets) can come in two formats: 1.  Sparse: A 1D array containing the indices of the correct classes. Example: <code>[0, 1, 1]</code>. 2.  One-Hot: A 2D array. Example: <code>[[1,0,0], [0,1,0], [0,1,0]]</code>.</p> <p>We can check the number of dimensions of the label array (<code>len(y_true.shape)</code>) to determine its format and handle it accordingly.</p> <ul> <li>If Sparse (shape=1): Use the indexing method as shown above.</li> <li>If One-Hot (shape=2): We go back to the original formula <code>\u03a3 (y * log(\u0177))</code>. Since <code>y</code> is one-hot, the multiplication <code>y * log(\u0177)</code> will turn all values for the incorrect classes to 0, leaving only the value for the correct class. Then we just need to sum along the rows (<code>axis=1</code>).</li> </ul> <p>Python<pre><code># Assume y_pred_clipped has been calculated\n# And y_true is a one-hot label array\nif len(y_true.shape) == 2:\n    correct_confidences = np.sum(\n        y_pred_clipped * y_true,\n        axis=1\n    )\n</code></pre> This method gives an equivalent result to indexing but is more general.</p>"},{"location":"Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#4-building-a-complete-loss-class","title":"4. Building a Complete Loss Class","text":"<p>To keep our code organized and reusable, we will create classes for the Loss.</p>"},{"location":"Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#the-base-loss-class","title":"The Base <code>Loss</code> Class","text":"<p>This is an abstract parent class. It has a <code>calculate</code> method that is common to all types of loss: calculate the loss for each sample (via a <code>forward</code> method) and then take the average.</p> Python<pre><code># Common loss class\nclass Loss:\n    # Calculates the data loss\n    def calculate(self, output, y):\n        # Calculate sample losses\n        sample_losses = self.forward(output, y)\n\n        # Calculate mean loss\n        data_loss = np.mean(sample_losses)\n\n        return data_loss\n</code></pre>"},{"location":"Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#the-loss_categoricalcrossentropy-class","title":"The <code>Loss_CategoricalCrossentropy</code> Class","text":"<p>This class inherits from <code>Loss</code> and implements the specific calculation logic for Categorical Cross-Entropy.</p> Python<pre><code># Cross-entropy loss\nclass Loss_CategoricalCrossentropy(Loss):\n\n    # Forward pass method\n    def forward(self, y_pred, y_true):\n        samples = len(y_pred)\n\n        # 1. Clip data to prevent division by 0\n        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n\n        # 2. Check label format and calculate\n        # If labels are sparse [0, 1, 1]\n        if len(y_true.shape) == 1:\n            correct_confidences = y_pred_clipped[\n                range(samples),\n                y_true\n            ]\n        # If labels are one-hot [[1,0,0], [0,1,0], ...]\n        elif len(y_true.shape) == 2:\n            correct_confidences = np.sum(\n                y_pred_clipped * y_true,\n                axis=1\n            )\n\n        # 3. Calculate negative log likelihoods (loss for each sample)\n        negative_log_likelihoods = -np.log(correct_confidences)\n        return negative_log_likelihoods\n</code></pre>"},{"location":"Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#5-additionally-calculating-accuracy","title":"5. Additionally Calculating Accuracy","text":"<p>Although not used for optimization, accuracy is still a very intuitive metric for humans to evaluate a model's performance. We will calculate it in parallel with the loss.</p> <p>The way to calculate accuracy is simple: 1.  Use <code>np.argmax(softmax_outputs, axis=1)</code> to find the predicted class with the highest probability for each sample. <code>axis=1</code> means finding the argmax along each row (each sample). 2.  Compare this array of predictions with the array of true labels. The comparison <code>predictions == class_targets</code> will return an array of <code>True</code> (if correct) and <code>False</code> (if incorrect) values. 3.  Take the mean of this boolean array. <code>np.mean()</code> will automatically treat <code>True</code> as 1 and <code>False</code> as 0.</p> Python<pre><code># Get predictions from the softmax output\npredictions = np.argmax(softmax_outputs, axis=1)\n\n# If labels are one-hot, convert to sparse\nif len(class_targets.shape) == 2:\n    class_targets = np.argmax(class_targets, axis=1)\n\n# Compare and calculate the mean\naccuracy = np.mean(predictions == class_targets)\nprint('acc:', accuracy)\n</code></pre>"},{"location":"Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#summary","title":"Summary","text":"<p>In this chapter, we have learned the core concepts:</p> <ol> <li>Why we need Loss: Loss is a detailed, continuous measure of the model's \"degree of error,\" which is much better for training than accuracy.</li> <li>Categorical Cross-Entropy: It's the standard loss function for multi-class classification, measuring the difference between the predicted probability distribution and the true distribution.</li> <li>Simplified Formula: When labels are one-hot, the complex formula simplifies to <code>-log(probability of the correct class)</code>.</li> <li>Practical Implementation:<ul> <li>Using NumPy indexing for efficient batch calculations.</li> <li>Solving the <code>log(0)</code> problem with the clipping technique.</li> <li>Building the code in an object-oriented (OOP) way with <code>Loss</code> classes for easy management and extension.</li> </ul> </li> <li>Accuracy: It remains an important metric for human monitoring and is calculated in parallel with the loss.</li> </ol> <p>By being able to calculate the Loss, we now have a \"signal\" to know where and how the model needs to improve. The next chapter will show us how to use this signal to actually \"teach\" the neural network through optimization and backpropagation.</p>"},{"location":"vi/","title":"Ch\u00e0o M\u1eebng","text":""},{"location":"vi/#chao-mung-en-voi-thanh-pho-ky-thuat-so","title":"\ud83c\udf0c Ch\u00e0o M\u1eebng \u0110\u1ebfn V\u1edbi Th\u00e0nh Ph\u1ed1 K\u1ef9 Thu\u1eadt S\u1ed1 \ud83c\udf03","text":"Text Only<pre><code>       \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n       \u2551  NEURAL NETWORKS FROM SCRATCH      \u2551\n       \u2551  H\u00e0nh Tr\u00ecnh X\u00e2y D\u1ef1ng T\u1eeb Con S\u1ed1 0   \u2551\n       \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n</code></pre> <p>Ch\u00e0o b\u1ea1n! \ud83d\ude0a M\u00ecnh l\u00e0 m\u1ed9t ng\u01b0\u1eddi y\u00eau th\u00edch c\u00f4ng ngh\u1ec7, \u0111ang m\u00e0y m\u00f2 kh\u00e1m ph\u00e1 th\u1ebf gi\u1edbi tr\u00ed tu\u1ec7 nh\u00e2n t\u1ea1o qua cu\u1ed1n s\u00e1ch Neural Networks from Scratch in Python. \u0110\u00e2y l\u00e0 n\u01a1i m\u00ecnh ghi l\u1ea1i h\u00e0nh tr\u00ecnh h\u1ecdc t\u1eadp, t\u1eebng b\u01b0\u1edbc m\u1ed9t, t\u1eeb nh\u1eefng d\u00f2ng code Python \u0111\u1ea7u ti\u00ean \u0111\u1ebfn nh\u1eefng m\u1ea1ng n\u01a1-ron ph\u1ee9c t\u1ea1p. Kh\u00f4ng c\u00f3 g\u00ec hoa m\u1ef9, ch\u1ec9 l\u00e0 m\u1ed9t g\u00f3c nh\u1ecf trong kh\u00f4ng gian s\u1ed1, n\u01a1i m\u00ecnh chia s\u1ebb nh\u1eefng g\u00ec h\u1ecdc \u0111\u01b0\u1ee3c, nh\u1eefng l\u1ea7n v\u1ea5p ng\u00e3, v\u00e0 c\u1ea3 ni\u1ec1m vui khi hi\u1ec3u th\u00eam m\u1ed9t \u0111i\u1ec1u m\u1edbi.</p>"},{"location":"vi/#vi-sao-hanh-trinh-nay-ac-biet","title":"\ud83d\udcbe V\u00ec Sao H\u00e0nh Tr\u00ecnh N\u00e0y \u0110\u1eb7c Bi\u1ec7t?","text":"<p>Trong th\u1ebf gi\u1edbi cyberpunk c\u1ee7a ri\u00eang m\u00ecnh, m\u00ecnh t\u01b0\u1edfng t\u01b0\u1ee3ng m\u1ed7i ch\u01b0\u01a1ng s\u00e1ch l\u00e0 m\u1ed9t node trong m\u1ea1ng l\u01b0\u1edbi ki\u1ebfn th\u1ee9c r\u1ed9ng l\u1edbn. M\u1ed7i file Python, m\u1ed7i th\u01b0 m\u1ee5c t\u00e0i li\u1ec7u, l\u00e0 m\u1ed9t m\u1ea3nh gh\u00e9p trong b\u1ee9c tranh k\u1ef9 thu\u1eadt s\u1ed1 m\u00e0 m\u00ecnh \u0111ang v\u1ebd. M\u00ecnh kh\u00f4ng ph\u1ea3i chuy\u00ean gia, ch\u1ec9 l\u00e0 m\u1ed9t coder t\u00f2 m\u00f2, nh\u01b0ng m\u00ecnh tin r\u1eb1ng vi\u1ec7c h\u1ecdc t\u1eeb \u0111\u1ea7u, t\u1eebng b\u01b0\u1edbc nh\u1ecf, c\u00f3 th\u1ec3 d\u1eabn ch\u00fang ta \u0111\u1ebfn nh\u1eefng ch\u00e2n tr\u1eddi m\u1edbi.</p> <ul> <li>M\u1ed7i ch\u01b0\u01a1ng, m\u1ed9t th\u1eed th\u00e1ch: T\u1eebng file Python trong c\u00e1c th\u01b0 m\u1ee5c t\u01b0\u01a1ng \u1ee9ng v\u1edbi c\u00e1c ch\u01b0\u01a1ng s\u00e1ch, t\u1eeb c\u01a1 b\u1ea3n \u0111\u1ebfn n\u00e2ng cao.</li> <li>H\u1ecdc qua th\u1ef1c h\u00e0nh: M\u00ecnh vi\u1ebft l\u1ea1i code, th\u1eed nghi\u1ec7m, v\u00e0 ghi ch\u00fa \u0111\u1ec3 hi\u1ec3u r\u00f5 h\u01a1n c\u00e1ch ho\u1ea1t \u0111\u1ed9ng c\u1ee7a m\u1ea1ng n\u01a1-ron.</li> <li>Tinh th\u1ea7n m\u1edf: M\u00ecnh r\u1ea5t mong nh\u1eadn \u0111\u01b0\u1ee3c \u00fd ki\u1ebfn, b\u00ecnh lu\u1eadn, ho\u1eb7c \u0111\u00f3ng g\u00f3p t\u1eeb b\u1ea1n \u0111\u1ec3 h\u00e0nh tr\u00ecnh n\u00e0y th\u00eam phong ph\u00fa!</li> </ul>"},{"location":"vi/#kham-pha-thanh-pho-no-ron","title":"\ud83c\udf06 Kh\u00e1m Ph\u00e1 Th\u00e0nh Ph\u1ed1 N\u01a1-ron","text":"<p>H\u00e3y t\u01b0\u1edfng t\u01b0\u1ee3ng b\u1ea1n \u0111ang b\u01b0\u1edbc v\u00e0o m\u1ed9t th\u00e0nh ph\u1ed1 t\u01b0\u01a1ng lai, n\u01a1i c\u00e1c t\u00f2a nh\u00e0 l\u00e0 c\u00e1c l\u1edbp n\u01a1-ron, \u00e1nh s\u00e1ng neon l\u00e0 c\u00e1c tr\u1ecdng s\u1ed1, v\u00e0 d\u00f2ng d\u1eef li\u1ec7u ch\u1ea3y nh\u01b0 nh\u1eefng con \u0111\u01b0\u1eddng kh\u00f4ng bao gi\u1edd ng\u1ee7. Trang web n\u00e0y s\u1ebd d\u1eabn b\u1ea1n qua t\u1eebng con ph\u1ed1:</p> <ul> <li>C\u00e1c ch\u01b0\u01a1ng s\u00e1ch: M\u1ed7i th\u01b0 m\u1ee5c l\u00e0 m\u1ed9t ch\u01b0\u01a1ng, ch\u1ee9a code Python v\u00e0 t\u00e0i li\u1ec7u gi\u1ea3i th\u00edch chi ti\u1ebft.</li> <li>Ghi ch\u00fa c\u00e1 nh\u00e2n: Nh\u1eefng suy ngh\u0129, b\u00e0i h\u1ecdc, v\u00e0 c\u1ea3 l\u1ed7i sai m\u00ecnh g\u1eb7p ph\u1ea3i trong qu\u00e1 tr\u00ecnh h\u1ecdc.</li> <li>Kh\u00f4ng gian t\u01b0\u01a1ng t\u00e1c: M\u00ecnh hy v\u1ecdng b\u1ea1n s\u1ebd \u0111\u1ec3 l\u1ea1i m\u1ed9t b\u00ecnh lu\u1eadn, m\u1ed9t g\u1ee3i \u00fd, ho\u1eb7c th\u1eadm ch\u00ed m\u1ed9t d\u00f2ng code \u0111\u1ec3 c\u00f9ng nhau c\u1ea3i thi\u1ec7n d\u1ef1 \u00e1n n\u00e0y!</li> </ul> Text Only<pre><code>   \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n   \u2551  // \u0110I\u1ec2M KH\u1edeI \u0110\u1ea6U           \u2551\n   \u2551  &gt; Kh\u00e1m ph\u00e1 c\u00e1c th\u01b0 m\u1ee5c     \u2551\n   \u2551  &gt; Ch\u1ea1y th\u1eed code Python     \u2551\n   \u2551  &gt; \u0110\u1ec3 l\u1ea1i d\u1ea5u \u1ea5n c\u1ee7a b\u1ea1n!   \u2551\n   \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n</code></pre>"},{"location":"vi/#hay-tham-gia-cung-minh","title":"\ud83d\udca1 H\u00e3y Tham Gia C\u00f9ng M\u00ecnh!","text":"<p>M\u00ecnh kh\u00f4ng h\u1ee9a h\u1eb9n \u0111\u00e2y l\u00e0 m\u1ed9t d\u1ef1 \u00e1n ho\u00e0n h\u1ea3o, nh\u01b0ng n\u00f3 \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng b\u1eb1ng \u0111am m\u00ea v\u00e0 s\u1ef1 t\u00f2 m\u00f2. N\u1ebfu b\u1ea1n c\u0169ng y\u00eau th\u00edch AI, mu\u1ed1n t\u00ecm hi\u1ec3u c\u00e1ch m\u1ea1ng n\u01a1-ron ho\u1ea1t \u0111\u1ed9ng, ho\u1eb7c ch\u1ec9 \u0111\u01a1n gi\u1ea3n l\u00e0 t\u00f2 m\u00f2 v\u1ec1 th\u1ebf gi\u1edbi l\u1eadp tr\u00ecnh, h\u00e3y:</p> <ul> <li>Kh\u00e1m ph\u00e1 c\u00e1c ch\u01b0\u01a1ng: B\u1eaft \u0111\u1ea7u t\u1eeb th\u01b0 m\u1ee5c \u0111\u1ea7u ti\u00ean v\u00e0 c\u00f9ng m\u00ecnh x\u00e2y d\u1ef1ng m\u1ea1ng n\u01a1-ron t\u1eeb con s\u1ed1 0.</li> <li>Chia s\u1ebb \u00fd ki\u1ebfn: \u0110\u1ec3 l\u1ea1i b\u00ecnh lu\u1eadn, c\u00e2u h\u1ecfi, ho\u1eb7c g\u1ee3i \u00fd \u1edf ph\u1ea7n b\u00ecnh lu\u1eadn b\u00ean d\u01b0\u1edbi.</li> <li>\u0110\u00f3ng g\u00f3p code: N\u1ebfu b\u1ea1n th\u1ea5y c\u00e1ch c\u1ea3i thi\u1ec7n code ho\u1eb7c t\u00e0i li\u1ec7u, h\u00e3y g\u1eedi m\u1ed9t pull request tr\u00ean repo!</li> </ul> <p>C\u1ea3m \u01a1n b\u1ea1n \u0111\u00e3 gh\u00e9 th\u0103m g\u00f3c nh\u1ecf n\u00e0y trong v\u0169 tr\u1ee5 s\u1ed1. H\u00e3y c\u00f9ng nhau th\u1eafp s\u00e1ng nh\u1eefng \u00e1nh neon tri th\u1ee9c! \ud83d\ude80</p> <p>LI\u00caN H\u1ec6 &amp; \u0110\u00d3NG G\u00d3P GitHub Email</p> <p>H\u00e0nh tr\u00ecnh b\u1eaft \u0111\u1ea7u t\u1eeb m\u1ed9t d\u00f2ng code. B\u1ea1n \u0111\u00e3 s\u1eb5n s\u00e0ng ch\u01b0a?</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-forward-diagram/","title":"First Forward Diagram","text":""},{"location":"vi/Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-forward-diagram/#so-o-chi-tiet-tinh-toan-mot-hang-trong-forward-pass","title":"S\u01a1 \u0111\u1ed3 chi ti\u1ebft: T\u00ednh to\u00e1n m\u1ed9t h\u00e0ng trong Forward Pass","text":"<p>S\u01a1 \u0111\u1ed3 n\u00e0y \"ph\u00f3ng to\" v\u00e0o qu\u00e1 tr\u00ecnh t\u00ednh to\u00e1n \u0111\u1ec3 t\u1ea1o ra h\u00e0ng \u0111\u1ea7u ti\u00ean c\u1ee7a <code>dense1.output</code>.</p> Text Only<pre><code>======================================================================================================\n     S\u01a0 \u0110\u1ed2 TRUY\u1ec0N XU\u00d4I CHI TI\u1ebeT - T\u00cdNH TO\u00c1N CHO M\u1ed8T M\u1eaaU \u0110\u1ea6U V\u00c0O (H\u00c0NG \u0110\u1ea6U TI\u00caN)\n======================================================================================================\n\n    (\u0110\u1ea6U V\u00c0O - H\u00c0NG \u0110\u1ea6U TI\u00caN C\u1ee6A X)                        (TO\u00c0N B\u1ed8 TR\u1eccNG S\u1ed0 W)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 [x1, y1]  \u2502                                      \u2502 [[w11, w12, w13],     \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                                      \u2502  [w21, w22, w23]]     \u2502\n          \u2502                                            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                      \u2502\n                                      \u25bc\n                      (1) PH\u00c9P NH\u00c2N MA TR\u1eacN (DOT PRODUCT)\n                      \"L\u1ea5y h\u00e0ng nh\u00e2n v\u1edbi t\u1eebng c\u1ed9t\"\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u2502                                                       \u2502\n          \u25bc (\u0110\u1ec3 t\u00ednh gi\u00e1 tr\u1ecb \u0110\u1ea6U TI\u00caN c\u1ee7a h\u00e0ng k\u1ebft qu\u1ea3)           \u25bc (\u0110\u1ec3 t\u00ednh gi\u00e1 tr\u1ecb TH\u1ee8 HAI c\u1ee7a h\u00e0ng k\u1ebft qu\u1ea3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 [x1, y1]  \u25cf  [w11, w21]  \u2502                          \u2502 [x1, y1]  \u25cf  [w12, w22]  \u2502\n\u2502 (H\u00e0ng 1 c\u1ee7a X \u25cf C\u1ed9t 1 c\u1ee7a W) \u2502                          \u2502 (H\u00e0ng 1 c\u1ee7a X \u25cf C\u1ed9t 2 c\u1ee7a W) \u2502\n\u2502                          \u2502                          \u2502                          \u2502\n\u2502   x1 * w11               \u2502                          \u2502   x1 * w12               \u2502\n\u2502      +                   \u2502                          \u2502      +                   \u2502\n\u2502   y1 * w21               \u2502                          \u2502   y1 * w22               \u2502\n\u2502      =                   \u2502                          \u2502      =                   \u2502\n\u2502     o11                  \u2502                          \u2502     o12                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502                                                       \u2502\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                           \u2502\n                                       \u2502                           \u2502\n                                       \u25bc                           \u25bc\n                                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                \u2502   dot_product (H\u00c0NG \u0110\u1ea6U TI\u00caN)                   \u2502\n                                \u2502   [o11, o12, o13]                               \u2502  &lt;-- o13 \u0111\u01b0\u1ee3c t\u00ednh t\u01b0\u01a1ng t\u1ef1 v\u1edbi c\u1ed9t 3 c\u1ee7a W\n                                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                       \u2502\n                                                       \u25bc\n                                      (2) C\u1ed8NG THI\u00caN V\u1eca (BIAS)\n                                                       \u2502\n                                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                      \u2502                                 \u2502\n                                      \u25bc                                 \u25bc\n                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                          \u2502  [o11, o12, o13]  \u2502               \u2502    [b1, b2, b3]   \u2502\n                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502                                 \u2502\n                                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba    +    \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                    \u2502\n                                                    \u25bc\n                                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                        \u2502  output (H\u00c0NG \u0110\u1ea6U TI\u00caN)   \u2502\n                                        \u2502 [o11+b1, o12+b2, o13+b3]   \u2502\n                                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n------------------------------------------------------------------------------------------------------\n* QUAN TR\u1eccNG: Qu\u00e1 tr\u00ecnh n\u00e0y \u0111\u01b0\u1ee3c l\u1eb7p l\u1ea1i cho T\u1ea4T C\u1ea2 100 h\u00e0ng c\u1ee7a ma tr\u1eadn \u0111\u1ea7u v\u00e0o X \u0111\u1ec3 t\u1ea1o ra 100 h\u00e0ng\n* c\u1ee7a ma tr\u1eadn \u0111\u1ea7u ra `dense1.output`. NumPy th\u1ef1c hi\u1ec7n t\u1ea5t c\u1ea3 c\u00e1c ph\u00e9p t\u00ednh n\u00e0y song song m\u1ed9t c\u00e1ch\n* hi\u1ec7u qu\u1ea3.\n------------------------------------------------------------------------------------------------------\n</code></pre>"},{"location":"vi/Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/","title":"First Layer","text":""},{"location":"vi/Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#giai-thich-chi-tiet-cho-oan-ma","title":"Gi\u1ea3i th\u00edch chi ti\u1ebft cho \u0111o\u1ea1n m\u00e3.","text":"<p>Ch\u01b0\u01a1ng tr\u00ecnh <code>chapter01.py</code></p> <p>T\u1ed5ng quan: M\u1ee5c ti\u00eau c\u1ee7a ch\u01b0\u01a1ng tr\u00ecnh l\u00e0 g\u00ec?</p> <p>H\u00e3y t\u01b0\u1edfng t\u01b0\u1ee3ng ch\u00fang ta mu\u1ed1n x\u00e2y d\u1ef1ng m\u1ed9t \"b\u1ed9 n\u00e3o\" nh\u00e2n t\u1ea1o \u0111\u01a1n gi\u1ea3n. Ch\u01b0\u01a1ng tr\u00ecnh n\u00e0y th\u1ef1c hi\u1ec7n b\u01b0\u1edbc \u0111\u1ea7u ti\u00ean v\u00e0 c\u01a1 b\u1ea3n nh\u1ea5t:</p> <ol> <li>X\u00e2y d\u1ef1ng m\u1ed9t \"n\u01a1-ron th\u1ea7n kinh\": T\u1ea1o ra m\u1ed9t \u0111\u01a1n v\u1ecb x\u1eed l\u00fd th\u00f4ng tin c\u01a1 b\u1ea3n.</li> <li>Chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u: T\u1ea1o ra m\u1ed9t b\u1ed9 d\u1eef li\u1ec7u m\u1eabu \u0111\u1ec3 \"b\u1ed9 n\u00e3o\" c\u00f3 c\u00e1i \u0111\u1ec3 x\u1eed l\u00fd.</li> <li>Th\u1ef1c hi\u1ec7n m\u1ed9t ph\u00e9p t\u00ednh: \u0110\u01b0a d\u1eef li\u1ec7u qua \"b\u1ed9 n\u00e3o\" v\u00e0 xem k\u1ebft qu\u1ea3 \u0111\u1ea7u ra l\u00e0 g\u00ec.</li> </ol> <p>\u0110\u00e2y l\u00e0 n\u1ec1n t\u1ea3ng c\u1ee7a m\u1ecdi m\u1ea1ng n\u01a1-ron. Hi\u1ec3u r\u00f5 t\u1eebng d\u00f2ng m\u00e3 \u1edf \u0111\u00e2y s\u1ebd gi\u00fap b\u1ea1n n\u1eafm v\u1eefng c\u00e1c kh\u00e1i ni\u1ec7m ph\u1ee9c t\u1ea1p h\u01a1n sau n\u00e0y.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#phan-1-chuan-bi-cong-cu-va-nguyen-lieu-imports-data","title":"Ph\u1ea7n 1: Chu\u1ea9n b\u1ecb c\u00f4ng c\u1ee5 v\u00e0 nguy\u00ean li\u1ec7u (Imports &amp; Data)","text":"<p>\u0110\u00e2y l\u00e0 b\u01b0\u1edbc ch\u00fang ta t\u1eadp h\u1ee3p c\u00e1c th\u01b0 vi\u1ec7n v\u00e0 d\u1eef li\u1ec7u c\u1ea7n thi\u1ebft tr\u01b0\u1edbc khi b\u1eaft \u0111\u1ea7u \"x\u00e2y d\u1ef1ng\".</p> Python<pre><code>import numpy as np\nimport nnfs\nimport matplotlib.pyplot as plt\n\nfrom nnfs.datasets import spiral_data\nnnfs.init()\n</code></pre>"},{"location":"vi/Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#giai-thich-chi-tiet-tung-dong","title":"Gi\u1ea3i th\u00edch chi ti\u1ebft t\u1eebng d\u00f2ng:","text":"<ul> <li> <p><code>import numpy as np</code>:</p> <ul> <li>N\u00f3 l\u00e0 g\u00ec?: <code>NumPy</code> (Numerical Python) l\u00e0 th\u01b0 vi\u1ec7n c\u01a1 b\u1ea3n v\u00e0 quan tr\u1ecdng nh\u1ea5t cho khoa h\u1ecdc d\u1eef li\u1ec7u trong Python. N\u00f3 cung c\u1ea5p m\u1ed9t c\u1ea5u tr\u00fac d\u1eef li\u1ec7u c\u1ef1c k\u1ef3 hi\u1ec7u qu\u1ea3 g\u1ecdi l\u00e0 m\u1ea3ng (array) v\u00e0 c\u00e1c c\u00f4ng c\u1ee5 \u0111\u1ec3 th\u1ef1c hi\u1ec7n c\u00e1c ph\u00e9p to\u00e1n tr\u00ean m\u1ea3ng \u0111\u00f3, \u0111\u1eb7c bi\u1ec7t l\u00e0 to\u00e1n ma tr\u1eadn.</li> <li>T\u1ea1i sao c\u1ea7n n\u00f3?: M\u1ea1ng n\u01a1-ron v\u1ec1 b\u1ea3n ch\u1ea5t l\u00e0 m\u1ed9t chu\u1ed7i c\u00e1c ph\u00e9p to\u00e1n ma tr\u1eadn. NumPy gi\u00fap ch\u00fang ta th\u1ef1c hi\u1ec7n c\u00e1c ph\u00e9p nh\u00e2n, c\u1ed9ng ma tr\u1eadn n\u00e0y m\u1ed9t c\u00e1ch nhanh ch\u00f3ng v\u00e0 hi\u1ec7u qu\u1ea3 h\u01a1n r\u1ea5t nhi\u1ec1u so v\u1edbi vi\u1ec7c d\u00f9ng list th\u00f4ng th\u01b0\u1eddng c\u1ee7a Python. <code>as np</code> l\u00e0 m\u1ed9t quy \u01b0\u1edbc ph\u1ed5 bi\u1ebfn \u0111\u1ec3 \u0111\u1eb7t t\u00ean vi\u1ebft t\u1eaft cho th\u01b0 vi\u1ec7n.</li> </ul> </li> <li> <p><code>import nnfs</code>:</p> <ul> <li>N\u00f3 l\u00e0 g\u00ec?: <code>nnfs</code> (Neural Networks from Scratch) l\u00e0 m\u1ed9t th\u01b0 vi\u1ec7n h\u1ed7 tr\u1ee3 \u0111\u01b0\u1ee3c vi\u1ebft ri\u00eang cho cu\u1ed1n s\u00e1ch c\u00f9ng t\u00ean. M\u1ee5c \u0111\u00edch c\u1ee7a n\u00f3 l\u00e0 gi\u00fap ng\u01b0\u1eddi h\u1ecdc t\u1eadp trung v\u00e0o kh\u00e1i ni\u1ec7m m\u1ea1ng n\u01a1-ron thay v\u00ec b\u1ecb sa \u0111\u00e0 v\u00e0o c\u00e1c chi ti\u1ebft ph\u1ee5.</li> <li>T\u1ea1i sao c\u1ea7n n\u00f3?: N\u00f3 cung c\u1ea5p c\u00e1c h\u00e0m ti\u1ec7n \u00edch, nh\u01b0 t\u1ea1o d\u1eef li\u1ec7u m\u1eabu (<code>spiral_data</code>) v\u00e0 kh\u1edfi t\u1ea1o m\u00f4i tr\u01b0\u1eddng (<code>init</code>) \u0111\u1ec3 \u0111\u1ea3m b\u1ea3o k\u1ebft qu\u1ea3 c\u1ee7a m\u1ecdi ng\u01b0\u1eddi \u0111\u1ec1u gi\u1ed1ng nhau, d\u1ec5 d\u00e0ng cho vi\u1ec7c h\u1ecdc v\u00e0 g\u1ee1 l\u1ed7i.</li> </ul> </li> <li> <p><code>import matplotlib.pyplot as plt</code>:</p> <ul> <li>N\u00f3 l\u00e0 g\u00ec?: <code>Matplotlib</code> l\u00e0 th\u01b0 vi\u1ec7n tr\u1ef1c quan h\u00f3a d\u1eef li\u1ec7u (v\u1ebd \u0111\u1ed3 th\u1ecb) ph\u1ed5 bi\u1ebfn nh\u1ea5t trong Python. <code>pyplot</code> l\u00e0 m\u1ed9t module trong Matplotlib cung c\u1ea5p giao di\u1ec7n gi\u1ed1ng nh\u01b0 MATLAB.</li> <li>T\u1ea1i sao c\u1ea7n n\u00f3?: \"Tr\u0103m nghe kh\u00f4ng b\u1eb1ng m\u1ed9t th\u1ea5y\". Th\u01b0 vi\u1ec7n n\u00e0y cho ph\u00e9p ch\u00fang ta v\u1ebd d\u1eef li\u1ec7u l\u00ean bi\u1ec3u \u0111\u1ed3 \u0111\u1ec3 xem n\u00f3 tr\u00f4ng nh\u01b0 th\u1ebf n\u00e0o. Vi\u1ec7c nh\u00ecn th\u1ea5y d\u1eef li\u1ec7u h\u00ecnh xo\u1eafn \u1ed1c gi\u00fap ta hi\u1ec3u r\u00f5 h\u01a1n b\u00e0i to\u00e1n m\u00e0 m\u1ea1ng n\u01a1-ron \u0111ang c\u1ed1 g\u1eafng gi\u1ea3i quy\u1ebft.</li> </ul> </li> <li> <p><code>from nnfs.datasets import spiral_data</code>:</p> <ul> <li>N\u00f3 l\u00e0 g\u00ec?: \u0110\u00e2y l\u00e0 m\u1ed9t l\u1ec7nh <code>import</code> c\u1ee5 th\u1ec3. Thay v\u00ec nh\u1eadp c\u1ea3 th\u01b0 vi\u1ec7n <code>nnfs.datasets</code>, ch\u00fang ta ch\u1ec9 l\u1ea5y ri\u00eang h\u00e0m <code>spiral_data</code> t\u1eeb \u0111\u00f3.</li> <li>T\u1ea1i sao c\u1ea7n n\u00f3?: <code>spiral_data</code> l\u00e0 m\u1ed9t h\u00e0m gi\u00fap t\u1ea1o ra b\u1ed9 d\u1eef li\u1ec7u h\u00ecnh xo\u1eafn \u1ed1c n\u1ed5i ti\u1ebfng, m\u1ed9t b\u00e0i to\u00e1n kinh \u0111i\u1ec3n \u0111\u1ec3 ki\u1ec3m tra kh\u1ea3 n\u0103ng c\u1ee7a c\u00e1c m\u00f4 h\u00ecnh ph\u00e2n lo\u1ea1i.</li> </ul> </li> <li> <p><code>nnfs.init()</code>:</p> <ul> <li>N\u00f3 l\u00e0 g\u00ec?: L\u1ec7nh n\u00e0y g\u1ecdi h\u00e0m <code>init</code> t\u1eeb th\u01b0 vi\u1ec7n <code>nnfs</code>.</li> <li>T\u1ea1i sao c\u1ea7n n\u00f3?: H\u00e0m n\u00e0y th\u1ef1c hi\u1ec7n m\u1ed9t s\u1ed1 c\u00e0i \u0111\u1eb7t n\u1ec1n, quan tr\u1ecdng nh\u1ea5t l\u00e0 c\u1ed1 \u0111\u1ecbnh seed cho vi\u1ec7c sinh s\u1ed1 ng\u1eabu nhi\u00ean c\u1ee7a NumPy v\u00e0 thi\u1ebft l\u1eadp ki\u1ec3u d\u1eef li\u1ec7u m\u1eb7c \u0111\u1ecbnh. \u0110i\u1ec1u n\u00e0y \u0111\u1ea3m b\u1ea3o r\u1eb1ng m\u1ed7i khi b\u1ea1n ch\u1ea1y l\u1ea1i m\u00e3, c\u00e1c \"tr\u1ecdng s\u1ed1 ng\u1eabu nhi\u00ean\" v\u00e0 \"d\u1eef li\u1ec7u\" \u0111\u01b0\u1ee3c t\u1ea1o ra s\u1ebd lu\u00f4n gi\u1ed1ng h\u1ec7t nhau, gi\u00fap vi\u1ec7c h\u1ecdc v\u00e0 t\u00e1i t\u1ea1o k\u1ebft qu\u1ea3 tr\u1edf n\u00ean nh\u1ea5t qu\u00e1n.</li> </ul> </li> </ul>"},{"location":"vi/Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#phan-2-xay-dung-ban-thiet-ke-cua-mot-vi-giam-khao-class-layer_dense","title":"Ph\u1ea7n 2: X\u00e2y d\u1ef1ng \"B\u1ea3n thi\u1ebft k\u1ebf c\u1ee7a m\u1ed9t V\u1ecb Gi\u00e1m kh\u1ea3o\" (<code>class Layer_Dense</code>)","text":"<p>\u0110\u00e2y l\u00e0 tr\u00e1i tim c\u1ee7a ch\u01b0\u01a1ng tr\u00ecnh. Ch\u00fang ta kh\u00f4ng x\u00e2y d\u1ef1ng m\u1ed9t n\u01a1-ron ri\u00eang l\u1ebb, m\u00e0 l\u00e0 m\u1ed9t \"b\u1ea3n thi\u1ebft k\u1ebf\" (<code>class</code>) \u0111\u1ec3 c\u00f3 th\u1ec3 t\u1ea1o ra c\u1ea3 m\u1ed9t l\u1edbp/m\u1ed9t ban gi\u00e1m kh\u1ea3o m\u1ed9t c\u00e1ch d\u1ec5 d\u00e0ng.</p> Python<pre><code>class Layer_Dense:\n        def __init__(self, n_inputs, n_neurons):\n            self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n            self.biases = np.zeros((1, n_neurons))\n        def forward(self, inputs):\n            self.output = np.dot(inputs, self.weights) + self.biases\n</code></pre>"},{"location":"vi/Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#giai-thich-chi-tiet-tung-phan","title":"Gi\u1ea3i th\u00edch chi ti\u1ebft t\u1eebng ph\u1ea7n:","text":"<ul> <li> <p><code>class Layer_Dense:</code>: Khai b\u00e1o m\u1ed9t \"b\u1ea3n thi\u1ebft k\u1ebf\" t\u00ean l\u00e0 <code>Layer_Dense</code>. M\u1ecdi th\u1ee9 b\u00ean trong n\u00f3 s\u1ebd \u0111\u1ecbnh ngh\u0129a c\u00e1c thu\u1ed9c t\u00ednh v\u00e0 h\u00e0nh vi c\u1ee7a m\u1ed9t l\u1edbp n\u01a1-ron d\u00e0y \u0111\u1eb7c.</p> </li> <li> <p><code>def __init__(self, n_inputs, n_neurons):</code>: H\u00e0m kh\u1edfi t\u1ea1o (constructor).</p> <ul> <li>N\u00f3 l\u00e0m g\u00ec?: H\u00e0m n\u00e0y \u0111\u01b0\u1ee3c t\u1ef1 \u0111\u1ed9ng g\u1ecdi m\u1ed7i khi m\u1ed9t \u0111\u1ed1i t\u01b0\u1ee3ng m\u1edbi \u0111\u01b0\u1ee3c t\u1ea1o ra t\u1eeb b\u1ea3n thi\u1ebft k\u1ebf n\u00e0y (v\u00ed d\u1ee5 <code>dense1 = Layer_Dense(...)</code>). N\u00f3 d\u00f9ng \u0111\u1ec3 thi\u1ebft l\u1eadp c\u00e1c thu\u1ed9c t\u00ednh ban \u0111\u1ea7u.</li> <li><code>self</code>: \u0110\u1ea1i di\u1ec7n cho ch\u00ednh \u0111\u1ed1i t\u01b0\u1ee3ng s\u1ebd \u0111\u01b0\u1ee3c t\u1ea1o ra. Khi b\u1ea1n g\u1ecdi <code>dense1.weights</code>, <code>self</code> ch\u00ednh l\u00e0 <code>dense1</code>.</li> <li><code>n_inputs</code>: S\u1ed1 l\u01b0\u1ee3ng \u0111\u1eb7c tr\u01b0ng \u0111\u1ea7u v\u00e0o m\u00e0 l\u1edbp n\u00e0y s\u1ebd nh\u1eadn (v\u00ed d\u1ee5: 2 \u0111\u1eb7c tr\u01b0ng l\u00e0 \"\u0110\u1ed9 \u0111\u1ecf\" v\u00e0 \"\u0110\u1ed9 tr\u00f2n\" c\u1ee7a hoa qu\u1ea3).</li> <li><code>n_neurons</code>: S\u1ed1 l\u01b0\u1ee3ng n\u01a1-ron trong l\u1edbp n\u00e0y (v\u00ed d\u1ee5: 3 gi\u00e1m kh\u1ea3o, m\u1ed7i ng\u01b0\u1eddi cho m\u1ed9t lo\u1ea1i qu\u1ea3).</li> <li><code>self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)</code>: \u0110\u00e2y l\u00e0 d\u00f2ng c\u1ef1c k\u1ef3 quan tr\u1ecdng.<ul> <li><code>np.random.randn(n_inputs, n_neurons)</code>: T\u1ea1o m\u1ed9t ma tr\u1eadn c\u00f3 k\u00edch th\u01b0\u1edbc <code>(s\u1ed1_\u0111\u1ea7u_v\u00e0o, s\u1ed1_n\u01a1_ron)</code> ch\u1ee9a \u0111\u1ea7y c\u00e1c s\u1ed1 ng\u1eabu nhi\u00ean theo ph\u00e2n ph\u1ed1i chu\u1ea9n (ph\u00e2n ph\u1ed1i Gauss, c\u00f3 gi\u00e1 tr\u1ecb trung b\u00ecnh l\u00e0 0 v\u00e0 ph\u01b0\u01a1ng sai l\u00e0 1). \u0110\u00e2y ch\u00ednh l\u00e0 \"s\u1ef1 \u01b0u ti\u00ean\" ban \u0111\u1ea7u, ho\u00e0n to\u00e0n ng\u1eabu nhi\u00ean c\u1ee7a c\u00e1c v\u1ecb gi\u00e1m kh\u1ea3o.</li> <li><code>* 0.01</code>: Nh\u00e2n t\u1ea5t c\u1ea3 c\u00e1c tr\u1ecdng s\u1ed1 ng\u1eabu nhi\u00ean v\u1edbi m\u1ed9t s\u1ed1 r\u1ea5t nh\u1ecf. \u0110\u00e2y l\u00e0 m\u1ed9t k\u1ef9 thu\u1eadt ph\u1ed5 bi\u1ebfn \u0111\u1ec3 ng\u0103n c\u00e1c gi\u00e1 tr\u1ecb \u0111\u1ea7u ra ban \u0111\u1ea7u qu\u00e1 l\u1edbn, gi\u00fap qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n sau n\u00e0y \u1ed5n \u0111\u1ecbnh h\u01a1n.</li> </ul> </li> <li><code>self.biases = np.zeros((1, n_neurons))</code>:<ul> <li><code>np.zeros((1, n_neurons))</code>: T\u1ea1o m\u1ed9t ma tr\u1eadn h\u00e0ng (vector) c\u00f3 k\u00edch th\u01b0\u1edbc <code>(1, s\u1ed1_n\u01a1_ron)</code> ch\u1ee9a to\u00e0n s\u1ed1 0. \u0110\u00e2y l\u00e0 \"th\u00e0nh ki\u1ebfn\" hay \"t\u00e2m tr\u1ea1ng\" ban \u0111\u1ea7u c\u1ee7a c\u00e1c v\u1ecb gi\u00e1m kh\u1ea3o. Vi\u1ec7c kh\u1edfi t\u1ea1o b\u1eb1ng 0 c\u00f3 ngh\u0129a l\u00e0 ban \u0111\u1ea7u, h\u1ecd kh\u00f4ng c\u00f3 b\u1ea5t k\u1ef3 thi\u00ean v\u1ecb n\u00e0o.</li> </ul> </li> </ul> </li> <li> <p><code>def forward(self, inputs):</code>: Ph\u01b0\u01a1ng th\u1ee9c h\u00e0nh \u0111\u1ed9ng.</p> <ul> <li>N\u00f3 l\u00e0m g\u00ec?: \u0110\u1ecbnh ngh\u0129a h\u00e0nh vi ch\u00ednh c\u1ee7a l\u1edbp: nh\u1eadn d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o v\u00e0 t\u00ednh to\u00e1n \u0111\u1ea7u ra. Qu\u00e1 tr\u00ecnh n\u00e0y \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 truy\u1ec1n xu\u00f4i (forward pass).</li> <li><code>inputs</code>: D\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o s\u1ebd \u0111\u01b0\u1ee3c \u0111\u01b0a v\u00e0o l\u1edbp (v\u00ed d\u1ee5: danh s\u00e1ch c\u00e1c \u0111\u1eb7c \u0111i\u1ec3m c\u1ee7a t\u1ea5t c\u1ea3 hoa qu\u1ea3).</li> <li><code>self.output = np.dot(inputs, self.weights) + self.biases</code>: C\u00f4ng th\u1ee9c to\u00e1n h\u1ecdc c\u1ed1t l\u00f5i.<ul> <li><code>np.dot(inputs, self.weights)</code>: Ph\u00e9p nh\u00e2n ma tr\u1eadn. \u0110\u00e2y l\u00e0 l\u00fac m\u1ed7i gi\u00e1m kh\u1ea3o \"nh\u00ecn\" v\u00e0o c\u00e1c \u0111\u1eb7c \u0111i\u1ec3m c\u1ee7a hoa qu\u1ea3 v\u00e0 nh\u00e2n ch\u00fang v\u1edbi \"s\u1ef1 \u01b0u ti\u00ean\" (tr\u1ecdng s\u1ed1) c\u1ee7a m\u00ecnh \u0111\u1ec3 \u0111\u01b0a ra m\u1ed9t \u0111i\u1ec3m s\u1ed1 s\u01a1 b\u1ed9.</li> <li><code>+ self.biases</code>: C\u1ed9ng th\u00eam \"th\u00e0nh ki\u1ebfn\" (thi\u00ean v\u1ecb) c\u1ee7a m\u1ed7i gi\u00e1m kh\u1ea3o v\u00e0o \u0111i\u1ec3m s\u1ed1 c\u1ee7a h\u1ecd.</li> <li><code>self.output = ...</code>: K\u1ebft qu\u1ea3 cu\u1ed1i c\u00f9ng \u0111\u01b0\u1ee3c l\u01b0u v\u00e0o thu\u1ed9c t\u00ednh <code>output</code> c\u1ee7a l\u1edbp.</li> </ul> </li> </ul> </li> </ul>"},{"location":"vi/Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#phan-3-cuoc-thi-bat-au-su-dung-lop-va-du-lieu","title":"Ph\u1ea7n 3: Cu\u1ed9c thi b\u1eaft \u0111\u1ea7u! (S\u1eed d\u1ee5ng l\u1edbp v\u00e0 d\u1eef li\u1ec7u)","text":"<p>B\u00e2y gi\u1edd ch\u00fang ta s\u1ebd s\u1eed d\u1ee5ng \"b\u1ea3n thi\u1ebft k\u1ebf\" v\u00e0 \"nguy\u00ean li\u1ec7u\" \u0111\u00e3 chu\u1ea9n b\u1ecb \u1edf tr\u00ean \u0111\u1ec3 ti\u1ebfn h\u00e0nh m\u1ed9t cu\u1ed9c thi th\u1ef1c s\u1ef1.</p> Python<pre><code># Create dataset\nX, y = spiral_data(samples=100, classes=3)\n# Visualize dataset\nplt.scatter(X[:,0], X[:,1], c=y, cmap='brg')\nplt.show()\n</code></pre> <ul> <li><code>X, y = spiral_data(samples=100, classes=3)</code>: G\u1ecdi h\u00e0m \u0111\u00e3 nh\u1eadp \u0111\u1ec3 t\u1ea1o d\u1eef li\u1ec7u.<ul> <li><code>X</code>: S\u1ebd l\u00e0 m\u1ed9t m\u1ea3ng NumPy k\u00edch th\u01b0\u1edbc <code>(300, 2)</code>. 300 l\u00e0 v\u00ec c\u00f3 3 l\u1edbp (<code>classes</code>), m\u1ed7i l\u1edbp 100 m\u1eabu (<code>samples</code>). 2 l\u00e0 v\u00ec m\u1ed7i m\u1eabu c\u00f3 2 \u0111\u1eb7c tr\u01b0ng (t\u1ecda \u0111\u1ed9 x, y). \u0110\u00e2y l\u00e0 \"danh s\u00e1ch c\u00e1c th\u00ed sinh hoa qu\u1ea3 v\u00e0 \u0111\u1eb7c \u0111i\u1ec3m c\u1ee7a ch\u00fang\".</li> <li><code>y</code>: S\u1ebd l\u00e0 m\u1ed9t m\u1ea3ng NumPy k\u00edch th\u01b0\u1edbc <code>(300,)</code> ch\u1ee9a c\u00e1c nh\u00e3n <code>0, 1, 2</code>. \u0110\u00e2y l\u00e0 \"\u0111\u00e1p \u00e1n \u0111\u00fang\" cho m\u1ed7i th\u00ed sinh (T\u00e1o, Cam, hay Chu\u1ed1i).</li> </ul> </li> <li><code>plt.scatter(X[:,0], X[:,1], c=y, cmap='brg')</code>: Chu\u1ea9n b\u1ecb v\u1ebd \u0111\u1ed3 th\u1ecb.<ul> <li><code>X[:,0]</code>: L\u1ea5y t\u1ea5t c\u1ea3 c\u00e1c h\u00e0ng, c\u1ed9t \u0111\u1ea7u ti\u00ean (t\u1ea5t c\u1ea3 t\u1ecda \u0111\u1ed9 x).</li> <li><code>X[:,1]</code>: L\u1ea5y t\u1ea5t c\u1ea3 c\u00e1c h\u00e0ng, c\u1ed9t th\u1ee9 hai (t\u1ea5t c\u1ea3 t\u1ecda \u0111\u1ed9 y).</li> <li><code>c=y</code>: <code>c</code> l\u00e0 vi\u1ebft t\u1eaft c\u1ee7a color. L\u1ec7nh n\u00e0y b\u1ea3o Matplotlib h\u00e3y t\u00f4 m\u00e0u cho m\u1ed7i \u0111i\u1ec3m <code>(x, y)</code> d\u1ef1a tr\u00ean gi\u00e1 tr\u1ecb t\u01b0\u01a1ng \u1ee9ng trong m\u1ea3ng <code>y</code>. C\u00e1c \u0111i\u1ec3m c\u00f3 <code>y=0</code> s\u1ebd c\u00f9ng m\u00e0u, <code>y=1</code> c\u00f9ng m\u00e0u kh\u00e1c,...</li> <li><code>cmap='brg'</code>: Color map. Ch\u1ecdn b\u1ea3ng m\u00e0u Xanh-\u0110\u1ecf-L\u00e1 (Blue-Red-Green).</li> </ul> </li> <li><code>plt.show()</code>: Hi\u1ec3n th\u1ecb \u0111\u1ed3 th\u1ecb \u0111\u00e3 chu\u1ea9n b\u1ecb l\u00ean m\u00e0n h\u00ecnh.</li> </ul> Python<pre><code># Create Dense layer with 2 input features and 3 output values\ndense1 = Layer_Dense(2, 3)\n</code></pre> <ul> <li>\u0110\u00e2y l\u00e0 l\u00fac ch\u00fang ta t\u1ea1o ra m\u1ed9t \u0111\u1ed1i t\u01b0\u1ee3ng t\u1eeb b\u1ea3n thi\u1ebft k\u1ebf <code>Layer_Dense</code>. Ch\u00fang ta \u0111ang \"thu\u00ea m\u1ed9t ban gi\u00e1m kh\u1ea3o\".</li> <li><code>dense1 = ...</code>: T\u1ea1o m\u1ed9t ban gi\u00e1m kh\u1ea3o c\u1ee5 th\u1ec3 t\u00ean l\u00e0 <code>dense1</code>.</li> <li><code>Layer_Dense(2, 3)</code>: G\u1ecdi h\u00e0m <code>__init__</code>.<ul> <li><code>n_inputs=2</code>: V\u00ec m\u1ed7i \"th\u00ed sinh hoa qu\u1ea3\" (<code>X</code>) c\u00f3 2 \u0111\u1eb7c tr\u01b0ng (t\u1ecda \u0111\u1ed9 x, y).</li> <li><code>n_neurons=3</code>: V\u00ec ch\u00fang ta c\u1ea7n ph\u00e2n lo\u1ea1i th\u00e0nh 3 lo\u1ea1i qu\u1ea3 (3 l\u1edbp trong <code>y</code>). Ch\u00fang ta c\u1ea7n 3 gi\u00e1m kh\u1ea3o, m\u1ed7i ng\u01b0\u1eddi chuy\u00ean v\u1ec1 m\u1ed9t lo\u1ea1i.</li> </ul> </li> </ul> Python<pre><code># Let's see initial weights and biases\nprint(\"&gt;&gt;&gt; Initial weights and biases of the first layer:\")\nprint(dense1.weights)\nprint(dense1.biases)\n</code></pre> <ul> <li>In ra c\u00e1c thu\u1ed9c t\u00ednh <code>weights</code> v\u00e0 <code>biases</code> c\u1ee7a \u0111\u1ed1i t\u01b0\u1ee3ng <code>dense1</code> v\u1eeba t\u1ea1o. \u0110i\u1ec1u n\u00e0y cho ch\u00fang ta th\u1ea5y \"s\u1ef1 \u01b0u ti\u00ean\" v\u00e0 \"th\u00e0nh ki\u1ebfn\" ban \u0111\u1ea7u, ho\u00e0n to\u00e0n ng\u1eabu nhi\u00ean c\u1ee7a ban gi\u00e1m kh\u1ea3o tr\u01b0\u1edbc khi h\u1ecd ch\u1ea5m \u0111i\u1ec3m b\u1ea5t k\u1ef3 th\u00ed sinh n\u00e0o.</li> </ul> Python<pre><code># Perform a forward pass of our training data through this layer\ndense1.forward(X)\n</code></pre> <ul> <li>\u0110\u00e2y l\u00e0 kho\u1ea3nh kh\u1eafc h\u00e0nh \u0111\u1ed9ng. Ch\u00fang ta g\u1ecdi ph\u01b0\u01a1ng th\u1ee9c <code>forward</code> c\u1ee7a <code>dense1</code> v\u00e0 \u0111\u01b0a to\u00e0n b\u1ed9 \"danh s\u00e1ch th\u00ed sinh\" (<code>X</code>) v\u00e0o. Ph\u00e9p t\u00ednh <code>np.dot(X, dense1.weights) + dense1.biases</code> \u0111\u01b0\u1ee3c th\u1ef1c thi. Ban gi\u00e1m kh\u1ea3o b\u1eaft \u0111\u1ea7u ch\u1ea5m \u0111i\u1ec3m.</li> </ul> Python<pre><code># Let's see output of the first few samples:\nprint(\"&gt;&gt;&gt; Output of the first few samples:\")\nprint(dense1.output[:5])\n</code></pre> <ul> <li>Sau khi <code>forward()</code> ch\u1ea1y xong, k\u1ebft qu\u1ea3 \u0111\u01b0\u1ee3c l\u01b0u trong <code>dense1.output</code>.</li> <li><code>dense1.output[:5]</code>: Ch\u00fang ta in ra k\u1ebft qu\u1ea3 ch\u1ea5m \u0111i\u1ec3m cho 5 \"th\u00ed sinh hoa qu\u1ea3\" \u0111\u1ea7u ti\u00ean \u0111\u1ec3 xem th\u1eed. M\u1ed7i h\u00e0ng l\u00e0 m\u1ed9t th\u00ed sinh, m\u1ed7i c\u1ed9t l\u00e0 \u0111i\u1ec3m s\u1ed1 t\u1eeb m\u1ed9t gi\u00e1m kh\u1ea3o. C\u00e1c gi\u00e1 tr\u1ecb n\u00e0y \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 logits.</li> </ul>"},{"location":"vi/Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#phan-4-dien-giai-truu-tuong-cuoc-thi-phan-loai-hoa-qua","title":"Ph\u1ea7n 4: Di\u1ec5n gi\u1ea3i tr\u1eebu t\u01b0\u1ee3ng - Cu\u1ed9c thi ph\u00e2n lo\u1ea1i hoa qu\u1ea3","text":"<p>H\u00e3y k\u1ec3 l\u1ea1i to\u00e0n b\u1ed9 c\u00e2u chuy\u1ec7n m\u1ed9t c\u00e1ch li\u1ec1n m\u1ea1ch:</p> <ol> <li> <p>B\u1ed1i c\u1ea3nh: Ch\u00fang ta t\u1ed5 ch\u1ee9c m\u1ed9t cu\u1ed9c thi \u0111\u1ec3 ph\u00e2n lo\u1ea1i 3 lo\u1ea1i qu\u1ea3: T\u00e1o, Cam, v\u00e0 Chu\u1ed1i.</p> </li> <li> <p>C\u00e1c th\u00ed sinh (<code>X</code>, <code>y</code>): C\u00f3 300 qu\u1ea3 tham gia. V\u1edbi m\u1ed7i qu\u1ea3, ch\u00fang ta d\u00f9ng m\u00e1y \u0111o \u0111\u01b0\u1ee3c 2 \u0111\u1eb7c \u0111i\u1ec3m: \"\u0110\u1ed9 \u0111\u1ecf\" v\u00e0 \"\u0110\u1ed9 tr\u00f2n\" (\u0111\u00e2y l\u00e0 2 c\u1ed9t c\u1ee7a <code>X</code>). Ch\u00fang ta c\u0169ng bi\u1ebft tr\u01b0\u1edbc \u0111\u00e1p \u00e1n m\u1ed7i qu\u1ea3 l\u00e0 g\u00ec (\u0111\u00e2y l\u00e0 <code>y</code>).</p> </li> <li> <p>Thu\u00ea ban gi\u00e1m kh\u1ea3o (<code>dense1 = Layer_Dense(2, 3)</code>): Ch\u00fang ta thu\u00ea m\u1ed9t ban gi\u00e1m kh\u1ea3o g\u1ed3m 3 ng\u01b0\u1eddi:</p> <ul> <li>Gi\u00e1m kh\u1ea3o 1: Chuy\u00ean gia v\u1ec1 T\u00e1o.</li> <li>Gi\u00e1m kh\u1ea3o 2: Chuy\u00ean gia v\u1ec1 Cam.</li> <li>Gi\u00e1m kh\u1ea3o 3: Chuy\u00ean gia v\u1ec1 Chu\u1ed1i. H\u1ecd l\u00e0 nh\u1eefng ng\u01b0\u1eddi m\u1edbi v\u00e0o ngh\u1ec1, n\u00ean \"ki\u1ebfn th\u1ee9c\" c\u1ee7a h\u1ecd ban \u0111\u1ea7u l\u00e0 ng\u1eabu nhi\u00ean.</li> </ul> </li> <li> <p>Ki\u1ebfn th\u1ee9c c\u1ee7a gi\u00e1m kh\u1ea3o (<code>weights</code> v\u00e0 <code>biases</code>):</p> <ul> <li>S\u1ef1 \u01b0u ti\u00ean (<code>weights</code>): M\u1ed7i gi\u00e1m kh\u1ea3o c\u00f3 m\u1ed9t b\u1ed9 \"\u01b0u ti\u00ean\" ri\u00eang cho 2 \u0111\u1eb7c \u0111i\u1ec3m \"\u0110\u1ed9 \u0111\u1ecf\" v\u00e0 \"\u0110\u1ed9 tr\u00f2n\". V\u00ed d\u1ee5, chuy\u00ean gia T\u00e1o l\u00fd t\u01b0\u1edfng s\u1ebd c\u00f3 \u01b0u ti\u00ean cao cho \"\u0110\u1ed9 \u0111\u1ecf\" v\u00e0 \"\u0110\u1ed9 tr\u00f2n\". Chuy\u00ean gia Chu\u1ed1i s\u1ebd c\u00f3 \u01b0u ti\u00ean \u00e2m cho \"\u0110\u1ed9 tr\u00f2n\" (v\u00ec chu\u1ed1i d\u00e0i). Nh\u01b0ng v\u00ec h\u1ecd l\u00e0 ng\u01b0\u1eddi m\u1edbi, c\u00e1c \u01b0u ti\u00ean n\u00e0y \u0111\u01b0\u1ee3c g\u00e1n ng\u1eabu nhi\u00ean (v\u00ed d\u1ee5: chuy\u00ean gia T\u00e1o l\u1ea1i c\u00f3 th\u1ec3 th\u00edch qu\u1ea3 kh\u00f4ng \u0111\u1ecf, chuy\u00ean gia Chu\u1ed1i l\u1ea1i th\u00edch qu\u1ea3 tr\u00f2n).</li> <li>T\u00e2m tr\u1ea1ng (<code>biases</code>): Ban \u0111\u1ea7u, c\u1ea3 3 gi\u00e1m kh\u1ea3o \u0111\u1ec1u c\u00f3 t\u00e2m tr\u1ea1ng trung l\u1eadp (b\u1eb1ng 0).</li> </ul> </li> <li> <p>Qu\u00e1 tr\u00ecnh ch\u1ea5m \u0111i\u1ec3m (<code>dense1.forward(X)</code>):</p> <ul> <li>T\u1eebng qu\u1ea3 m\u1ed9t \u0111\u01b0\u1ee3c \u0111\u01b0a ra tr\u01b0\u1edbc ban gi\u00e1m kh\u1ea3o.</li> <li>M\u1ed7i gi\u00e1m kh\u1ea3o t\u00ednh \u0111i\u1ec3m c\u1ee7a m\u00ecnh theo c\u00f4ng th\u1ee9c:     <code>\u0110i\u1ec3m = (\u0110\u1ed9 \u0111\u1ecf * \u01afu ti\u00ean cho \u0111\u1ed9 \u0111\u1ecf) + (\u0110\u1ed9 tr\u00f2n * \u01afu ti\u00ean cho \u0111\u1ed9 tr\u00f2n) + T\u00e2m tr\u1ea1ng</code></li> <li>Qu\u00e1 tr\u00ecnh n\u00e0y di\u1ec5n ra cho t\u1ea5t c\u1ea3 300 qu\u1ea3.</li> </ul> </li> <li> <p>B\u1ea3ng \u0111i\u1ec3m (<code>dense1.output</code>):</p> <ul> <li>K\u1ebft qu\u1ea3 cu\u1ed1i c\u00f9ng l\u00e0 m\u1ed9t b\u1ea3ng \u0111i\u1ec3m l\u1edbn. M\u1ed7i h\u00e0ng l\u00e0 m\u1ed9t qu\u1ea3, m\u1ed7i c\u1ed9t l\u00e0 \u0111i\u1ec3m s\u1ed1 t\u1eeb m\u1ed9t gi\u00e1m kh\u1ea3o.</li> <li>V\u00ed d\u1ee5, d\u00f2ng \u0111\u1ea7u ti\u00ean c\u00f3 th\u1ec3 l\u00e0 <code>[0.0012, -0.0045, 0.0031]</code>. \u0110i\u1ec1u n\u00e0y c\u00f3 ngh\u0129a l\u00e0 v\u1edbi ki\u1ebfn th\u1ee9c ng\u1eabu nhi\u00ean hi\u1ec7n t\u1ea1i, Gi\u00e1m kh\u1ea3o T\u00e1o cho qu\u1ea3 n\u00e0y 0.0012 \u0111i\u1ec3m, Gi\u00e1m kh\u1ea3o Cam cho -0.0045 \u0111i\u1ec3m, v\u00e0 Gi\u00e1m kh\u1ea3o Chu\u1ed1i cho 0.0031 \u0111i\u1ec3m.</li> </ul> </li> </ol> <p>K\u1ebft lu\u1eadn quan tr\u1ecdng: V\u00ec \"ki\u1ebfn th\u1ee9c\" (weights) c\u1ee7a ban gi\u00e1m kh\u1ea3o l\u00e0 ng\u1eabu nhi\u00ean, n\u00ean \"b\u1ea3ng \u0111i\u1ec3m\" (output) n\u00e0y ho\u00e0n to\u00e0n v\u00f4 ngh\u0129a. Qu\u00e1 tr\u00ecnh \"hu\u1ea5n luy\u1ec7n\" (training), kh\u00f4ng c\u00f3 trong m\u00e3 n\u00e0y, ch\u00ednh l\u00e0 vi\u1ec7c cho ban gi\u00e1m kh\u1ea3o xem \u0111\u00e1p \u00e1n \u0111\u00fang (<code>y</code>), ch\u1ec9 ra l\u1ed7i sai c\u1ee7a h\u1ecd, v\u00e0 gi\u00fap h\u1ecd \u0111i\u1ec1u ch\u1ec9nh l\u1ea1i \"s\u1ef1 \u01b0u ti\u00ean\" (<code>weights</code>) v\u00e0 \"t\u00e2m tr\u1ea1ng\" (<code>biases</code>) qua h\u00e0ng ng\u00e0n l\u1ea7n l\u1eb7p, \u0111\u1ec3 cu\u1ed1i c\u00f9ng b\u1ea3ng \u0111i\u1ec3m c\u1ee7a h\u1ecd ph\u1ea3n \u00e1nh \u0111\u00fang lo\u1ea1i qu\u1ea3.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#phan-5-so-o-minh-hoa-ascii","title":"Ph\u1ea7n 5: S\u01a1 \u0111\u1ed3 minh h\u1ecda (ASCII)","text":"<p>S\u01a1 \u0111\u1ed3 cho m\u1ed9t qu\u1ea3 duy nh\u1ea5t \u0111i qua ban gi\u00e1m kh\u1ea3o:  Text Only<pre><code>        \u0110\u1ea6U V\u00c0O (1 qu\u1ea3)\n        (2 \u0111\u1eb7c tr\u01b0ng)\n        +----------------------+\n        | \u0110\u1ed9 \u0111\u1ecf, \u0110\u1ed9 tr\u00f2n       |\n        +----------------------+\n               |\n               |                                 BAN GI\u00c1M KH\u1ea2O (dense1)\n               |                                 (3 Gi\u00e1m kh\u1ea3o/N\u01a1-ron)\n               |\n               |       \u01afu ti\u00ean (w11, w21)      +--------------------+   (\u0110i\u1ec3m t\u1eeb GK T\u00e1o)\n               +-----------------------------&gt;|  Gi\u00e1m kh\u1ea3o T\u00c1O   + b1|-----&gt; output_1\n               |                             +--------------------+\n               |\n               |       \u01afu ti\u00ean (w12, w22)      +--------------------+   (\u0110i\u1ec3m t\u1eeb GK Cam)\n               +-----------------------------&gt;|  Gi\u00e1m kh\u1ea3o CAM   + b2|-----&gt; output_2\n               |                             +--------------------+\n               |\n               |       \u01afu ti\u00ean (w13, w23)      +--------------------+   (\u0110i\u1ec3m t\u1eeb GK Chu\u1ed1i)\n               +-----------------------------&gt;|  Gi\u00e1m kh\u1ea3o CHU\u1ed0I + b3|-----&gt; output_3\n                                             +--------------------+\n\n\nC\u00f4ng th\u1ee9c t\u00ednh \u0111i\u1ec3m c\u1ee7a Gi\u00e1m kh\u1ea3o T\u00c1O:\noutput_1 = (\u0110\u1ed9 \u0111\u1ecf * w11) + (\u0110\u1ed9 tr\u00f2n * w21) + b1\n\nK\u1ebft qu\u1ea3 cu\u1ed1i c\u00f9ng cho 1 qu\u1ea3 l\u00e0 m\u1ed9t b\u1ed9 3 \u0111i\u1ec3m: [output_1, output_2, output_3]\n</code></pre></p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#phu-luc-giai-thich-spiral-data","title":"Ph\u1ee5 L\u1ee5c Gi\u1ea3i Th\u00edch - Spiral Data :","text":"<p>\u0110\u00e2y l\u00e0 ph\u1ea7n gi\u1ea3i th\u00edch v\u1ec1 \"D\u1eef li\u1ec7u h\u00ecnh xo\u1eafn \u1ed1c\" - nghe c\u00f3 v\u1ebb tr\u1eebu t\u01b0\u1ee3ng, nh\u01b0ng n\u00f3 l\u00e0 m\u1ed9t trong nh\u1eefng b\u1ed9 d\u1eef li\u1ec7u m\u1eabu kinh \u0111i\u1ec3n v\u00e0 quan tr\u1ecdng nh\u1ea5t khi b\u1eaft \u0111\u1ea7u h\u1ecdc v\u1ec1 m\u1ea1ng n\u01a1-ron.</p> <p>H\u00e3y c\u00f9ng ph\u00e2n t\u00edch.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#1-inh-nghia-on-gian","title":"1. \u0110\u1ecbnh ngh\u0129a \u0111\u01a1n gi\u1ea3n","text":"<p>D\u1eef li\u1ec7u h\u00ecnh xo\u1eafn \u1ed1c (Spiral Data) l\u00e0 m\u1ed9t b\u1ed9 d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c t\u1ea1o ra m\u1ed9t c\u00e1ch nh\u00e2n t\u1ea1o, trong \u0111\u00f3 c\u00e1c \u0111i\u1ec3m d\u1eef li\u1ec7u thu\u1ed9c c\u00e1c l\u1edbp kh\u00e1c nhau \u0111\u01b0\u1ee3c s\u1eafp x\u1ebfp th\u00e0nh c\u00e1c h\u00ecnh xo\u1eafn \u1ed1c l\u1ed3ng v\u00e0o nhau.</p> <p>H\u00e3y nh\u00ecn l\u1ea1i ch\u00ednh bi\u1ec3u \u0111\u1ed3 m\u00e0 b\u1ea1n \u0111\u00e3 t\u1ea1o ra:</p> <ul> <li>B\u1ea1n c\u00f3 3 l\u1edbp (classes), t\u01b0\u01a1ng \u1ee9ng v\u1edbi 3 m\u00e0u: \u0110\u1ecf, Xanh l\u00e1, v\u00e0 Xanh d\u01b0\u01a1ng.</li> <li>M\u1ed7i \u0111i\u1ec3m c\u00f3 m\u1ed9t v\u1ecb tr\u00ed (t\u1ecda \u0111\u1ed9 x, y).</li> <li>C\u00e1c \u0111i\u1ec3m c\u00f9ng m\u00e0u t\u1ea1o th\u00e0nh m\u1ed9t \"c\u00e1nh tay\" xo\u1eafn \u1ed1c.</li> <li>C\u00e1c c\u00e1nh tay n\u00e0y \u0111an xen, qu\u1ea5n l\u1ea5y nhau.</li> </ul>"},{"location":"vi/Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#2-tai-sao-no-lai-quan-trong-va-noi-tieng","title":"2. T\u1ea1i sao n\u00f3 l\u1ea1i quan tr\u1ecdng v\u00e0 n\u1ed5i ti\u1ebfng?","text":"<p>L\u00fd do b\u1ed9 d\u1eef li\u1ec7u n\u00e0y \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng r\u1ed9ng r\u00e3i l\u00e0 v\u00ec n\u00f3 l\u00e0 m\u1ed9t th\u1eed th\u00e1ch ho\u00e0n h\u1ea3o \u0111\u1ec3 ch\u1ee9ng minh s\u1ee9c m\u1ea1nh c\u1ee7a m\u1ea1ng n\u01a1-ron.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#a-no-anh-bai-cac-mo-hinh-on-gian-tuyen-tinh","title":"A. N\u00f3 \"\u0111\u00e1nh b\u1ea1i\" c\u00e1c m\u00f4 h\u00ecnh \u0111\u01a1n gi\u1ea3n (Tuy\u1ebfn t\u00ednh)","text":"<p>H\u00e3y t\u01b0\u1edfng t\u01b0\u1ee3ng b\u1ea1n ch\u1ec9 c\u00f3 m\u1ed9t c\u00e2y th\u01b0\u1edbc k\u1ebb. Nhi\u1ec7m v\u1ee5 c\u1ee7a b\u1ea1n l\u00e0 k\u1ebb m\u1ed9t ho\u1eb7c nhi\u1ec1u \u0111\u01b0\u1eddng th\u1eb3ng \u0111\u1ec3 ph\u00e2n chia 3 nh\u00f3m m\u00e0u n\u00e0y ra, sao cho m\u1ed7i v\u00f9ng ch\u1ec9 ch\u1ee9a m\u1ed9t m\u00e0u duy nh\u1ea5t.</p> <p>B\u1ea1n s\u1ebd th\u1ea5y ngay l\u00e0 b\u1ea5t kh\u1ea3 thi.</p> Text Only<pre><code>       /\n      /    &lt;-- B\u1ea1n kh\u00f4ng th\u1ec3 k\u1ebb m\u1ed9t \u0111\u01b0\u1eddng th\u1eb3ng n\u00e0o\n     /         \u0111\u1ec3 t\u00e1ch m\u00e0u \u0110\u1ecf (R) ra kh\u1ecfi Xanh (G) v\u00e0 Xanh d\u01b0\u01a1ng (B)\n    /\n   RRRRR\n  G B R G\n B G R B G\nB B G G B B\n R R B R R\n  R G B R\n   BBBBB\n</code></pre> <p>M\u1ed9t m\u00f4 h\u00ecnh ch\u1ec9 c\u00f3 th\u1ec3 k\u1ebb c\u00e1c \u0111\u01b0\u1eddng th\u1eb3ng \u0111\u1ec3 ph\u00e2n lo\u1ea1i \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 m\u00f4 h\u00ecnh tuy\u1ebfn t\u00ednh (linear model). D\u1eef li\u1ec7u xo\u1eafn \u1ed1c l\u00e0 m\u1ed9t v\u00ed d\u1ee5 kinh \u0111i\u1ec3n c\u1ee7a d\u1eef li\u1ec7u phi tuy\u1ebfn (non-linear), n\u01a1i ranh gi\u1edbi gi\u1eefa c\u00e1c l\u1edbp kh\u00f4ng ph\u1ea3i l\u00e0 \u0111\u01b0\u1eddng th\u1eb3ng m\u00e0 l\u00e0 nh\u1eefng \u0111\u01b0\u1eddng cong ph\u1ee9c t\u1ea1p.</p> <p>N\u00f3i c\u00e1ch kh\u00e1c, d\u1eef li\u1ec7u xo\u1eafn \u1ed1c \u0111\u01b0\u1ee3c t\u1ea1o ra \u0111\u1ec3 c\u1ed1 t\u00ecnh l\u00e0m kh\u00f3 c\u00e1c thu\u1eadt to\u00e1n ph\u00e2n lo\u1ea1i \u0111\u01a1n gi\u1ea3n.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#b-no-chung-to-su-can-thiet-cua-mang-no-ron","title":"B. N\u00f3 ch\u1ee9ng t\u1ecf s\u1ef1 c\u1ea7n thi\u1ebft c\u1ee7a M\u1ea1ng N\u01a1-ron","text":"<p>M\u1ea1ng n\u01a1-ron, \u0111\u1eb7c bi\u1ec7t l\u00e0 c\u00e1c m\u1ea1ng c\u00f3 c\u00e1c l\u1edbp \u1ea9n (hidden layers) v\u00e0 c\u00e1c h\u00e0m k\u00edch ho\u1ea1t phi tuy\u1ebfn (s\u1ebd h\u1ecdc sau), c\u00f3 kh\u1ea3 n\u0103ng h\u1ecdc \u0111\u01b0\u1ee3c c\u00e1c ranh gi\u1edbi quy\u1ebft \u0111\u1ecbnh (decision boundaries) c\u1ef1c k\u1ef3 ph\u1ee9c t\u1ea1p v\u00e0 u\u1ed1n l\u01b0\u1ee3n.</p> <p>M\u1ed9t m\u1ea1ng n\u01a1-ron \u0111\u01b0\u1ee3c hu\u1ea5n luy\u1ec7n t\u1ed1t c\u00f3 th\u1ec3 t\u1ea1o ra m\u1ed9t ranh gi\u1edbi tr\u00f4ng gi\u1ed1ng nh\u01b0 th\u1ebf n\u00e0y:</p> <p>N\u00f3 kh\u00f4ng d\u00f9ng \"th\u01b0\u1edbc k\u1ebb\", m\u00e0 n\u00f3 h\u1ecdc c\u00e1ch \"v\u1ebd\" ra nh\u1eefng \u0111\u01b0\u1eddng cong m\u1ec1m m\u1ea1i \u0111\u1ec3 bao quanh t\u1eebng nh\u00f3m d\u1eef li\u1ec7u m\u1ed9t c\u00e1ch ho\u00e0n h\u1ea3o.</p> <p>K\u1ebft lu\u1eadn: D\u1eef li\u1ec7u xo\u1eafn \u1ed1c l\u00e0 m\u1ed9t b\u00e0i ki\u1ec3m tra \"t\u1ed1t nghi\u1ec7p\" cho m\u1ed9t m\u00f4 h\u00ecnh ph\u00e2n lo\u1ea1i. N\u1ebfu m\u00f4 h\u00ecnh c\u1ee7a b\u1ea1n c\u00f3 th\u1ec3 gi\u1ea3i quy\u1ebft \u0111\u01b0\u1ee3c b\u00e0i to\u00e1n n\u00e0y, n\u00f3 ch\u1ee9ng t\u1ecf r\u1eb1ng n\u00f3 c\u00f3 kh\u1ea3 n\u0103ng x\u1eed l\u00fd c\u00e1c m\u1ed1i quan h\u1ec7 ph\u1ee9c t\u1ea1p, phi tuy\u1ebfn trong d\u1eef li\u1ec7u, \u0111i\u1ec1u m\u00e0 c\u00e1c m\u00f4 h\u00ecnh \u0111\u01a1n gi\u1ea3n kh\u00f4ng l\u00e0m \u0111\u01b0\u1ee3c.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.01_First_Layer/01-first-layer/#3-ham-spiral_data-tao-ra-cai-gi","title":"3. H\u00e0m <code>spiral_data</code> t\u1ea1o ra c\u00e1i g\u00ec?","text":"<p>Khi b\u1ea1n g\u1ecdi <code>X, y = spiral_data(samples=100, classes=3)</code>, h\u00e0m n\u00e0y s\u1ebd t\u00ednh to\u00e1n v\u00e0 tr\u1ea3 v\u1ec1 hai th\u1ee9:</p> <ol> <li> <p><code>X</code> (C\u00e1c \u0111\u1eb7c tr\u01b0ng - The Features):</p> <ul> <li>L\u00e0 m\u1ed9t m\u1ea3ng NumPy ch\u1ee9a t\u1ecda \u0111\u1ed9 <code>[x, y]</code> c\u1ee7a t\u1ea5t c\u1ea3 c\u00e1c \u0111i\u1ec3m.</li> <li>V\u1edbi <code>samples=100</code> v\u00e0 <code>classes=3</code>, n\u00f3 s\u1ebd t\u1ea1o ra <code>100 * 3 = 300</code> \u0111i\u1ec3m.</li> <li>V\u00ec v\u1eady, <code>X</code> s\u1ebd c\u00f3 k\u00edch th\u01b0\u1edbc l\u00e0 <code>(300, 2)</code>.</li> <li>Trong c\u00e2u chuy\u1ec7n \"gi\u00e1m kh\u1ea3o hoa qu\u1ea3\" c\u1ee7a ch\u00fang ta, <code>X</code> t\u01b0\u01a1ng \u0111\u01b0\u01a1ng v\u1edbi m\u1ed9t danh s\u00e1ch 300 qu\u1ea3, m\u1ed7i qu\u1ea3 c\u00f3 2 \u0111\u1eb7c \u0111i\u1ec3m l\u00e0 \"\u0110\u1ed9 \u0111\u1ecf\" v\u00e0 \"\u0110 \u0110\u1ed9 tr\u00f2n\".</li> </ul> </li> <li> <p><code>y</code> (C\u00e1c nh\u00e3n - The Labels):</p> <ul> <li>L\u00e0 m\u1ed9t m\u1ea3ng NumPy ch\u1ee9a nh\u00e3n l\u1edbp cho m\u1ed7i \u0111i\u1ec3m t\u01b0\u01a1ng \u1ee9ng trong <code>X</code>.</li> <li>N\u00f3 s\u1ebd ch\u1ee9a 300 con s\u1ed1, bao g\u1ed3m 100 s\u1ed1 <code>0</code>, 100 s\u1ed1 <code>1</code>, v\u00e0 100 s\u1ed1 <code>2</code>.</li> <li><code>y[i]</code> l\u00e0 nh\u00e3n (\u0111\u00e1p \u00e1n \u0111\u00fang) cho \u0111i\u1ec3m <code>X[i]</code>.</li> <li>Trong c\u00e2u chuy\u1ec7n c\u1ee7a ch\u00fang ta, <code>y</code> l\u00e0 danh s\u00e1ch \u0111\u00e1p \u00e1n \u0111\u00fang: qu\u1ea3 n\u00e0o l\u00e0 \"T\u00e1o\" (l\u1edbp 0), qu\u1ea3 n\u00e0o l\u00e0 \"Cam\" (l\u1edbp 1), qu\u1ea3 n\u00e0o l\u00e0 \"Chu\u1ed1i\" (l\u1edbp 2).</li> </ul> </li> </ol> <p>V\u00ec v\u1eady, <code>spiral_data</code> kh\u00f4ng ch\u1ec9 l\u00e0 m\u1ed9t b\u1ed9 d\u1eef li\u1ec7u, m\u00e0 n\u00f3 l\u00e0 m\u1ed9t b\u00e0i to\u00e1n ph\u00e2n lo\u1ea1i phi tuy\u1ebfn kinh \u0111i\u1ec3n \u0111\u01b0\u1ee3c \u0111\u00f3ng g\u00f3i s\u1eb5n \u0111\u1ec3 b\u1ea1n c\u00f3 th\u1ec3 nhanh ch\u00f3ng ki\u1ec3m tra m\u00f4 h\u00ecnh c\u1ee7a m\u00ecnh.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu-diagram/","title":"ReLU Diagram","text":""},{"location":"vi/Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu-diagram/#so-o-ascii-tong-hop-tu-iem-du-lieu-en-kich-hoat-relu","title":"S\u01a1 \u0111\u1ed3 ASCII T\u1ed5ng h\u1ee3p: T\u1eeb \u0110i\u1ec3m D\u1eef li\u1ec7u \u0111\u1ebfn K\u00edch ho\u1ea1t ReLU","text":"<p>S\u01a1 \u0111\u1ed3 n\u00e0y s\u1ebd theo d\u00f5i m\u1ed9t \u0111i\u1ec3m d\u1eef li\u1ec7u duy nh\u1ea5t <code>O(0.5, 1.0)</code> qua to\u00e0n b\u1ed9 qu\u00e1 tr\u00ecnh: L\u1edbp Dense 1 -&gt; H\u00e0m k\u00edch ho\u1ea1t ReLU 1.</p> Text Only<pre><code>\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551             S\u01a0 \u0110\u1ed2 D\u00d2NG CH\u1ea2Y HO\u00c0N CH\u1ec8NH: BI\u1ebeN \u0110\u1ed4I TUY\u1ebeN T\u00cdNH &amp; K\u00cdCH HO\u1ea0T PHI TUY\u1ebeN (RELU)                      \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\n    (B\u01af\u1edaC 1: \u0110\u1ea6U V\u00c0O)\n    M\u1ed9t \u0111i\u1ec3m d\u1eef li\u1ec7u trong kh\u00f4ng gian 2D\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502     Input: O(x, y)      \u2502\n    \u2502      [0.5, 1.0]         \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n                \u25bc\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551   (B\u01af\u1edaC 2: PH\u00c9P BI\u1ebeN \u0110\u1ed4I TUY\u1ebeN T\u00cdNH - L\u1edaP DENSE 1) - `output = v \u00b7 W + b`                                     \u2551\n\u2551   M\u1ee5c ti\u00eau: Chi\u1ebfu \u0111i\u1ec3m O t\u1eeb kh\u00f4ng gian 2D sang kh\u00f4ng gian 3D m\u1edbi.                                            \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                                                                                    \u2502\n    \u25bc (2a: Ph\u00e9p nh\u00e2n Dot Product)                                                        \u25bc (Th\u00f4ng s\u1ed1 c\u1ee7a L\u1edbp)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 [0.5, 1.0]\u2502         \u2502      Weights: W         \u2502                            \u2502       Biases: b         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518         \u2502    (Ma tr\u1eadn 2x3)        \u2502                            \u2502     (Vector 1x3)        \u2502\n      \u2502               \u2502 [[ 0.2, 0.8,-0.5],     \u2502                            \u2502     [[2.0, 3.0, 0.5]]   \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502  [-0.9, 0.2, 0.4]]      \u2502                            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                          \u2502\n                  \u25bc                                                                        \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502          T\u00cdNH TO\u00c1N CHI TI\u1ebeT CHO T\u1eeaNG NEURON (M\u1ed7i neuron t\u1ea1o ra m\u1ed9t chi\u1ec1u m\u1edbi)            \u2502 \u2502\n\u2502                                                                                        \u2502 \u2502\n\u2502  Neuron 0: (0.5 * 0.2) + (1.0 * -0.9) = -0.8  &lt;\u2500\u2500\u2500\u2510                                    \u2502 \u2502\n\u2502  Neuron 1: (0.5 * 0.8) + (1.0 *  0.2) =  0.6  &lt;\u2500\u2500\u2500\u253c\u2500\u2500\u2510                                 \u2502 \u2502\n\u2502  Neuron 2: (0.5 *-0.5) + (1.0 *  0.4) =  0.15 &lt;\u2500\u2500\u2500\u253c\u2500\u2500\u253c\u2500\u2500\u2510                              \u2502 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502  \u2502  \u2502                              \u2502\n                                                    \u2502  \u2502  \u2502                              \u2502\n    (K\u1ebft qu\u1ea3 v \u00b7 W)                                 \u25bc  \u25bc  \u25bc                              \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                          \u2502\n    \u2502  [-0.8,  0.6,   0.15]    \u2502               \u2502 (2b: C\u1ed9ng Bias) \u2502                          \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba   +   \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                    \u2502\n                                                    \u25bc\n                                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                          \u2502     dense1.output       \u2502 (T\u1ecda \u0111\u1ed9 trong kh\u00f4ng gian m\u1edbi)\n                                          \u2502      (Logits)           \u2502\n                                          \u2502 [1.2,  3.6,  0.65]      \u2502\n                                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                      \u2502\n                                                      \u25bc\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551   (B\u01af\u1edaC 3: PH\u00c9P BI\u1ebeN \u0110\u1ed4I PHI TUY\u1ebeN - H\u00c0M K\u00cdCH HO\u1ea0T RELU) - `output = max(0, x)`                             \u2551\n\u2551   M\u1ee5c ti\u00eau: \"B\u1ebb g\u00e3y\" kh\u00f4ng gian, lo\u1ea1i b\u1ecf c\u00e1c gi\u00e1 tr\u1ecb \u00e2m \u0111\u1ec3 t\u1ea1o kh\u1ea3 n\u0103ng h\u1ecdc phi tuy\u1ebfn.                      \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                                                                                    \u2502\n    \u25bc (H\u00e0nh \u0111\u1ed9ng tr\u00ean t\u1eebng ph\u1ea7n t\u1eed)                                                      \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     dense1.output       \u2502                                                    \u2502 activation1.output        \u2502\n\u2502   [1.2, 3.6, 0.65]      \u2502                                                    \u2502 (\u0110\u1ea7u ra cu\u1ed1i c\u00f9ng)        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n            \u2502                                                                  \u2502   [1.2, 3.6, 0.65]        \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           max(0, 1.2)  --&gt;  1.2                 \u2502\n\u2502           max(0, 3.6)  --&gt;  3.6                 \u2502\n\u2502           max(0, 0.65) --&gt;  0.65                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n* GHI CH\u00da: N\u1ebfu \u0111\u1ea7u v\u00e0o cho ReLU l\u00e0 [-0.8, 0.6, 0.15], \u0111\u1ea7u ra s\u1ebd l\u00e0 [0.0, 0.6, 0.15].\n* Ch\u00ednh ph\u00e9p bi\u1ebfn \u0111\u1ed5i th\u00e0nh 0 n\u00e0y l\u00e0 h\u00e0nh \u0111\u1ed9ng \"b\u1ebb g\u00e3y\" kh\u00f4ng gian.\n\n==============================================================================================================\nT\u1ed4NG K\u1ebeT D\u00d2NG CH\u1ea2Y:\nInput(2D) \u2500\u2500(Bi\u1ebfn \u0111\u1ed5i tuy\u1ebfn t\u00ednh)\u2500\u2500&gt; Logits(3D) \u2500\u2500(\"B\u1ebb g\u00e3y\" b\u1eb1ng ReLU)\u2500\u2500&gt; Output c\u1ee7a l\u1edbp \u1ea9n(3D)\n==============================================================================================================\n</code></pre>"},{"location":"vi/Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/","title":"Transformation & ReLU activation function","text":""},{"location":"vi/Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#phan-1-ham-kich-hoat-relu","title":"Ph\u1ea7n 1: H\u00e0m k\u00edch ho\u1ea1t ReLU","text":"<p>\u0110\u00e2y l\u00e0 m\u1ed9t trong nh\u1eefng ph\u1ea7n minh h\u1ecda hay nh\u1ea5t v\u1ec1 s\u1ee9c m\u1ea1nh c\u1ee7a h\u00e0m k\u00edch ho\u1ea1t ReLU, m\u1ed9t kh\u00e1i ni\u1ec7m n\u1ec1n t\u1ea3ng c\u1ee7a m\u1ea1ng neural hi\u1ec7n \u0111\u1ea1i.</p> <p>M\u1ee5c ti\u00eau ch\u00ednh c\u1ee7a ch\u01b0\u01a1ng n\u00e0y l\u00e0 tr\u1ea3 l\u1eddi c\u00e2u h\u1ecfi: \"L\u00e0m th\u1ebf n\u00e0o m\u1ed9t h\u00e0m \u0111\u01a1n gi\u1ea3n nh\u01b0 ReLU, v\u1ed1n ch\u1ec9 l\u00e0 m\u1ed9t \u0111\u01b0\u1eddng th\u1eb3ng b\u1ecb 'b\u1ebb g\u00e3y' t\u1ea1i \u0111i\u1ec3m 0, l\u1ea1i c\u00f3 th\u1ec3 gi\u00fap m\u1ea1ng neural h\u1ecdc \u0111\u01b0\u1ee3c c\u00e1c m\u1ed1i quan h\u1ec7 phi tuy\u1ebfn v\u00f4 c\u00f9ng ph\u1ee9c t\u1ea1p (nh\u01b0 h\u00ecnh sin)?\"</p> <p>H\u00e3y c\u00f9ng m\u1ed5 x\u1ebb n\u00f3 t\u1eebng b\u01b0\u1edbc m\u1ed9t.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#tom-tat-y-tuong-lon-the-big-idea","title":"T\u00f3m T\u1eaft \u00dd T\u01b0\u1edfng L\u1edbn (The Big Idea)","text":"<p>H\u00e3y t\u01b0\u1edfng t\u01b0\u1ee3ng b\u1ea1n ch\u1ec9 c\u00f3 c\u00e1c vi\u00ean g\u1ea1ch LEGO th\u1eb3ng. L\u00e0m th\u1ebf n\u00e0o \u0111\u1ec3 d\u00f9ng ch\u00fang x\u00e2y m\u1ed9t b\u1ee9c t\u01b0\u1eddng cong?</p> <p>C\u00e2u tr\u1ea3 l\u1eddi l\u00e0: B\u1ea1n kh\u00f4ng th\u1ec3 t\u1ea1o ra m\u1ed9t \u0111\u01b0\u1eddng cong m\u01b0\u1ee3t m\u00e0, nh\u01b0ng b\u1ea1n c\u00f3 th\u1ec3 x\u1ea5p x\u1ec9 (approximate) n\u00f3 b\u1eb1ng c\u00e1ch gh\u00e9p r\u1ea5t nhi\u1ec1u vi\u00ean g\u1ea1ch th\u1eb3ng ng\u1eafn l\u1ea1i v\u1edbi nhau. C\u00e0ng nhi\u1ec1u vi\u00ean g\u1ea1ch ng\u1eafn, b\u1ee9c t\u01b0\u1eddng c\u1ee7a b\u1ea1n tr\u00f4ng s\u1ebd c\u00e0ng cong v\u00e0 m\u01b0\u1ee3t.</p> <p>Trong m\u1ea1ng neural:</p> <ul> <li>H\u00e0m ReLU ch\u00ednh l\u00e0 vi\u00ean g\u1ea1ch th\u1eb3ng c\u1ee7a ch\u00fang ta.</li> <li>M\u1ed7i neuron (ho\u1eb7c c\u1eb7p neuron) l\u00e0 m\u1ed9t ng\u01b0\u1eddi th\u1ee3 x\u00e2y, c\u00f3 nhi\u1ec7m v\u1ee5 \u0111\u1eb7t m\u1ed9t vi\u00ean g\u1ea1ch.</li> <li>M\u1ea1ng neural l\u00e0 c\u1ea3 c\u00f4ng tr\u00ecnh, gh\u00e9p c\u00e1c vi\u00ean g\u1ea1ch n\u00e0y l\u1ea1i \u0111\u1ec3 t\u1ea1o ra h\u00ecnh d\u1ea1ng ph\u1ee9c t\u1ea1p cu\u1ed1i c\u00f9ng (nh\u01b0 \u0111\u01b0\u1eddng cong h\u00ecnh sin).</li> </ul> <p>Gi\u1edd h\u00e3y \u0111i v\u00e0o chi ti\u1ebft c\u00e1ch \"ng\u01b0\u1eddi th\u1ee3\" neuron l\u00e0m vi\u1ec7c.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#khoi-xay-dung-co-ban-mot-neuron-relu","title":"Kh\u1ed1i X\u00e2y D\u1ef1ng C\u01a1 B\u1ea3n - M\u1ed9t Neuron ReLU","text":"<p>M\u1ed9t neuron nh\u1eadn \u0111\u1ea7u v\u00e0o (input), nh\u00e2n v\u1edbi tr\u1ecdng s\u1ed1 (weight), c\u1ed9ng v\u1edbi thi\u00ean v\u1ecb (bias), r\u1ed3i cho qua h\u00e0m k\u00edch ho\u1ea1t ReLU.</p> <p>C\u00f4ng th\u1ee9c: <code>output = ReLU(weight * input + bias)</code> H\u00e0m ReLU: <code>ReLU(x) = max(0, x)</code>. Ngh\u0129a l\u00e0: *   N\u1ebfu <code>x &lt;= 0</code>, k\u1ebft qu\u1ea3 l\u00e0 <code>0</code>. *   N\u1ebfu <code>x &gt; 0</code>, k\u1ebft qu\u1ea3 l\u00e0 <code>x</code>.</p> <p>N\u00f3 gi\u1ed1ng nh\u01b0 m\u1ed9t c\u00e1i c\u1ed5ng: *   Input t\u00ednh to\u00e1n ra \u00e2m -&gt; C\u1ed5ng \u0111\u00f3ng -&gt; Output = 0. *   Input t\u00ednh to\u00e1n ra d\u01b0\u01a1ng -&gt; C\u1ed5ng m\u1edf -&gt; Output = gi\u00e1 tr\u1ecb t\u00ednh \u0111\u01b0\u1ee3c.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#vai-tro-cua-trong-so-weight-va-thien-vi-bias","title":"Vai tr\u00f2 c\u1ee7a Tr\u1ecdng s\u1ed1 (Weight) v\u00e0 Thi\u00ean v\u1ecb (Bias):","text":"<ol> <li> <p>Tr\u1ecdng s\u1ed1 (Weight): Thay \u0111\u1ed5i \u0111\u1ed9 d\u1ed1c.</p> <ul> <li>Weight l\u1edbn -&gt; \u0110\u01b0\u1eddng d\u1ed1c l\u00ean nhanh h\u01a1n.</li> <li>Weight nh\u1ecf -&gt; \u0110\u01b0\u1eddng d\u1ed1c l\u00ean tho\u1ea3i h\u01a1n.</li> <li>Weight \u00e2m -&gt; \u0110\u01b0\u1eddng b\u1ecb l\u1eadt ng\u01b0\u1ee3c, d\u1ed1c xu\u1ed1ng.</li> </ul> </li> <li> <p>Thi\u00ean v\u1ecb (Bias): D\u1ecbch chuy\u1ec3n \u0111i\u1ec3m \"b\u1ebb g\u00e3y\".</p> <ul> <li>Bias quy\u1ebft \u0111\u1ecbnh t\u1ea1i gi\u00e1 tr\u1ecb input n\u00e0o th\u00ec neuron b\u1eaft \u0111\u1ea7u \"k\u00edch ho\u1ea1t\" (t\u1ee9c l\u00e0 cho ra output &gt; 0). N\u00f3 d\u1ecbch chuy\u1ec3n \u0111\u1ed3 th\u1ecb sang tr\u00e1i ho\u1eb7c ph\u1ea3i.</li> </ul> </li> </ol> <p>Minh h\u1ecda ASCII:</p> <p>H\u00e3y xem \u0111\u1ed3 th\u1ecb c\u1ee7a m\u1ed9t neuron ReLU. Tr\u1ee5c ho\u00e0nh l\u00e0 <code>input</code>, tr\u1ee5c tung l\u00e0 <code>output</code>.</p> Text Only<pre><code>        C\u01a1 b\u1ea3n (w=1, b=0)          T\u0103ng Weight (w=2)           Th\u00eam Bias (w=1, b= -1)\n           /                          //                              /\n          /                          / /                             /\n         /                          / /                             /\n        +------- (input)          +------- (input)                +------- (input)\n                                                                 |\n                                                               (\u0111i\u1ec3m b\u1ebb g\u00e3y d\u1ecbch sang ph\u1ea3i)\n</code></pre> <p>T\u00f3m l\u1ea1i: V\u1edbi m\u1ed9t neuron, ch\u00fang ta c\u00f3 th\u1ec3 t\u1ea1o ra m\u1ed9t \"\u0111o\u1ea1n d\u1ed1c\" b\u1eaft \u0111\u1ea7u t\u1eeb m\u1ed9t \u0111i\u1ec3m t\u00f9y \u00fd v\u00e0 c\u00f3 \u0111\u1ed9 d\u1ed1c t\u00f9y \u00fd. Nh\u01b0ng n\u00f3 v\u1eabn ch\u1ec9 l\u00e0 m\u1ed9t \u0111\u01b0\u1eddng th\u1eb3ng.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#phep-mau-bat-au-ket-hop-hai-neuron","title":"Ph\u00e9p M\u00e0u B\u1eaft \u0110\u1ea7u - K\u1ebft H\u1ee3p Hai Neuron","text":"<p>\u0110\u00e2y l\u00e0 ph\u1ea7n c\u1ed1t l\u00f5i v\u00e0 k\u1ef3 di\u1ec7u nh\u1ea5t. S\u00e1ch minh h\u1ecda b\u1eb1ng c\u00e1ch d\u00f9ng m\u1ed9t c\u1eb7p neuron (m\u1ed9t \u1edf l\u1edbp \u1ea9n 1, m\u1ed9t \u1edf l\u1edbp \u1ea9n 2). Khi k\u1ebft h\u1ee3p ch\u00fang l\u1ea1i, ch\u00fang ta c\u00f3 th\u1ec3 t\u1ea1o ra m\u1ed9t th\u1ee9 g\u1ecdi l\u00e0 \"v\u00f9ng \u1ea3nh h\u01b0\u1edfng\" (area of effect).</p> <p>H\u00e3y t\u01b0\u1edfng t\u01b0\u1ee3ng th\u1ebf n\u00e0y:</p> <ol> <li> <p>Neuron 1 (Neuron \"K\u00edch ho\u1ea1t\"): T\u1ea1o ra m\u1ed9t \u0111\u01b0\u1eddng d\u1ed1c l\u00ean.</p> <ul> <li>V\u00ed d\u1ee5: <code>y1 = ReLU(1.0 * x - 0.2)</code></li> <li>N\u00f3 s\u1ebd t\u1ea1o ra m\u1ed9t \u0111\u01b0\u1eddng d\u1ed1c l\u00ean, b\u1eaft \u0111\u1ea7u t\u1eeb <code>x = 0.2</code>.</li> </ul> </li> <li> <p>Neuron 2 (Neuron \"V\u00f4 hi\u1ec7u h\u00f3a\"): T\u1ea1o ra m\u1ed9t \u0111\u01b0\u1eddng d\u1ed1c xu\u1ed1ng.</p> <ul> <li>V\u00ed d\u1ee5: <code>y2 = ReLU(-1.0 * x + 0.8)</code></li> <li>N\u00f3 s\u1ebd t\u1ea1o ra m\u1ed9t \u0111\u01b0\u1eddng d\u1ed1c xu\u1ed1ng, b\u1eaft \u0111\u1ea7u t\u1eeb <code>x = 0.8</code>.</li> </ul> </li> </ol> <p>Khi b\u1ea1n c\u1ed9ng k\u1ebft qu\u1ea3 c\u1ee7a hai neuron n\u00e0y l\u1ea1i (\u0111\u00e2y l\u00e0 \u0111i\u1ec1u x\u1ea3y ra \u1edf l\u1edbp ti\u1ebfp theo), \u0111i\u1ec1u th\u00fa v\u1ecb s\u1ebd x\u1ea3y ra:</p> <ul> <li>Khi x &lt; 0.2: C\u1ea3 hai neuron \u0111\u1ec1u cho output l\u00e0 0. T\u1ed5ng l\u00e0 0.</li> <li>Khi 0.2 &lt; x &lt; 0.8: Neuron 1 ho\u1ea1t \u0111\u1ed9ng (d\u1ed1c l\u00ean), Neuron 2 v\u1eabn l\u00e0 0. T\u1ed5ng l\u00e0 m\u1ed9t \u0111\u01b0\u1eddng d\u1ed1c l\u00ean.</li> <li>Khi x &gt; 0.8: C\u1ea3 hai neuron \u0111\u1ec1u ho\u1ea1t \u0111\u1ed9ng. \u0110\u01b0\u1eddng d\u1ed1c l\u00ean c\u1ee7a Neuron 1 s\u1ebd b\u1ecb tri\u1ec7t ti\u00eau b\u1edfi \u0111\u01b0\u1eddng d\u1ed1c xu\u1ed1ng c\u1ee7a Neuron 2. T\u1ed5ng l\u00e0 m\u1ed9t \u0111\u01b0\u1eddng n\u1eb1m ngang.</li> </ul> <p>K\u1ebft qu\u1ea3 l\u00e0 m\u1ed9t h\u00ecnh \"c\u00e1i l\u1ec1u\" ho\u1eb7c \"c\u00e1i n\u00f3n\"!</p> <p>Minh h\u1ecda ASCII:</p> Text Only<pre><code>   \u0110\u1ea7u ra c\u1ee7a Neuron 1        +    \u0110\u1ea7u ra c\u1ee7a Neuron 2        =       K\u1ebft qu\u1ea3 cu\u1ed1i c\u00f9ng\n  (d\u1ed1c l\u00ean t\u1eeb x=0.2)               (d\u1ed1c xu\u1ed1ng t\u1eeb x=0.8)                 (h\u00ecnh c\u00e1i l\u1ec1u)\n            /                                 |                               /\\\n           /                                  |                              /  \\\n          /                                   |                             /    \\\n---------+---------- (input)    --------------+---------- (input)   ---------+----+------ (input)\n       0.2                                  0.8                             0.2  0.8\n                                              \\\n                                               \\\n</code></pre> <p>\u0110\u00e2y ch\u00ednh l\u00e0 \"vi\u00ean g\u1ea1ch LEGO\" h\u00ecnh tam gi\u00e1c c\u1ee7a ch\u00fang ta! B\u1eb1ng c\u00e1ch \u0111i\u1ec1u ch\u1ec9nh weight v\u00e0 bias c\u1ee7a c\u1eb7p neuron n\u00e0y, ch\u00fang ta c\u00f3 th\u1ec3 ki\u1ec3m so\u00e1t: *   V\u1ecb tr\u00ed c\u1ee7a c\u00e1i l\u1ec1u (d\u1ecbch tr\u00e1i/ph\u1ea3i). *   Chi\u1ec1u cao c\u1ee7a c\u00e1i l\u1ec1u. *   \u0110\u1ed9 d\u1ed1c c\u1ee7a hai b\u00ean s\u01b0\u1eddn l\u1ec1u.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#xay-dung-song-sin-ghep-nhieu-vien-gach","title":"X\u00e2y D\u1ef1ng S\u00f3ng Sin - Gh\u00e9p Nhi\u1ec1u \"Vi\u00ean G\u1ea1ch\"","text":"<p>B\u00e2y gi\u1edd b\u1ea1n \u0111\u00e3 c\u00f3 nh\u1eefng \"vi\u00ean g\u1ea1ch h\u00ecnh l\u1ec1u\". Vi\u1ec7c x\u1ea5p x\u1ec9 m\u1ed9t \u0111\u01b0\u1eddng cong h\u00ecnh sin tr\u1edf n\u00ean \u0111\u01a1n gi\u1ea3n:</p> <p>B\u1ea1n ch\u1ec9 c\u1ea7n d\u00f9ng nhi\u1ec1u c\u1eb7p neuron, m\u1ed7i c\u1eb7p t\u1ea1o ra m\u1ed9t \"c\u00e1i l\u1ec1u\" \u0111\u1ec3 m\u00f4 ph\u1ecfng m\u1ed9t \u0111o\u1ea1n c\u1ee7a s\u00f3ng sin.</p> <ul> <li>C\u1eb7p neuron 1: T\u1ea1o ra \u0111o\u1ea1n d\u1ed1c l\u00ean \u0111\u1ea7u ti\u00ean c\u1ee7a s\u00f3ng sin.</li> <li>C\u1eb7p neuron 2: T\u1ea1o ra \u0111o\u1ea1n d\u1ed1c xu\u1ed1ng ti\u1ebfp theo.</li> <li>C\u1eb7p neuron 3: T\u1ea1o ra \u0111o\u1ea1n d\u1ed1c l\u00ean c\u1ee7a ph\u1ea7n \u00e2m.</li> <li>... v\u00e0 c\u1ee9 th\u1ebf.</li> </ul> <p>Qu\u00e1 tr\u00ecnh \"hand-tuning\" (ch\u1ec9nh tay) trong s\u00e1ch (t\u1eeb Fig 4.20 \u0111\u1ebfn 4.33) ch\u1ec9 l\u00e0 \u0111\u1ec3 minh h\u1ecda cho b\u1ea1n th\u1ea5y: *   \"\u00c0, n\u1ebfu t\u00f4i ch\u1ec9nh <code>weight</code> n\u00e0y, c\u00e1i d\u1ed1c n\u00e0y s\u1ebd cao h\u01a1n.\" *   \"N\u1ebfu t\u00f4i ch\u1ec9nh <code>bias</code> kia, c\u00e1i l\u1ec1u n\u00e0y s\u1ebd d\u1ecbch sang ph\u1ea3i.\" *   \"N\u1ebfu t\u00f4i d\u00f9ng <code>weight</code> \u00e2m \u1edf \u0111\u1ea7u ra, c\u00e1i l\u1ec1u s\u1ebd b\u1ecb l\u1eadt ng\u01b0\u1ee3c xu\u1ed1ng d\u01b0\u1edbi (t\u1ea1o ra \u0111\u00e1y c\u1ee7a s\u00f3ng sin).\"</p> <p>Minh h\u1ecda tr\u1eebu t\u01b0\u1ee3ng b\u1eb1ng ASCII:</p> Text Only<pre><code>   S\u00f3ng sin m\u1ee5c ti\u00eau:\n       __/ \\__\n      /     \\\n     /       \\\n            / \\\n    _______/   \\______\n\n   M\u1ea1ng neural x\u1ea5p x\u1ec9 b\u1eb1ng c\u00e1ch c\u1ed9ng c\u00e1c \"c\u00e1i l\u1ec1u\":\n\n     L\u1ec1u 1      L\u1ec1u 2 (l\u1eadt ng\u01b0\u1ee3c)     L\u1ec1u 3...\n       /\\               _                /\\\n      /  \\             / \\              /  \\\n     /    \\           /   \\            /    \\\n    -------  +      \\/     +  ...   =   K\u1ebft qu\u1ea3 g\u1ea7n gi\u1ed1ng s\u00f3ng sin\n</code></pre>"},{"location":"vi/Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#tong-ket","title":"T\u1ed5ng K\u1ebft","text":"<ol> <li> <p>S\u1ee9c m\u1ea1nh c\u1ee7a ReLU: B\u1ea3n th\u00e2n ReLU r\u1ea5t \u0111\u01a1n gi\u1ea3n, nh\u01b0ng khi \u0111\u01b0\u1ee3c k\u1ebft h\u1ee3p trong m\u1ed9t m\u1ea1ng l\u01b0\u1edbi nhi\u1ec1u l\u1edbp, ch\u00fang c\u00f3 kh\u1ea3 n\u0103ng t\u1ea1o ra c\u00e1c h\u00e0m tuy\u1ebfn t\u00ednh t\u1eebng \u0111o\u1ea1n (piecewise linear) v\u00f4 c\u00f9ng ph\u1ee9c t\u1ea1p. Nh\u1eefng h\u00e0m n\u00e0y c\u00f3 th\u1ec3 x\u1ea5p x\u1ec9 b\u1ea5t k\u1ef3 h\u00e0m li\u00ean t\u1ee5c n\u00e0o (\u0111\u00e2y l\u00e0 \u00fd t\u01b0\u1edfng c\u1ee7a \u0110\u1ecbnh l\u00fd X\u1ea5p x\u1ec9 Ph\u1ed5 qu\u00e1t - Universal Approximation Theorem).</p> </li> <li> <p>T\u1ea1i sao c\u1ea7n nhi\u1ec1u l\u1edbp \u1ea9n? L\u1edbp \u1ea9n \u0111\u1ea7u ti\u00ean t\u1ea1o ra c\u00e1c \"\u0111o\u1ea1n d\u1ed1c\". L\u1edbp \u1ea9n th\u1ee9 hai k\u1ebft h\u1ee3p c\u00e1c \"\u0111o\u1ea1n d\u1ed1c\" \u0111\u00f3 \u0111\u1ec3 t\u1ea1o th\u00e0nh c\u00e1c \"c\u00e1i l\u1ec1u\". C\u00e1c l\u1edbp sau c\u00f3 th\u1ec3 k\u1ebft h\u1ee3p c\u00e1c \"c\u00e1i l\u1ec1u\" n\u00e0y th\u00e0nh nh\u1eefng h\u00ecnh d\u1ea1ng c\u00f2n ph\u1ee9c t\u1ea1p h\u01a1n n\u1eefa.</p> </li> <li> <p>T\u1eeb \"Ch\u1ec9nh Tay\" \u0111\u1ebfn \"T\u1ef1 H\u1ecdc\": Vi\u1ec7c ch\u1ec9nh tay c\u00e1c th\u00f4ng s\u1ed1 trong s\u00e1ch r\u1ea5t t\u1ed1n c\u00f4ng v\u00e0 ch\u1ec9 mang t\u00ednh minh h\u1ecda. Trong th\u1ef1c t\u1ebf, qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n (training) m\u1ea1ng neural ch\u00ednh l\u00e0 qu\u00e1 tr\u00ecnh m\u00e1y t\u00ednh t\u1ef1 \u0111\u1ed9ng t\u00ecm ra c\u00e1c gi\u00e1 tr\u1ecb <code>weight</code> v\u00e0 <code>bias</code> t\u1ed1t nh\u1ea5t cho t\u1ea5t c\u1ea3 c\u00e1c neuron, \u0111\u1ec3 k\u1ebft qu\u1ea3 \u0111\u1ea7u ra c\u1ee7a m\u1ea1ng kh\u1edbp v\u1edbi d\u1eef li\u1ec7u m\u1ee5c ti\u00eau nh\u1ea5t c\u00f3 th\u1ec3. B\u1ed9 t\u1ed1i \u01b0u h\u00f3a (optimizer) nh\u01b0 Gradient Descent ch\u00ednh l\u00e0 \"ki\u1ebfn tr\u00fac s\u01b0 tr\u01b0\u1edfng\" l\u00e0m c\u00f4ng vi\u1ec7c n\u00e0y.</p> </li> <li> <p>Nhi\u1ec1u Neuron H\u01a1n = T\u1ed1t H\u01a1n: khi t\u0103ng s\u1ed1 neuron l\u00ean, v\u00ed d\u1ee5: 64, m\u1ea1ng c\u00f3 nhi\u1ec1u \"vi\u00ean g\u1ea1ch\" h\u01a1n \u0111\u1ec3 x\u00e2y d\u1ef1ng. K\u1ebft qu\u1ea3 l\u00e0 \u0111\u01b0\u1eddng cong x\u1ea5p x\u1ec9 tr\u00f4ng m\u01b0\u1ee3t m\u00e0 v\u00e0 ch\u00ednh x\u00e1c h\u01a1n r\u1ea5t nhi\u1ec1u.</p> </li> </ol> <p>ReLU, d\u00f9 \u0111\u01a1n gi\u1ea3n, l\u1ea1i l\u00e0 n\u1ec1n t\u1ea3ng cho s\u1ee9c m\u1ea1nh phi th\u01b0\u1eddng c\u1ee7a m\u1ea1ng neural hi\u1ec7n \u0111\u1ea1i.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#phan-2-i-sau-vao-qua-trinh-bien-oi-mot-iem-du-lieu-duy-nhat","title":"Ph\u1ea7n 2: \u0110i s\u00e2u v\u00e0o qu\u00e1 tr\u00ecnh bi\u1ebfn \u0111\u1ed5i m\u1ed9t \u0111i\u1ec3m d\u1eef li\u1ec7u duy nh\u1ea5t.","text":"<p>Gi\u1ea3 s\u1eed ch\u00fang ta c\u00f3:</p> <ul> <li>M\u1ed9t \u0111i\u1ec3m d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o <code>O</code> c\u00f3 t\u1ecda \u0111\u1ed9 <code>(0.5, 1.0)</code>.</li> <li>M\u1ed9t l\u1edbp <code>Layer_Dense</code> c\u00f3 2 \u0111\u1ea7u v\u00e0o v\u00e0 3 neuron.</li> </ul>"},{"location":"vi/Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#1-khoi-tao-initialization","title":"1. Kh\u1edfi T\u1ea1o (Initialization)","text":"<p>L\u1edbp <code>dense1</code> \u0111\u01b0\u1ee3c kh\u1edfi t\u1ea1o v\u1edbi <code>weights</code> v\u00e0 <code>biases</code>. Gi\u1ea3 s\u1eed sau khi kh\u1edfi t\u1ea1o ng\u1eabu nhi\u00ean, ch\u00fang ta c\u00f3 c\u00e1c gi\u00e1 tr\u1ecb c\u1ee5 th\u1ec3 nh\u01b0 sau:</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#ma-tran-trong-so-w-selfweights-kich-thuoc-2-3","title":"Ma tr\u1eadn Tr\u1ecdng s\u1ed1 <code>W</code> (self.weights) - K\u00edch th\u01b0\u1edbc (2, 3)","text":"<ul> <li>H\u00e0ng 0: Tr\u1ecdng s\u1ed1 cho \u0111\u1ea7u v\u00e0o th\u1ee9 nh\u1ea5t (<code>i1 = 0.5</code>).</li> <li>H\u00e0ng 1: Tr\u1ecdng s\u1ed1 cho \u0111\u1ea7u v\u00e0o th\u1ee9 hai (<code>i2 = 1.0</code>).</li> <li>C\u1ed9t 0, 1, 2: T\u01b0\u01a1ng \u1ee9ng v\u1edbi Neuron 0, 1, 2.</li> </ul> Text Only<pre><code>          Neuron 0   Neuron 1   Neuron 2\n         +----------+----------+----------+\nInput 0  |   0.2    |   0.8    |  -0.5    |\n(i1=0.5) +----------+----------+----------+\nInput 1  |  -0.9    |   0.2    |   0.4    |\n(i2=1.0) +----------+----------+----------+\n</code></pre>"},{"location":"vi/Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#vector-bias-b-selfbiases-kich-thuoc-1-3","title":"Vector Bias <code>b</code> (self.biases) - K\u00edch th\u01b0\u1edbc (1, 3)","text":"<ul> <li>M\u1ed7i gi\u00e1 tr\u1ecb t\u01b0\u01a1ng \u1ee9ng v\u1edbi thi\u00ean ki\u1ebfn c\u1ee7a m\u1ed9t neuron.</li> </ul> Text Only<pre><code>         +----------+----------+----------+\n         |   2.0    |   3.0    |   0.5    |\n         +----------+----------+----------+\n           Neuron 0   Neuron 1   Neuron 2\n</code></pre>"},{"location":"vi/Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#2-qua-trinh-bien-oi-transformation-dense1forwardo","title":"2. Qu\u00e1 tr\u00ecnh Bi\u1ebfn \u0111\u1ed5i (Transformation) - <code>dense1.forward(O)</code>","text":"<p>Ch\u00fang ta th\u1ef1c hi\u1ec7n ph\u00e9p to\u00e1n: <code>v' = v \u00b7 W + b</code></p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#buoc-21-phep-nhan-ma-tran-dot-product-v-w","title":"B\u01b0\u1edbc 2.1: Ph\u00e9p nh\u00e2n ma tr\u1eadn (Dot Product) <code>v \u00b7 W</code>","text":"<ul> <li><code>v</code> l\u00e0 vector \u0111\u1ea7u v\u00e0o: <code>[0.5, 1.0]</code> (K\u00edch th\u01b0\u1edbc 1x2)</li> <li><code>W</code> l\u00e0 ma tr\u1eadn tr\u1ecdng s\u1ed1 (K\u00edch th\u01b0\u1edbc 2x3)</li> <li>K\u1ebft qu\u1ea3 s\u1ebd l\u00e0 m\u1ed9t vector k\u00edch th\u01b0\u1edbc 1x3.</li> </ul> Text Only<pre><code>                                       +-------+-------+-------+\n                                       |  0.2  |  0.8  | -0.5  |\n                                       | -0.9  |  0.2  |  0.4  |\n                                       +-------+-------+-------+\n                                                 ^\n                                                 |\n                                                 \u00b7 (Dot Product)\n+-------+-------+\n|  0.5  |  1.0  |\n+-------+-------+\n      |\n      +-------------------------------------------------------------+\n      |                                                             |\n      v                                                             v\n    T\u00ednh to\u00e1n cho Neuron 0:                                       T\u00ednh to\u00e1n cho Neuron 1:\n    (0.5 * 0.2) + (1.0 * -0.9)                                    (0.5 * 0.8) + (1.0 * 0.2)\n    = 0.1 - 0.9                                                   = 0.4 + 0.2\n    = -0.8                                                        = 0.6\n\n                                                                     T\u00ednh to\u00e1n cho Neuron 2:\n                                                                     (0.5 * -0.5) + (1.0 * 0.4)\n                                                                     = -0.25 + 0.4\n                                                                     = 0.15\n</code></pre> <p>K\u1ebft qu\u1ea3 c\u1ee7a ph\u00e9p nh\u00e2n ma tr\u1eadn l\u00e0 vector <code>[-0.8, 0.6, 0.15]</code>.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#buoc-22-cong-vector-bias-b","title":"B\u01b0\u1edbc 2.2: C\u1ed9ng Vector Bias <code>+ b</code>","text":"<p>B\u00e2y gi\u1edd, ch\u00fang ta l\u1ea5y k\u1ebft qu\u1ea3 \u1edf tr\u00ean v\u00e0 c\u1ed9ng v\u1edbi vector bias.</p> Text Only<pre><code>      K\u1ebft qu\u1ea3 t\u1eeb v \u00b7 W                  Vector Bias b                Vector \u0111\u1ea7u ra v'\n+--------+-------+--------+     +     +-------+-------+-------+     =     +-------+-------+-------+\n|  -0.8  |  0.6  |  0.15  |           |  2.0  |  3.0  |  0.5  |           |  1.2  |  3.6  |  0.65 |\n+--------+-------+--------+           +-------+-------+-------+           +-------+-------+-------+\n     |        |        |                 |        |        |                 |        |        |\n     |        |        +-----------------|--------|--------|-----------------+        |\n     |        +--------------------------|--------|--------+--------------------------+\n     +-----------------------------------|--------+-----------------------------------+\n\n     -0.8 + 2.0 = 1.2\n           0.6 + 3.0 = 3.6\n                 0.15 + 0.5 = 0.65\n</code></pre> <p>K\u1ebft qu\u1ea3: Vector <code>v'</code> (\u0111\u1ea7u ra c\u1ee7a <code>dense1</code>) l\u00e0 <code>[1.2, 3.6, 0.65]</code>. \u0110\u00e2y ch\u00ednh l\u00e0 t\u1ecda \u0111\u1ed9 c\u1ee7a \u0111i\u1ec3m <code>O</code> trong kh\u00f4ng gian 3 chi\u1ec1u m\u1edbi sau ph\u00e9p bi\u1ebfn \u0111\u1ed5i tuy\u1ebfn t\u00ednh.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#3-kich-hoat-relu-activation1forwardv","title":"3. K\u00edch Ho\u1ea1t ReLU - <code>activation1.forward(v')</code>","text":"<p>B\u00e2y gi\u1edd, ch\u00fang ta \u0111\u01b0a vector <code>v'</code> qua h\u00e0m ReLU. H\u00e0m n\u00e0y ho\u1ea1t \u0111\u1ed9ng tr\u00ean t\u1eebng ph\u1ea7n t\u1eed (element-wise).</p> Text Only<pre><code>     Vector \u0111\u1ea7u v\u00e0o v' cho ReLU           H\u00e0nh \u0111\u1ed9ng c\u1ee7a ReLU           Vector cu\u1ed1i c\u00f9ng v''\n+-------+-------+-------+        max(0, x)       +-------+-------+-------+\n|  1.2  |  3.6  |  0.65 |  ----------------&gt;     |  1.2  |  3.6  |  0.65 |\n+-------+-------+-------+                        +-------+-------+-------+\n     |        |        |\n     |        |        +-----&gt; max(0, 0.65) = 0.65\n     |        +--------------&gt; max(0, 3.6)  = 3.6\n     +-----------------------&gt; max(0, 1.2)  = 1.2\n</code></pre> <p>Trong v\u00ed d\u1ee5 n\u00e0y, v\u00ec t\u1ea5t c\u1ea3 c\u00e1c th\u00e0nh ph\u1ea7n c\u1ee7a <code>v'</code> \u0111\u1ec1u l\u00e0 s\u1ed1 d\u01b0\u01a1ng, n\u00ean \u0111\u1ea7u ra c\u1ee7a ReLU <code>v''</code> gi\u1ed1ng h\u1ec7t <code>v'</code>.</p> <p>N\u1ebfu <code>v'</code> l\u00e0 <code>[-0.8, 0.6, 0.15]</code> (tr\u01b0\u1edbc khi c\u1ed9ng bias), th\u00ec k\u1ebft qu\u1ea3 s\u1ebd kh\u00e1c:</p> Text Only<pre><code>     Vector \u0111\u1ea7u v\u00e0o v' cho ReLU           H\u00e0nh \u0111\u1ed9ng c\u1ee7a ReLU           Vector cu\u1ed1i c\u00f9ng v''\n+--------+-------+--------+        max(0, x)       +-------+-------+--------+\n|  -0.8  |  0.6  |  0.15  |  ----------------&gt;     |  0.0  |  0.6  |  0.15  |\n+--------+-------+--------+                        +-------+-------+--------+\n     |        |        |\n     |        |        +-----&gt; max(0, 0.15) = 0.15\n     |        +--------------&gt; max(0, 0.6)  = 0.6\n     +-----------------------&gt; max(0, -0.8) = 0.0\n</code></pre> <p>S\u01a1 \u0111\u1ed3 n\u00e0y \u0111\u00e3 m\u00f4 t\u1ea3 to\u00e0n b\u1ed9 qu\u00e1 tr\u00ecnh to\u00e1n h\u1ecdc t\u1eeb m\u1ed9t vector \u0111\u1ea7u v\u00e0o <code>v</code> \u0111\u1ebfn vector cu\u1ed1i c\u00f9ng <code>v''</code> sau khi qua m\u1ed9t l\u1edbp d\u00e0y \u0111\u1eb7c v\u00e0 m\u1ed9t l\u1edbp k\u00edch ho\u1ea1t ReLU.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#truu-tuong-hoa","title":"Tr\u1eebu t\u01b0\u1ee3ng h\u00f3a","text":"<p>M\u1ed7i neuron \u0111\u00f3ng g\u00f3p v\u00e0o vi\u1ec7c t\u1ea1o ra \"ch\u1eef k\u00fd\" cu\u1ed1i c\u00f9ng nh\u01b0 th\u1ebf n\u00e0o?</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#phan-tich","title":"Ph\u00e2n T\u00edch","text":"<ol> <li> <p>Neural 1 \u0111\u00f3ng g\u00f3p m\u1ed9t ph\u1ea7n v\u00e0o vi\u1ec7c t\u1ea1o ra ch\u1eef k\u00fd cu\u1ed1i c\u00f9ng. N\u00f3 gi\u1ed1ng nh\u01b0 m\u1ed9t nh\u1ea1c c\u00f4ng trong d\u00e0n nh\u1ea1c. Nh\u1ea1c c\u00f4ng violin kh\u00f4ng \"mang\" b\u1ea3n giao h\u01b0\u1edfng, anh ta ch\u1ec9 ch\u01a1i ph\u1ea7n violin c\u1ee7a m\u00ecnh. B\u1ea3n giao h\u01b0\u1edfng (ch\u1eef k\u00fd) l\u00e0 s\u1ef1 k\u1ebft h\u1ee3p c\u1ee7a t\u1ea5t c\u1ea3 c\u00e1c nh\u1ea1c c\u00f4ng. C\u00e1ch di\u1ec5n \u0111\u1ea1t ch\u00ednh x\u00e1c h\u01a1n: \"Neuron 1 c\u00f3 m\u1ed9t b\u1ed9 ti\u00eau ch\u00ed ri\u00eang (weights v\u00e0 bias c\u1ee7a n\u00f3).\"</p> </li> <li> <p>\"bi\u1ebfn \u0111\u1ed5i O(x,y) --&gt; O'(x,y,z)\": To\u00e0n b\u1ed9 l\u1edbp (g\u1ed3m c\u1ea3 3 neuron) c\u00f9ng nhau th\u1ef1c hi\u1ec7n ph\u00e9p bi\u1ebfn \u0111\u1ed5i n\u00e0y.</p> </li> </ol>"},{"location":"vi/Explain%20Documents/Ch4pt3r.02_Transformation%26ReLU/02-transformation-relu/#dien-giai","title":"Di\u1ec5n Gi\u1ea3i","text":"<ol> <li> <p>M\u1ed7i Neuron l\u00e0 m\u1ed9t \"M\u00e1y \u0110o \u0110\u1eb7c Tr\u01b0ng\":</p> <ul> <li>Neuron 1 \u0111\u01b0\u1ee3c trang b\u1ecb m\u1ed9t b\u1ed9 ti\u00eau ch\u00ed <code>(w1, b1)</code>. N\u00f3 \u0111o xem \u0111i\u1ec3m <code>O(x,y)</code> ph\u00f9 h\u1ee3p v\u1edbi ti\u00eau ch\u00ed n\u00e0y \u0111\u1ebfn \u0111\u00e2u v\u00e0 cho ra m\u1ed9t \u0111i\u1ec3m s\u1ed1 l\u00e0 <code>x'</code>.</li> <li>Neuron 2 \u0111\u01b0\u1ee3c trang b\u1ecb m\u1ed9t b\u1ed9 ti\u00eau ch\u00ed <code>(w2, b2)</code>. N\u00f3 \u0111o xem \u0111i\u1ec3m <code>O(x,y)</code> ph\u00f9 h\u1ee3p v\u1edbi ti\u00eau ch\u00ed n\u00e0y \u0111\u1ebfn \u0111\u00e2u v\u00e0 cho ra m\u1ed9t \u0111i\u1ec3m s\u1ed1 l\u00e0 <code>y'</code>.</li> <li>Neuron 3 \u0111\u01b0\u1ee3c trang b\u1ecb m\u1ed9t b\u1ed9 ti\u00eau ch\u00ed <code>(w3, b3)</code>. N\u00f3 \u0111o xem \u0111i\u1ec3m <code>O(x,y)</code> ph\u00f9 h\u1ee3p v\u1edbi ti\u00eau ch\u00ed n\u00e0y \u0111\u1ebfn \u0111\u00e2u v\u00e0 cho ra m\u1ed9t \u0111i\u1ec3m s\u1ed1 l\u00e0 <code>z'</code>.</li> </ul> </li> <li> <p>T\u1ea1o Ra \"Ch\u1eef K\u00fd\":</p> <ul> <li>\"Ch\u1eef k\u00fd\" c\u1ee7a \u0111i\u1ec3m <code>O</code> kh\u00f4ng ph\u1ea3i l\u00e0 do m\u1ed9t neuron t\u1ea1o ra. \"Ch\u1eef k\u00fd\" ch\u00ednh l\u00e0 vector k\u1ebft qu\u1ea3 <code>O'(x', y', z')</code>. N\u00f3 l\u00e0 t\u1eadp h\u1ee3p c\u00e1c \u0111i\u1ec3m s\u1ed1 m\u00e0 t\u1ea5t c\u1ea3 c\u00e1c \"m\u00e1y \u0111o\" \u0111\u00e3 \u0111\u01b0a ra.</li> </ul> </li> <li> <p>M\u1ee5c Ti\u00eau Hu\u1ea5n Luy\u1ec7n (Training):</p> <ul> <li>Qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n s\u1ebd \u0111i\u1ec1u ch\u1ec9nh c\u00e1c b\u1ed9 ti\u00eau ch\u00ed <code>(w, b)</code> c\u1ee7a t\u1eebng neuron sao cho:<ul> <li>T\u1ea5t c\u1ea3 c\u00e1c \u0111i\u1ec3m <code>O</code> thu\u1ed9c l\u1edbp \"Xanh\" khi \u0111i qua 3 \"m\u00e1y \u0111o\" n\u00e0y s\u1ebd t\u1ea1o ra c\u00e1c vector <code>O'</code> (c\u00e1c ch\u1eef k\u00fd) n\u1eb1m g\u1ea7n nhau trong m\u1ed9t v\u00f9ng kh\u00f4ng gian.</li> <li>T\u1ea5t c\u1ea3 c\u00e1c \u0111i\u1ec3m <code>O</code> thu\u1ed9c l\u1edbp \"\u0110\u1ecf\" s\u1ebd t\u1ea1o ra c\u00e1c ch\u1eef k\u00fd n\u1eb1m g\u1ea7n nhau trong m\u1ed9t v\u00f9ng kh\u00f4ng gian kh\u00e1c.</li> <li>V\u00e0 t\u01b0\u01a1ng t\u1ef1 cho l\u1edbp \"L\u00e1\".</li> </ul> </li> </ul> </li> </ol> <p>S\u01a1 \u0110\u1ed3 ASCII Ph\u1ea3n \u00c1nh \u00dd T\u01b0\u1edfng N\u00e0y</p> Text Only<pre><code>  \u0110i\u1ec3m \u0110\u1ea7u V\u00e0o O(x,y)\n          |\n          |\n+---------+---------+\n|                   |\nv                   v\nM\u00e1y \u0110o 1            M\u00e1y \u0110o 2            M\u00e1y \u0110o 3\n(Ti\u00eau ch\u00ed w1, b1)   (Ti\u00eau ch\u00ed w2, b2)   (Ti\u00eau ch\u00ed w3, b3)\n|                   |                   |\nv                   v                   v\n\u0110i\u1ec3m s\u1ed1 x'          \u0110i\u1ec3m s\u1ed1 y'          \u0110i\u1ec3m s\u1ed1 z'\n|                   |                   |\n+---------+---------+-------------------+\n          |\n          v\n\"Ch\u1eef K\u00fd\" = O'(x', y', z')\n(Vector k\u1ebft qu\u1ea3 trong kh\u00f4ng gian m\u1edbi)\n</code></pre> <p>V\u00ed d\u1ee5: Sau khi hu\u1ea5n luy\u1ec7n, c\u00f3 th\u1ec3 x\u1ea3y ra tr\u01b0\u1eddng h\u1ee3p:</p> <ul> <li>Ti\u00eau ch\u00ed 1 (c\u1ee7a Neuron 1) tr\u1edf th\u00e0nh \"ph\u00e1t hi\u1ec7n \u0111\u01b0\u1eddng cong h\u01b0\u1edbng l\u00ean\".</li> <li>Ti\u00eau ch\u00ed 2 (c\u1ee7a Neuron 2) tr\u1edf th\u00e0nh \"ph\u00e1t hi\u1ec7n v\u1ecb tr\u00ed g\u1ea7n g\u1ed1c t\u1ecda \u0111\u1ed9\".</li> <li>M\u1ed9t \u0111i\u1ec3m <code>O</code> thu\u1ed9c l\u1edbp \"Xanh\" c\u00f3 th\u1ec3 v\u1eeba cong l\u00ean, v\u1eeba g\u1ea7n g\u1ed1c t\u1ecda \u0111\u1ed9. Ch\u1eef k\u00fd c\u1ee7a n\u00f3 s\u1ebd l\u00e0 <code>O'(CAO, CAO, ...)</code>.</li> <li>M\u1ed9t \u0111i\u1ec3m <code>O</code> thu\u1ed9c l\u1edbp \"\u0110\u1ecf\" c\u00f3 th\u1ec3 cong l\u00ean nh\u01b0ng xa g\u1ed1c t\u1ecda \u0111\u1ed9. Ch\u1eef k\u00fd c\u1ee7a n\u00f3 s\u1ebd l\u00e0 <code>O'(CAO, TH\u1ea4P, ...)</code>.</li> </ul> <p>K\u1ebft lu\u1eadn: M\u1ed7i neuron c\u00f3 m\u1ed9t vai tr\u00f2 ri\u00eang. Vai tr\u00f2 \u0111\u00f3 l\u00e0 \"\u0111o l\u01b0\u1eddng m\u1ed9t \u0111\u1eb7c tr\u01b0ng\". \"Ch\u1eef k\u00fd\" cu\u1ed1i c\u00f9ng c\u1ee7a m\u1ed9t \u0111i\u1ec3m d\u1eef li\u1ec7u l\u00e0 t\u1ed5 h\u1ee3p k\u1ebft qu\u1ea3 t\u1eeb t\u1ea5t c\u1ea3 c\u00e1c ph\u00e9p \u0111o \u0111\u1eb7c tr\u01b0ng \u0111\u00f3.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.03_Softmax/03-softmax-diagram/","title":"Softmax Diagram","text":"<p>V\u1ebd l\u1ea1i s\u01a1 \u0111\u1ed3, t\u1eadp trung v\u00e0o m\u1ed9t m\u1eabu d\u1eef li\u1ec7u duy nh\u1ea5t \u0111i qua L\u1edbp Ra (dense2) v\u00e0 H\u00e0m k\u00edch ho\u1ea1t Softmax. S\u01a1 \u0111\u1ed3 n\u00e0y s\u1ebd ti\u1ebfp n\u1ed1i ngay sau khi ch\u00fang ta \u0111\u00e3 c\u00f3 \u0111\u1ea7u ra c\u1ee7a l\u1edbp \u1ea9n 1.</p> <p>Gi\u1ea3 s\u1eed \u0111\u1ea7u ra c\u1ee7a <code>activation1.output</code> cho m\u1ed9t m\u1eabu duy nh\u1ea5t l\u00e0 <code>[0.8, 0.2, 0.0]</code>. Ch\u00fang ta s\u1ebd theo d\u00f5i vector n\u00e0y \u0111i qua c\u00e1c b\u01b0\u1edbc ti\u1ebfp theo.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.03_Softmax/03-softmax-diagram/#so-o-chi-tiet-tu-lop-an-en-phan-phoi-xac-suat-cho-mot-mau","title":"S\u01a1 \u0111\u1ed3 Chi ti\u1ebft: T\u1eeb L\u1edbp \u1ea8n \u0111\u1ebfn Ph\u00e2n ph\u1ed1i X\u00e1c su\u1ea5t (cho m\u1ed9t m\u1eabu)","text":"Text Only<pre><code>\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551                S\u01a0 \u0110\u1ed2 CHI TI\u1ebeT T\u1eeaNG PH\u00c9P T\u00cdNH: L\u1edaP RA (DENSE 2) &amp; SOFTMAX CHO M\u1ed8T M\u1eaaU D\u1eee LI\u1ec6U                            \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\n    (B\u01af\u1edaC 3: \u0110\u1ea6U V\u00c0O CHO L\u1edaP RA)\n    \u0110\u1ea7u ra c\u1ee7a L\u1edbp \u1ea8n 1 cho m\u1ed9t m\u1eabu duy nh\u1ea5t\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 act1.output: [0.8, 0.2, 0.0] \u2502 (Vector 1xH, H=3)\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n                \u25bc\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551                                        (B\u01af\u1edaC 4a: PH\u00c9P BI\u1ebeN \u0110\u1ed4I TUY\u1ebeN T\u00cdNH - L\u1edaP DENSE 2)                                  \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                                                                                    \u2502\n    \u25bc (Ph\u00e9p nh\u00e2n Dot Product: v' \u00b7 W2)                                                   \u25bc (Th\u00f4ng s\u1ed1 c\u1ee7a L\u1edbp Ra)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502[0.8,0.2,0.0]\u2502     \u2502     Weights W2 (HxC)    \u2502                                          \u2502      Biases b2 (1xC)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518     \u2502 [[ 0.1,  0.4, -0.3],    \u2502 (C=3)                                    \u2502      [[2.0, 1.5, 0.9]]  \u2502\n      \u2502           \u2502  [ 0.6, -0.9,  0.2],    \u2502                                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502  [-0.2,  0.5,  0.7]]    \u2502                                                        \u2502\n              \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                        \u2502\n              \u25bc                                                                                    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502          T\u00cdNH TO\u00c1N CHI TI\u1ebeT CHO T\u1eeaNG NEURON RA (OUTPUT NEURON)                                   \u2502 \u2502\n\u2502                                                                                                \u2502 \u2502\n\u2502 Neuron Ra 0: (0.8*0.1) + (0.2*0.6) + (0.0*-0.2) = 0.08 + 0.12 + 0.0 = 0.2 &lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u2502 \u2502\n\u2502 Neuron Ra 1: (0.8*0.4) + (0.2*-0.9) + (0.0*0.5) = 0.32 - 0.18 + 0.0 = 0.14 &lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2510       \u2502 \u2502\n\u2502 Neuron Ra 2: (0.8*-0.3) + (0.2*0.2) + (0.0*0.7) = -0.24 + 0.04 + 0.0 = -0.2 &lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2510   \u2502 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502   \u2502   \u2502   \u2502\n                                                                                       \u2502   \u2502   \u2502   \u2502\n(K\u1ebft qu\u1ea3 v'\u00b7W2) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                    \u25bc   \u25bc   \u25bc   \u2502\n                \u2502 [0.2, 0.14, -0.2] \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba         +         \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     (C\u1ed9ng Bias)\n                                                              \u2502\n                                                              \u25bc\n                                               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                               \u2502   dense2.output (Logits)         \u2502\n                                               \u2502 [0.2+2.0, 0.14+1.5, -0.2+0.9]    \u2502\n                                               \u2502      [2.2, 1.64, 0.7]            \u2502\n                                               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                              \u2502\n                                                              \u25bc\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551                                        (B\u01af\u1edaC 4b: H\u00c0M K\u00cdCH HO\u1ea0T SOFTMAX)                                                  \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n                                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                  \u2502   (i) TR\u1eea \u0110I MAX \u0110\u1ec2 CH\u1ed0NG TR\u00c0N S\u1ed0             \u2502\n                                  \u2502   max([2.2, 1.64, 0.7]) = 2.2                 \u2502\n                                  \u2502   [2.2-2.2, 1.64-2.2, 0.7-2.2]                \u2502\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                          \u2502\n                                                          \u25bc\n                                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                  \u2502      Shifted Logits: [0.0, -0.56, -1.5]       \u2502\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                          \u2502\n                                                          \u25bc\n                                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                  \u2502       (ii) L\u0168Y TH\u1eeaA H\u00d3A (e^x)                 \u2502\n                                  \u2502   e^0.0   = 1.0                               \u2502\n                                  \u2502   e^-0.56 = 0.571                             \u2502\n                                  \u2502   e^-1.5  = 0.223                             \u2502\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                          \u2502\n                                                          \u25bc\n                                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                  \u2502   Exponentiated: [1.0, 0.571, 0.223]          \u2502\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                          \u2502\n                                                          \u25bc\n                                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                  \u2502       (iii) CHU\u1ea8N H\u00d3A (Chia cho t\u1ed5ng)         \u2502\n                                  \u2502   T\u1ed5ng = 1.0 + 0.571 + 0.223 = 1.794          \u2502\n                                  \u2502                                               \u2502\n                                  \u2502   1.0   / 1.794 = 0.557                       \u2502\n                                  \u2502   0.571 / 1.794 = 0.318                       \u2502\n                                  \u2502   0.223 / 1.794 = 0.124                       \u2502\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                          \u2502\n                                                          \u25bc\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551                                              (B\u01af\u1edaC 5: K\u1ebeT QU\u1ea2 CU\u1ed0I C\u00d9NG)                                                \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n                                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                  \u2502      Final Probabilities (\u0110\u1ed9 t\u1ef1 tin)          \u2502\n                                  \u2502         [0.557, 0.318, 0.124]                 \u2502\n                                  \u2502 (T\u1ed5ng = 0.557 + 0.318 + 0.124 = 0.999 \u2248 1.0)  \u2502\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"vi/Explain%20Documents/Ch4pt3r.03_Softmax/03-softmax-diagram/#i-sau-vao-phong-may-cua-mang-no-ron-va-xem-xet-tung-phep-nhan-phep-cong-ma-tran","title":"\u0110i s\u00e2u v\u00e0o \"ph\u00f2ng m\u00e1y\" c\u1ee7a m\u1ea1ng n\u01a1-ron v\u00e0 xem x\u00e9t t\u1eebng ph\u00e9p nh\u00e2n, ph\u00e9p c\u1ed9ng ma tr\u1eadn","text":"<p>Gi\u1ea3 s\u1eed ch\u00fang ta c\u00f3 m\u1ed9t b\u00e0i to\u00e1n \u0111\u01a1n gi\u1ea3n: *   \u0110\u1ea7u v\u00e0o: M\u1ed9t vector c\u00f3 2 \u0111\u1eb7c tr\u01b0ng (v\u00ed d\u1ee5: chi\u1ec1u cao, c\u00e2n n\u1eb7ng). *   Ki\u1ebfn tr\u00fac m\u1ea1ng:     *   L\u1edbp \u1ea9n (Hidden Layer) c\u00f3 3 neuron.     *   L\u1edbp ra (Output Layer) c\u00f3 2 neuron (t\u01b0\u01a1ng \u1ee9ng 2 l\u1edbp, v\u00ed d\u1ee5: \"Lo\u1ea1i A\" v\u00e0 \"Lo\u1ea1i B\"). *   H\u00e0m k\u00edch ho\u1ea1t: ReLU cho l\u1edbp \u1ea9n, Softmax cho l\u1edbp ra.</p> <p>H\u00e3y b\u1eaft \u0111\u1ea7u v\u1edbi c\u00e1c gi\u00e1 tr\u1ecb c\u1ee5 th\u1ec3.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.03_Softmax/03-softmax-diagram/#so-o-chi-tiet-tung-buoc-tinh-toan-ma-tran","title":"S\u01a1 \u0110\u1ed3 Chi Ti\u1ebft: T\u1eebng B\u01b0\u1edbc T\u00ednh To\u00e1n Ma Tr\u1eadn","text":"<p>B\u01b0\u1edbc 0: Kh\u1edfi t\u1ea1o c\u00e1c tham s\u1ed1</p> <ul> <li> <p>D\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o (m\u1ed9t m\u1eabu): <code>inputs = [1, 2]</code>  (shape: 1x2)</p> </li> <li> <p>Tr\u1ecdng s\u1ed1 v\u00e0 bias L\u1edbp \u1ea8n (W1, b1): <code>W1 = [[0.2, 0.8, -0.5],</code> <code>[0.5, -0.9, 0.3]]</code> (shape: 2x3)</p> <p><code>b1 = [2, 3, 0.5]</code> (shape: 1x3)</p> </li> <li> <p>Tr\u1ecdng s\u1ed1 v\u00e0 bias L\u1edbp Ra (W2, b2): <code>W2 = [[0.1, -0.4],</code> <code>[-0.2, 0.6],</code> <code>[0.7, -0.9]]</code> (shape: 3x2)</p> <p><code>b2 = [0.1, -0.2]</code> (shape: 1x2)</p> </li> </ul> <p>B\u01b0\u1edbc 1: T\u00ednh to\u00e1n t\u1ea1i L\u1edbp \u1ea8n (Hidden Layer)</p> <p>Ph\u00e9p to\u00e1n: <code>z1 = inputs \u00b7 W1 + b1</code> (Ph\u00e9p nh\u00e2n ma tr\u1eadn <code>\u00b7</code> v\u00e0 c\u1ed9ng vector)</p> Text Only<pre><code>        [ Tr\u1ecdng s\u1ed1 W1 ]\n         (shape 2x3)\n      [[0.2, 0.8, -0.5],\n       [0.5, -0.9, 0.3]]\n            ^\n            |\n[inputs] \u00b7--+\n(1x2)\n[1, 2]\n\n================== Ph\u00e9p nh\u00e2n ma tr\u1eadn (inputs \u00b7 W1) ==================\n\nK\u1ebft qu\u1ea3 (1x3) = [(1*0.2 + 2*0.5), (1*0.8 + 2*-0.9), (1*-0.5 + 2*0.3)]\n              = [(0.2 + 1.0),   (0.8 - 1.8),    (-0.5 + 0.6)  ]\n              = [1.2,           -1.0,           0.1           ]\n\n================== Ph\u00e9p c\u1ed9ng bias ( + b1 ) =======================\n\n  [1.2, -1.0, 0.1]\n+ [2.0,  3.0, 0.5]\n--------------------\n= [3.2,  2.0, 0.6]  &lt;== \u0110\u00e2y l\u00e0 z1 (logits c\u1ee7a l\u1edbp \u1ea9n)\n</code></pre> <p>B\u01b0\u1edbc 2: \u00c1p d\u1ee5ng h\u00e0m k\u00edch ho\u1ea1t ReLU</p> <p>Ph\u00e9p to\u00e1n: <code>h1 = ReLU(z1) = max(0, z1)</code></p> Text Only<pre><code>  Input v\u00e0o ReLU: z1 = [3.2, 2.0, 0.6]\n\n  max(0, 3.2) -&gt; 3.2\n  max(0, 2.0) -&gt; 2.0\n  max(0, 0.6) -&gt; 0.6\n\n  K\u1ebft qu\u1ea3: h1 = [3.2, 2.0, 0.6] &lt;== \u0110\u1ea7u ra c\u1ee7a l\u1edbp \u1ea9n sau k\u00edch ho\u1ea1t\n  (Trong tr\u01b0\u1eddng h\u1ee3p n\u00e0y kh\u00f4ng c\u00f3 gi\u00e1 tr\u1ecb n\u00e0o b\u1ecb c\u1eaft v\u00ec t\u1ea5t c\u1ea3 \u0111\u1ec1u d\u01b0\u01a1ng)\n</code></pre> <p>B\u01b0\u1edbc 3: T\u00ednh to\u00e1n t\u1ea1i L\u1edbp Ra (Output Layer)</p> <p>Ph\u00e9p to\u00e1n: <code>z_out = h1 \u00b7 W2 + b2</code></p> Text Only<pre><code>        [ Tr\u1ecdng s\u1ed1 W2 ]\n         (shape 3x2)\n      [[0.1, -0.4],\n       [-0.2, 0.6],\n       [0.7, -0.9]]\n            ^\n            |\n   [h1] \u00b7---+\n  (1x3)\n[3.2, 2.0, 0.6]\n\n================== Ph\u00e9p nh\u00e2n ma tr\u1eadn (h1 \u00b7 W2) =====================\n\nK\u1ebft qu\u1ea3 (1x2) = [ (3.2*0.1 + 2.0*-0.2 + 0.6*0.7),  (3.2*-0.4 + 2.0*0.6 + 0.6*-0.9) ]\n              = [ (0.32  - 0.4    + 0.42),       (-1.28    + 1.2    - 0.54)       ]\n              = [ 0.34,                          -0.62                         ]\n\n================== Ph\u00e9p c\u1ed9ng bias ( + b2 ) ========================\n\n  [0.34, -0.62]\n+ [0.1,  -0.2]\n--------------------\n= [0.44, -0.82] &lt;== \u0110\u00e2y l\u00e0 z_out (logits cu\u1ed1i c\u00f9ng tr\u01b0\u1edbc Softmax)\n</code></pre> <p>B\u01b0\u1edbc 4: \u00c1p d\u1ee5ng h\u00e0m k\u00edch ho\u1ea1t Softmax</p> <p>Ph\u00e9p to\u00e1n: <code>probabilities = Softmax(z_out)</code></p> <ol> <li> <p>L\u0169y th\u1eeba h\u00f3a: <code>e^z_out</code> <code>e^0.44  \u2248 1.55</code> <code>e^-0.82 \u2248 0.44</code></p> </li> <li> <p>T\u00ednh t\u1ed5ng: <code>T\u1ed5ng = 1.55 + 0.44 = 1.99</code></p> </li> <li> <p>Chu\u1ea9n h\u00f3a: <code>X\u00e1c su\u1ea5t L\u1edbp A = 1.55 / 1.99 \u2248 0.779</code> <code>X\u00e1c su\u1ea5t L\u1edbp B = 0.44 / 1.99 \u2248 0.221</code></p> </li> </ol>"},{"location":"vi/Explain%20Documents/Ch4pt3r.03_Softmax/03-softmax-diagram/#ket-qua-cuoi-cung","title":"K\u1ebeT QU\u1ea2 CU\u1ed0I C\u00d9NG","text":"Text Only<pre><code>  Input: [1, 2]\n    |\n    V\n  z1 = [3.2, 2.0, 0.6]  (Sau L\u1edbp \u1ea8n 1)\n    |\n    V (ReLU)\n  h1 = [3.2, 2.0, 0.6]\n    |\n    V\n  z_out = [0.44, -0.82] (Sau L\u1edbp Ra)\n    |\n    V (Softmax)\n  Probabilities = [0.779, 0.221]\n\n==&gt; D\u1ef1 \u0111o\u00e1n: \"Lo\u1ea1i A\" v\u1edbi x\u00e1c su\u1ea5t 77.9%\n</code></pre> <p>S\u01a1 \u0111\u1ed3 chi ti\u1ebft n\u00e0y cho th\u1ea5y ch\u00ednh x\u00e1c c\u00e1ch m\u1ed9t vector \u0111\u1ea7u v\u00e0o <code>[1, 2]</code> \u0111i qua t\u1eebng l\u1edbp, qua t\u1eebng ph\u00e9p nh\u00e2n ma tr\u1eadn, c\u1ed9ng bias v\u00e0 h\u00e0m k\u00edch ho\u1ea1t \u0111\u1ec3 cu\u1ed1i c\u00f9ng cho ra m\u1ed9t d\u1ef1 \u0111o\u00e1n x\u00e1c su\u1ea5t c\u1ee5 th\u1ec3. M\u1ed7i b\u01b0\u1edbc \u0111\u1ec1u l\u00e0 m\u1ed9t ph\u00e9p to\u00e1n ma tr\u1eadn ho\u1eb7c vector \u0111\u01a1n gi\u1ea3n. Qu\u00e1 tr\u00ecnh \"h\u1ecdc\" c\u1ee7a m\u1ea1ng n\u01a1-ron ch\u00ednh l\u00e0 vi\u1ec7c \u0111i\u1ec1u ch\u1ec9nh c\u00e1c gi\u00e1 tr\u1ecb trong ma tr\u1eadn <code>W1, b1, W2, b2</code> \u0111\u1ec3 k\u1ebft qu\u1ea3 cu\u1ed1i c\u00f9ng ng\u00e0y c\u00e0ng ch\u00ednh x\u00e1c.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.03_Softmax/03-softmax/","title":"Softmax","text":""},{"location":"vi/Explain%20Documents/Ch4pt3r.03_Softmax/03-softmax/#giai-thich-ve-softmax-trong-neural-network","title":"Gi\u1ea3i th\u00edch v\u1ec1 Softmax trong Neural Network","text":""},{"location":"vi/Explain%20Documents/Ch4pt3r.03_Softmax/03-softmax/#phan-1-tai-sao-chung-ta-can-softmax-van-e-la-gi","title":"Ph\u1ea7n 1: T\u1ea1i sao ch\u00fang ta c\u1ea7n Softmax? V\u1ea5n \u0111\u1ec1 l\u00e0 g\u00ec?","text":"<p>\u1ede c\u00e1c ch\u01b0\u01a1ng tr\u01b0\u1edbc, b\u1ea1n c\u00f3 th\u1ec3 \u0111\u00e3 l\u00e0m quen v\u1edbi h\u00e0m k\u00edch ho\u1ea1t ReLU (Rectified Linear Unit). ReLU r\u1ea5t t\u1ed1t cho c\u00e1c l\u1edbp \u1ea9n (hidden layers) nh\u01b0ng l\u1ea1i c\u00f3 m\u1ed9t v\u00e0i v\u1ea5n \u0111\u1ec1 n\u1ebfu d\u00f9ng cho l\u1edbp cu\u1ed1i c\u00f9ng (output layer) c\u1ee7a m\u1ed9t m\u1ea1ng ph\u00e2n lo\u1ea1i:</p> <ol> <li>Kh\u00f4ng b\u1ecb ch\u1eb7n (Unbounded): \u0110\u1ea7u ra c\u1ee7a ReLU c\u00f3 th\u1ec3 l\u00e0 b\u1ea5t k\u1ef3 s\u1ed1 d\u01b0\u01a1ng n\u00e0o (v\u00ed d\u1ee5: <code>[12, 99, 318]</code>). Nh\u1eefng con s\u1ed1 n\u00e0y \u0111\u1ee9ng m\u1ed9t m\u00ecnh kh\u00f4ng c\u00f3 nhi\u1ec1u \u00fd ngh\u0129a. 318 l\u1edbn h\u01a1n 99, nh\u01b0ng l\u1edbn h\u01a1n \"bao nhi\u00eau\"? Li\u1ec7u n\u00f3 c\u00f3 \"ch\u1eafc ch\u1eafn\" h\u01a1n nhi\u1ec1u kh\u00f4ng? Ch\u00fang ta kh\u00f4ng c\u00f3 m\u1ed9t \"ng\u1eef c\u1ea3nh\" \u0111\u1ec3 so s\u00e1nh.</li> <li>Kh\u00f4ng \u0111\u01b0\u1ee3c chu\u1ea9n h\u00f3a (Not Normalized): C\u00e1c gi\u00e1 tr\u1ecb \u0111\u1ea7u ra kh\u00f4ng c\u00f3 m\u1ed1i li\u00ean h\u1ec7 t\u1ed5ng th\u1ec3. T\u1ed5ng c\u1ee7a ch\u00fang kh\u00f4ng b\u1eb1ng m\u1ed9t con s\u1ed1 c\u1ed1 \u0111\u1ecbnh n\u00e0o c\u1ea3.</li> <li>\u0110\u1ed9c quy\u1ec1n (Exclusive): \u0110\u1ea7u ra c\u1ee7a m\u1ed7i neuron l\u00e0 \u0111\u1ed9c l\u1eadp v\u1edbi c\u00e1c neuron kh\u00e1c.</li> </ol> <p>M\u1ee5c ti\u00eau c\u1ee7a ch\u00fang ta: V\u1edbi b\u00e0i to\u00e1n ph\u00e2n lo\u1ea1i, ch\u00fang ta mu\u1ed1n m\u1ea1ng n\u01a1-ron \"n\u00f3i\" cho ch\u00fang ta bi\u1ebft n\u00f3 \"ngh\u0129\" r\u1eb1ng \u0111\u1ea7u v\u00e0o thu\u1ed9c v\u1ec1 l\u1edbp n\u00e0o v\u1edbi m\u1ed9t m\u1ee9c \u0111\u1ed9 t\u1ef1 tin (confidence) r\u00f5 r\u00e0ng. V\u00ed d\u1ee5, v\u1edbi 3 l\u1edbp (ch\u00f3, m\u00e8o, chim), ch\u00fang ta mu\u1ed1n \u0111\u1ea7u ra c\u00f3 d\u1ea1ng nh\u01b0 <code>[0.05, 0.9, 0.05]</code>, ngh\u0129a l\u00e0: \"T\u00f4i tin ch\u1eafc 90% \u0111\u00e2y l\u00e0 m\u00e8o, 5% l\u00e0 ch\u00f3 v\u00e0 5% l\u00e0 chim.\"</p> <p>=&gt; Softmax ra \u0111\u1eddi \u0111\u1ec3 gi\u1ea3i quy\u1ebft v\u1ea5n \u0111\u1ec1 n\u00e0y. N\u00f3 nh\u1eadn v\u00e0o c\u00e1c s\u1ed1 th\u1ef1c b\u1ea5t k\u1ef3 (c\u00f3 th\u1ec3 \u00e2m, d\u01b0\u01a1ng, l\u1edbn, nh\u1ecf) v\u00e0 bi\u1ebfn ch\u00fang th\u00e0nh m\u1ed9t ph\u00e2n ph\u1ed1i x\u00e1c su\u1ea5t (probability distribution). C\u00e1c \u0111\u1eb7c \u0111i\u1ec3m c\u1ee7a ph\u00e2n ph\u1ed1i x\u00e1c su\u1ea5t n\u00e0y l\u00e0: *   T\u1ea5t c\u1ea3 c\u00e1c gi\u00e1 tr\u1ecb \u0111\u1ea7u ra \u0111\u1ec1u n\u1eb1m trong kho\u1ea3ng <code>[0, 1]</code>. *   T\u1ed5ng c\u1ee7a t\u1ea5t c\u1ea3 c\u00e1c gi\u00e1 tr\u1ecb \u0111\u1ea7u ra lu\u00f4n b\u1eb1ng 1.</p> <p>Nh\u1eefng gi\u00e1 tr\u1ecb n\u00e0y ch\u00ednh l\u00e0 \u0111i\u1ec3m t\u1ef1 tin (confidence scores) m\u00e0 ch\u00fang ta c\u1ea7n.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.03_Softmax/03-softmax/#muc-2-giai-phau-cong-thuc-softmax","title":"M\u1ee5c 2: \"Gi\u1ea3i ph\u1eabu\" c\u00f4ng th\u1ee9c Softmax","text":"<p>C\u00f4ng th\u1ee9c trong s\u00e1ch c\u00f3 v\u1ebb \u0111\u00e1ng s\u1ee3:</p> \\[ S_{i,j} = \\frac{e^{z_{i,j}}}{\\sum_{l=1}^{L} e^{z_{i,l}}} \\] <p>\u0110\u1eebng lo, h\u00e3y chia n\u00f3 th\u00e0nh 2 b\u01b0\u1edbc c\u1ef1c k\u1ef3 \u0111\u01a1n gi\u1ea3n:</p> <p>B\u01b0\u1edbc 1: L\u0169y th\u1eeba h\u00f3a (Exponentiation) - T\u1eed s\u1ed1 <code>e^z</code></p> <ul> <li><code>z</code> l\u00e0 c\u00e1c gi\u00e1 tr\u1ecb \u0111\u1ea7u ra t\u1eeb l\u1edbp tr\u01b0\u1edbc (v\u00ed d\u1ee5 <code>layer_outputs = [4.8, 1.21, 2.385]</code>).</li> <li><code>e</code> l\u00e0 h\u1eb1ng s\u1ed1 Euler (x\u1ea5p x\u1ec9 2.71828), l\u00e0 c\u01a1 s\u1ed1 c\u1ee7a logarit t\u1ef1 nhi\u00ean.</li> <li>\"L\u0169y th\u1eeba h\u00f3a\" \u0111\u01a1n gi\u1ea3n l\u00e0 l\u1ea5y <code>e</code> m\u0169 c\u00e1c gi\u00e1 tr\u1ecb <code>z</code> \u0111\u00f3. Trong Python, ch\u00fang ta d\u00f9ng <code>E ** output</code> ho\u1eb7c <code>math.exp(output)</code>.</li> </ul> Python<pre><code># V\u00ed d\u1ee5 t\u1eeb s\u00e1ch\nlayer_outputs = [4.8, 1.21, 2.385]\nE = 2.71828182846\n\n# T\u00ednh e^z cho m\u1ed7i gi\u00e1 tr\u1ecb\nexp_values = [E**4.8, E**1.21, E**2.385] \n# K\u1ebft qu\u1ea3: [121.51, 3.35, 10.86]\n</code></pre> <p>T\u1ea1i sao ph\u1ea3i l\u00e0m b\u01b0\u1edbc n\u00e0y? 1.  Lo\u1ea1i b\u1ecf s\u1ed1 \u00e2m: <code>e</code> m\u0169 b\u1ea5t c\u1ee9 s\u1ed1 n\u00e0o c\u0169ng lu\u00f4n cho ra k\u1ebft qu\u1ea3 d\u01b0\u01a1ng. \u0110i\u1ec1u n\u00e0y r\u1ea5t quan tr\u1ecdng v\u00ec x\u00e1c su\u1ea5t kh\u00f4ng th\u1ec3 l\u00e0 s\u1ed1 \u00e2m. 2.  Khu\u1ebfch \u0111\u1ea1i s\u1ef1 kh\u00e1c bi\u1ec7t: H\u00e0m m\u0169 l\u00e0m cho c\u00e1c gi\u00e1 tr\u1ecb l\u1edbn c\u00e0ng l\u1edbn h\u01a1n m\u1ed9t c\u00e1ch v\u01b0\u1ee3t tr\u1ed9i so v\u1edbi c\u00e1c gi\u00e1 tr\u1ecb nh\u1ecf. Gi\u00e1 tr\u1ecb <code>4.8</code> ch\u1ec9 l\u1edbn h\u01a1n <code>2.385</code> kho\u1ea3ng 2 l\u1ea7n, nh\u01b0ng sau khi l\u0169y th\u1eeba, <code>121.51</code> l\u1edbn h\u01a1n <code>10.86</code> t\u1edbi h\u01a1n 11 l\u1ea7n! \u0110i\u1ec1u n\u00e0y gi\u00fap m\u1ea1ng \"t\u1ef1 tin\" h\u01a1n v\u00e0o d\u1ef1 \u0111o\u00e1n c\u00f3 \u0111i\u1ec3m s\u1ed1 cao nh\u1ea5t.</p> <p>B\u01b0\u1edbc 2: Chu\u1ea9n h\u00f3a (Normalization) - Ph\u00e9p chia</p> <p>Sau khi c\u00f3 c\u00e1c gi\u00e1 tr\u1ecb \u0111\u00e3 \u0111\u01b0\u1ee3c l\u0169y th\u1eeba (<code>exp_values</code>), ch\u00fang ta ch\u1ec9 c\u1ea7n l\u00e0m m\u1ed9t vi\u1ec7c:</p> <ol> <li>T\u00ednh t\u1ed5ng t\u1ea5t c\u1ea3 c\u00e1c gi\u00e1 tr\u1ecb \u0111\u00f3 (m\u1eabu s\u1ed1 \\(\\sum_{l=1}^{L} e^{z_{i,l}}\\)).</li> <li>L\u1ea5y t\u1eebng gi\u00e1 tr\u1ecb chia cho t\u1ed5ng v\u1eeba t\u00ednh \u0111\u01b0\u1ee3c.</li> </ol> Python<pre><code># Ti\u1ebfp n\u1ed1i v\u00ed d\u1ee5 tr\u00ean\nexp_values = [121.51, 3.35, 10.86]\n\n# 1. T\u00ednh t\u1ed5ng\nnorm_base = sum(exp_values) # 121.51 + 3.35 + 10.86 = 135.72\n\n# 2. Chia t\u1eebng gi\u00e1 tr\u1ecb cho t\u1ed5ng\nnorm_values = [\n    121.51 / norm_base, # ~0.895\n    3.35 / norm_base,   # ~0.025\n    10.86 / norm_base   # ~0.080\n]\n\n# K\u1ebft qu\u1ea3: [0.895, 0.025, 0.080]\n# Ki\u1ec3m tra: 0.895 + 0.025 + 0.080 = 1.0\n</code></pre> <p>V\u1eady l\u00e0 xong! Ch\u00fang ta \u0111\u00e3 bi\u1ebfn <code>[4.8, 1.21, 2.385]</code> th\u00e0nh m\u1ed9t ph\u00e2n ph\u1ed1i x\u00e1c su\u1ea5t <code>[0.895, 0.025, 0.080]</code>.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.03_Softmax/03-softmax/#muc-3-toi-uu-voi-numpy-va-xu-ly-theo-lo-batch","title":"M\u1ee5c 3: T\u1ed1i \u01b0u v\u1edbi NumPy v\u00e0 x\u1eed l\u00fd theo L\u00f4 (Batch)","text":"<p>Trong th\u1ef1c t\u1ebf, ch\u00fang ta kh\u00f4ng x\u1eed l\u00fd t\u1eebng m\u1eabu d\u1eef li\u1ec7u m\u1ed9t m\u00e0 x\u1eed l\u00fd c\u1ea3 m\u1ed9t l\u00f4 (batch) \u0111\u1ec3 t\u0103ng t\u1ed1c \u0111\u1ed9. M\u1ed9t l\u00f4 d\u1eef li\u1ec7u s\u1ebd c\u00f3 d\u1ea1ng m\u1ed9t ma tr\u1eadn, trong \u0111\u00f3 m\u1ed7i h\u00e0ng l\u00e0 \u0111\u1ea7u ra cho m\u1ed9t m\u1eabu.</p> Python<pre><code># M\u1ed9t l\u00f4 c\u00f3 3 m\u1eabu, m\u1ed7i m\u1eabu c\u00f3 3 \u0111\u1ea7u ra\nlayer_outputs = np.array([[4.8, 1.21, 2.385],\n                          [8.9, -1.81, 0.2],\n                          [1.41, 1.051, 0.026]])\n</code></pre> <p>B\u00e2y gi\u1edd, ch\u00fang ta c\u1ea7n t\u00ednh Softmax cho t\u1eebng h\u00e0ng m\u1ed9t. \u0110\u00e2y l\u00e0 l\u00fac c\u00e1c tham s\u1ed1 <code>axis</code> v\u00e0 <code>keepdims</code> c\u1ee7a NumPy ph\u00e1t huy t\u00e1c d\u1ee5ng.</p> <ul> <li><code>np.exp(layer_outputs)</code>: NumPy th\u00f4ng minh s\u1ebd t\u1ef1 \u0111\u1ed9ng t\u00ednh l\u0169y th\u1eeba cho m\u1ecdi ph\u1ea7n t\u1eed trong ma tr\u1eadn.</li> <li><code>np.sum(..., axis=1)</code>: Ch\u00fang ta c\u1ea7n t\u00ednh t\u1ed5ng c\u1ee7a c\u00e1c gi\u00e1 tr\u1ecb tr\u00ean m\u1ed7i h\u00e0ng.<ul> <li><code>axis=0</code>: t\u00ednh t\u1ed5ng theo c\u1ed9t.</li> <li><code>axis=1</code>: t\u00ednh t\u1ed5ng theo h\u00e0ng. \u0110\u00e2y l\u00e0 c\u00e1i ch\u00fang ta c\u1ea7n.</li> </ul> </li> <li><code>keepdims=True</code>: Khi t\u00ednh t\u1ed5ng theo <code>axis=1</code>, k\u1ebft qu\u1ea3 s\u1ebd l\u00e0 m\u1ed9t vector h\u00e0ng <code>[8.395, 7.29, 2.487]</code>. N\u1ebfu ch\u00fang ta l\u1ea5y ma tr\u1eadn <code>(3, 3)</code> chia cho vector <code>(3,)</code>, NumPy c\u00f3 th\u1ec3 b\u00e1o l\u1ed7i ho\u1eb7c kh\u00f4ng th\u1ef1c hi\u1ec7n \u0111\u00fang ph\u00e9p chia theo h\u00e0ng. <code>keepdims=True</code> s\u1ebd gi\u1eef nguy\u00ean s\u1ed1 chi\u1ec1u, bi\u1ebfn k\u1ebft qu\u1ea3 th\u00e0nh m\u1ed9t vector c\u1ed9t <code>[[8.395], [7.29], [2.487]]</code> c\u00f3 shape <code>(3, 1)</code>. L\u00fac n\u00e0y, NumPy c\u00f3 th\u1ec3 th\u1ef1c hi\u1ec7n ph\u00e9p chia ma tr\u1eadn <code>(3, 3)</code> cho vector c\u1ed9t <code>(3, 1)</code> m\u1ed9t c\u00e1ch ch\u00ednh x\u00e1c (m\u1ed7i h\u00e0ng c\u1ee7a ma tr\u1eadn \u0111\u01b0\u1ee3c chia cho gi\u00e1 tr\u1ecb t\u01b0\u01a1ng \u1ee9ng trong vector c\u1ed9t).</li> </ul>"},{"location":"vi/Explain%20Documents/Ch4pt3r.03_Softmax/03-softmax/#muc-4-bi-kip-chong-tran-so-overflow-prevention","title":"M\u1ee5c 4: \"B\u00ed k\u00edp\" ch\u1ed1ng tr\u00e0n s\u1ed1 (Overflow Prevention)","text":"<p>H\u00e0m m\u0169 <code>e^x</code> t\u0103ng r\u1ea5t nhanh. N\u1ebfu \u0111\u1ea7u v\u00e0o <code>z</code> l\u00e0 m\u1ed9t s\u1ed1 l\u1edbn (v\u00ed d\u1ee5 <code>1000</code>), <code>np.exp(1000)</code> s\u1ebd tr\u1ea3 v\u1ec1 <code>inf</code> (v\u00f4 c\u1ef1c), g\u00e2y ra l\u1ed7i tr\u00e0n s\u1ed1 (overflow) v\u00e0 l\u00e0m h\u1ecfng to\u00e0n b\u1ed9 ph\u00e9p t\u00ednh.</p> <p>Gi\u1ea3i ph\u00e1p: Ch\u00fang ta c\u00f3 th\u1ec3 tr\u1eeb \u0111i m\u1ed9t s\u1ed1 b\u1ea5t k\u1ef3 t\u1eeb t\u1ea5t c\u1ea3 c\u00e1c gi\u00e1 tr\u1ecb \u0111\u1ea7u v\u00e0o <code>z</code> m\u00e0 kh\u00f4ng l\u00e0m thay \u0111\u1ed5i k\u1ebft qu\u1ea3 cu\u1ed1i c\u00f9ng c\u1ee7a Softmax. T\u1ea1i sao? V\u00ec t\u00ednh ch\u1ea5t c\u1ee7a ph\u00e9p l\u0169y th\u1eeba v\u00e0 ph\u00e9p chia: $$ \\frac{e<sup>{z_1}}{e</sup>{z_1} + e^{z_2}} = \\frac{e^{z_1} \\cdot e<sup>{-C}}{e</sup>{z_1} \\cdot e^{-C} + e^{z_2} \\cdot e^{-C}} = \\frac{e^{z_1 - C}}{e^{z_1 - C} + e^{z_2 - C}} $$</p> <p>V\u1eady ch\u00fang ta n\u00ean tr\u1eeb \u0111i s\u1ed1 n\u00e0o? S\u1ed1 l\u1edbn nh\u1ea5t (max) trong c\u00e1c gi\u00e1 tr\u1ecb \u0111\u1ea7u v\u00e0o c\u1ee7a h\u00e0ng \u0111\u00f3.</p> Python<pre><code>inputs = [1, 2, 3]\nmax_value = 3\nshifted_inputs = [1-3, 2-3, 3-3] # -&gt; [-2, -1, 0]\n</code></pre> <p>L\u1ee3i \u00edch c\u1ee7a vi\u1ec7c n\u00e0y: 1.  Gi\u00e1 tr\u1ecb l\u1edbn nh\u1ea5t sau khi tr\u1eeb s\u1ebd l\u00e0 <code>0</code>. (<code>e^0 = 1</code>) 2.  T\u1ea5t c\u1ea3 c\u00e1c gi\u00e1 tr\u1ecb kh\u00e1c s\u1ebd l\u00e0 s\u1ed1 \u00e2m. (<code>e</code> m\u0169 s\u1ed1 \u00e2m lu\u00f4n l\u00e0 m\u1ed9t s\u1ed1 nh\u1ecf h\u01a1n 1). 3.  \u0110i\u1ec1u n\u00e0y \u0111\u1ea3m b\u1ea3o r\u1eb1ng \u0111\u1ea7u v\u00e0o cho h\u00e0m <code>exp</code> s\u1ebd kh\u00f4ng bao gi\u1edd l\u00e0 m\u1ed9t s\u1ed1 d\u01b0\u01a1ng l\u1edbn, t\u1eeb \u0111\u00f3 ng\u0103n ch\u1eb7n ho\u00e0n to\u00e0n l\u1ed7i tr\u00e0n s\u1ed1.</p> <p>\u0110\u00e2y ch\u00ednh l\u00e0 l\u00fd do trong \u0111o\u1ea1n code cu\u1ed1i c\u00f9ng c\u1ee7a s\u00e1ch, b\u1ea1n s\u1ebd th\u1ea5y d\u00f2ng n\u00e0y: Python<pre><code>exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n</code></pre> \u0110\u00e2y l\u00e0 phi\u00ean b\u1ea3n Softmax ho\u00e0n ch\u1ec9nh, an to\u00e0n v\u00e0 hi\u1ec7u qu\u1ea3.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.03_Softmax/03-softmax/#phan-2-giai-thich-truu-tuong-de-hieu","title":"Ph\u1ea7n 2: Gi\u1ea3i Th\u00edch Tr\u1eebu T\u01b0\u1ee3ng &amp; D\u1ec5 Hi\u1ec3u","text":"<p>H\u00e3y qu\u00ean \u0111i c\u00f4ng th\u1ee9c to\u00e1n h\u1ecdc v\u00e0 code. H\u00e3y t\u01b0\u1edfng t\u01b0\u1ee3ng Softmax l\u00e0 m\u1ed9t \"M\u00e1y Ph\u00e2n B\u1ed5 S\u1ef1 T\u1ef1 Tin\" cho m\u1ed9t cu\u1ed9c thi t\u00e0i n\u0103ng.</p> <p>1. V\u00f2ng S\u01a1 Kh\u1ea3o - \u0110i\u1ec3m th\u00f4 (Raw Scores)</p> <p>Gi\u1ea3 s\u1eed c\u00f3 3 th\u00ed sinh (Ch\u00f3, M\u00e8o, Chim) tham gia m\u1ed9t cu\u1ed9c thi. Ban gi\u00e1m kh\u1ea3o (l\u1edbp m\u1ea1ng n\u01a1-ron ph\u00eda tr\u01b0\u1edbc) cho \u0111i\u1ec3m th\u00f4. \u0110i\u1ec3m n\u00e0y c\u00f3 th\u1ec3 r\u1ea5t l\u1ed9n x\u1ed9n: <code>\u0110i\u1ec3m th\u00f4 = [4.8, 1.21, 2.385]</code></p> <p>Nh\u00ecn v\u00e0o \u0111i\u1ec3m n\u00e0y, ch\u00fang ta bi\u1ebft th\u00ed sinh Ch\u00f3 c\u00f3 \u0111i\u1ec3m cao nh\u1ea5t, nh\u01b0ng \"cao h\u01a1n\" nh\u01b0 th\u1ebf n\u00e0o? M\u1ee9c \u0111\u1ed9 \"chi\u1ebfn th\u1eafng\" c\u00f3 \u00e1p \u0111\u1ea3o kh\u00f4ng? R\u1ea5t kh\u00f3 n\u00f3i.</p> <p>2. B\u01b0\u1edbc 1: M\u00e1y \"Hype\" - L\u0169y th\u1eeba h\u00f3a</p> <p>\u0110\u1ec3 l\u00e0m cho k\u1ebft qu\u1ea3 r\u00f5 r\u00e0ng h\u01a1n, MC cho c\u00e1c \u0111i\u1ec3m s\u1ed1 n\u00e0y v\u00e0o m\u1ed9t c\u00e1i M\u00e1y \"Hype\". M\u00e1y n\u00e0y c\u00f3 2 ch\u1ee9c n\u0103ng: *   Kh\u00f4ng c\u00f3 \u0111i\u1ec3m \u00e2m: N\u00f3 bi\u1ebfn m\u1ecdi \u0111i\u1ec3m s\u1ed1 th\u00e0nh \u0111i\u1ec3m \"nhi\u1ec7t t\u00ecnh\" (lu\u00f4n d\u01b0\u01a1ng). *   T\u00e2ng b\u1ed1c ng\u01b0\u1eddi gi\u1ecfi nh\u1ea5t: M\u00e1y n\u00e0y c\u1ef1c k\u1ef3 \"thi\u00ean v\u1ecb\". Ai \u0111i\u1ec3m cao s\u1eb5n r\u1ed3i s\u1ebd \u0111\u01b0\u1ee3c t\u00e2ng b\u1ed1c l\u00ean t\u1eadn m\u00e2y xanh, trong khi ng\u01b0\u1eddi \u0111i\u1ec3m th\u1ea5p ch\u1ec9 \u0111\u01b0\u1ee3c t\u0103ng nh\u1eb9.</p> <p>Sau khi qua M\u00e1y \"Hype\" (t\u1ee9c l\u00e0 <code>e^x</code>): <code>\u0110i\u1ec3m Hype = [121.5, 3.4, 10.9]</code></p> <p>B\u00e2y gi\u1edd th\u00ec s\u1ef1 kh\u00e1c bi\u1ec7t \u0111\u00e3 qu\u00e1 r\u00f5 r\u00e0ng! Th\u00ed sinh Ch\u00f3 kh\u00f4ng ch\u1ec9 cao \u0111i\u1ec3m h\u01a1n, m\u00e0 c\u00f2n \u00e1p \u0111\u1ea3o ho\u00e0n to\u00e0n ph\u1ea7n c\u00f2n l\u1ea1i.</p> <p>3. B\u01b0\u1edbc 2: Chia \"Chi\u1ebfc B\u00e1nh T\u1ef1 Tin\" - Chu\u1ea9n h\u00f3a</p> <p>B\u00e2y gi\u1edd, \u0111\u1ec3 kh\u00e1n gi\u1ea3 d\u1ec5 hi\u1ec3u, MC quy\u1ebft \u0111\u1ecbnh kh\u00f4ng d\u00f9ng \u0111i\u1ec3m hype n\u1eefa m\u00e0 s\u1ebd chia m\u1ed9t \"chi\u1ebfc b\u00e1nh t\u1ef1 tin\" 100% cho 3 th\u00ed sinh, d\u1ef1a tr\u00ean t\u1ef7 l\u1ec7 \u0111i\u1ec3m hype c\u1ee7a h\u1ecd.</p> <ul> <li>T\u1ed5ng \u0111i\u1ec3m Hype = 121.5 + 3.4 + 10.9 = 135.8</li> <li>Ph\u1ea7n b\u00e1nh c\u1ee7a Ch\u00f3: <code>121.5 / 135.8 \u2248 89.5%</code></li> <li>Ph\u1ea7n b\u00e1nh c\u1ee7a M\u00e8o: <code>3.4 / 135.8 \u2248 2.5%</code></li> <li>Ph\u1ea7n b\u00e1nh c\u1ee7a Chim: <code>10.9 / 135.8 \u2248 8.0%</code></li> </ul> <p>K\u1ebft qu\u1ea3 cu\u1ed1i c\u00f9ng: <code>M\u1ee9c \u0111\u1ed9 t\u1ef1 tin = [0.895, 0.025, 0.080]</code></p> <p>\u0110\u00e2y ch\u00ednh l\u00e0 \u0111\u1ea7u ra c\u1ee7a Softmax. N\u00f3 cho ch\u00fang ta m\u1ed9t k\u1ebft lu\u1eadn r\u1ea5t r\u00f5 r\u00e0ng: \"D\u1ef1a tr\u00ean m\u00e0n tr\u00ecnh di\u1ec5n, t\u00f4i tin ch\u1eafc 89.5% ng\u01b0\u1eddi chi\u1ebfn th\u1eafng l\u00e0 Ch\u00f3.\"</p> <p>V\u1ec1 \"b\u00ed k\u00edp\" ch\u1ed1ng tr\u00e0n s\u1ed1: H\u00e3y t\u01b0\u1edfng t\u01b0\u1ee3ng m\u1ed9t gi\u00e1m kh\u1ea3o qu\u00e1 ph\u1ea5n kh\u00edch cho \u0111i\u1ec3m <code>1000</code>. M\u00e1y \"Hype\" s\u1ebd b\u1ecb \"ch\u00e1y\" (overflow). MC th\u00f4ng minh nh\u1eadn ra r\u1eb1ng \u0111i\u1ec1u quan tr\u1ecdng l\u00e0 s\u1ef1 ch\u00eanh l\u1ec7ch \u0111i\u1ec3m s\u1ed1 ch\u1ee9 kh\u00f4ng ph\u1ea3i b\u1ea3n th\u00e2n \u0111i\u1ec3m s\u1ed1. V\u00ec v\u1eady, tr\u01b0\u1edbc khi cho v\u00e0o m\u00e1y, anh ta t\u00ecm \u0111i\u1ec3m cao nh\u1ea5t (1000) v\u00e0 tr\u1eeb n\u00f3 kh\u1ecfi \u0111i\u1ec3m c\u1ee7a m\u1ecdi ng\u01b0\u1eddi. K\u1ebft qu\u1ea3 cu\u1ed1i c\u00f9ng sau khi chia b\u00e1nh v\u1eabn kh\u00f4ng h\u1ec1 thay \u0111\u1ed5i, nh\u01b0ng c\u00e1i m\u00e1y hype \u0111\u00e3 \u0111\u01b0\u1ee3c c\u1ee9u!</p> <p>T\u00f3m l\u1ea1i, Softmax l\u00e0m hai vi\u1ec7c:</p> <ol> <li>S\u1eed d\u1ee5ng h\u00e0m m\u0169 <code>e^x</code> \u0111\u1ec3 khu\u1ebfch \u0111\u1ea1i \u0111i\u1ec3m s\u1ed1 cao nh\u1ea5t, bi\u1ebfn n\u00f3 th\u00e0nh \"ng\u01b0\u1eddi d\u1eabn \u0111\u1ea7u\" r\u00f5 r\u1ec7t.</li> <li>Chu\u1ea9n h\u00f3a c\u00e1c \u0111i\u1ec3m s\u1ed1 \u0111\u00e3 \u0111\u01b0\u1ee3c khu\u1ebfch \u0111\u1ea1i \u0111\u00f3 th\u00e0nh m\u1ed9t t\u1ef7 l\u1ec7 ph\u1ea7n tr\u0103m (ho\u1eb7c x\u00e1c su\u1ea5t), \u0111\u1ec3 t\u1ea5t c\u1ea3 c\u1ed9ng l\u1ea1i b\u1eb1ng 1.</li> </ol>"},{"location":"vi/Explain%20Documents/Ch4pt3r.04_L0SS/04-loss-diagram/","title":"LOSS Diagram","text":"Text Only<pre><code>================================================================================\n          S\u01a0 \u0110\u1ed2 T\u00cdNH TO\u00c1N LOSS &amp; ACCURACY (QUY TR\u00ccNH MA TR\u1eacN)\n================================================================================\n\n                                  (\u0110\u1ea6U V\u00c0O)\n            +---------------------------------+      +---------------------+\n            |      softmax_outputs (\u0177)        |      |  class_targets (y)  |\n            |     (Ma tr\u1eadn d\u1ef1 \u0111o\u00e1n N x C)     |      |  (Nh\u00e3n th\u1eadt)        |\n            | [[0.7, 0.2, 0.1],               |      | [0, 1, 1]           | &lt;-- D\u1ea1ng Sparse\n            |  [0.5, 0.1, 0.4],               |      |   HO\u1eb6C              |\n            |  [0.02, 0.9, 0.08]]             |      | [[1, 0, 0],         |\n            +---------------------------------+      |  [0, 1, 0],         | &lt;-- D\u1ea1ng One-Hot\n                                                   |  [0, 1, 0]]         |\n                                                   +---------------------+\n                                     |\n                                     v\n+-------------------------------------------------------------------------------+\n|                        LU\u1ed2NG 1: T\u00cdNH TO\u00c1N M\u1ea4T M\u00c1T (LOSS)                        |\n+-------------------------------------------------------------------------------+\n                                     |\n                                     v\n                          (H\u00c0NH \u0110\u1ed8NG: Clipping \u0111\u1ec3 tr\u00e1nh log(0))\n                           np.clip(y_pred, 1e-7, 1-1e-7)\n                                     |\n                                     v\n                      +-------------------------------+\n                      |      y_pred_clipped           |\n                      | [[0.7, 0.2, 0.1],            |\n                      |  [0.5, 0.1, 0.4],            |\n                      |  [0.02, 0.9, 0.08]]          |\n                      +-------------------------------+\n                                     |\n           +-------------------------+-------------------------+\n           |                                                   |\n           v (N\u1ebeU NH\u00c3N L\u00c0 SPARSE)                              v (N\u1ebeU NH\u00c3N L\u00c0 ONE-HOT)\n+------------------------------------+             +--------------------------------------+\n| L\u1ea5y gi\u00e1 tr\u1ecb theo ch\u1ec9 s\u1ed1 (Indexing) |             |    Nh\u00e2n theo ph\u1ea7n t\u1eed (Element-wise)  |\n| y_pred[range(N), y_true]           |             |      y_pred_clipped * y_true         |\n|                                    |             |                                      |\n|  [0.7, 0.2, 0.1] ---&lt;--- [0]       |             |  [[0.7, 0.2, 0.1],   * [[1, 0, 0],   |\n|  [0.5, 0.1, 0.4] -------&lt;--- [1]   |             |   [0.5, 0.1, 0.4],     [0, 1, 0],   |\n|  [0.02, 0.9, 0.08] -----^---&lt;--- [1]   |             |   [0.02, 0.9, 0.08]]    [0, 1, 0]]   |\n|                      |           |             |            =                         |\n|                      v           |             |  [[0.7, 0.0, 0.0],                   |\n|  (Vector x\u00e1c su\u1ea5t \u0111\u00fang)          |             |   [0.0, 0.1, 0.0],                   |\n|      [0.7, 0.5, 0.9]             |             |   [0.0, 0.9, 0.0]]                   |\n+------------------------------------+             |            |                         |\n                                                   |            v (np.sum(axis=1))        |\n                                                   +--------------------------------------+\n                                     |                         |\n                                     +-----------&gt;-------------+\n                                                 |\n                                                 v\n                                    +--------------------------+\n                                    | correct_confidences      |\n                                    | (Vector x\u00e1c su\u1ea5t \u0111\u00fang)   |\n                                    |    [0.7, 0.5, 0.9]       |\n                                    +--------------------------+\n                                                 |\n                                                 v (H\u00c0NH \u0110\u1ed8NG: L\u1ea5y Negative Log)\n                                                    -np.log()\n                                                 |\n                                                 v\n                                    +--------------------------+\n                                    |      sample_losses       |\n                                    | [0.356, 0.693, 0.105]    |\n                                    +--------------------------+\n                                                 |\n                                                 v (H\u00c0NH \u0110\u1ed8NG: L\u1ea5y trung b\u00ecnh)\n                                                     np.mean()\n                                                 |\n                                                 v\n                                     +-----------------------+\n                                     |     average_loss      |\n                                     | (M\u1ed9t s\u1ed1 v\u00f4 h\u01b0\u1edbng)      |\n                                     |       0.385           |\n                                     +-----------------------+\n\n\n+-------------------------------------------------------------------------------+\n|               LU\u1ed2NG 2: T\u00cdNH TO\u00c1N \u0110\u1ed8 CH\u00cdNH X\u00c1C (ACCURACY) - \u0110\u00c3 S\u1eecA L\u1ed6I           |\n+-------------------------------------------------------------------------------+\n                                     |\n     (S\u1eed d\u1ee5ng `softmax_outputs` G\u1ed0C) |\n+---------------------------------+  |  +---------------------------------------+\n|      softmax_outputs (\u0177)        |  |  |            class_targets (y)          |\n| [[0.7, 0.2, 0.1],               |  |  |                                       |\n|  [0.5, 0.1, 0.4],               |  |  | N\u1ebfu l\u00e0 One-Hot:                        |\n|  [0.02, 0.9, 0.08]]             |  |  | [[1,0,0], [0,1,0], [0,1,0]]            |\n+---------------------------------+  |  |      |                                |\n                 |                   |  |      v np.argmax(axis=1)              |\n                 v np.argmax(axis=1)   |  |                                       |\n                 |                   |  | N\u1ebfu l\u00e0 Sparse (kh\u00f4ng c\u1ea7n l\u00e0m g\u00ec):      |\n+----------------+                   |  | [0, 1, 1]                             |\n|  predictions   |                   |  +-----------------|---------------------+\n|   [0, 0, 1]    |                   |                    |\n+----------------+                   +-----------&gt;&lt;-------+\n                 |                              |\n                 +---------------&gt;&lt;-------------+\n                                |\n                                v (H\u00c0NH \u0110\u1ed8NG: So s\u00e1nh b\u1eb1ng nhau)\n                                       predictions == y\n                                |\n                                v\n                   +----------------------------+\n                   |   [0, 0, 1] == [0, 1, 1]   |\n                   +----------------------------+\n                                |\n                                v\n                   +----------------------------+\n                   |   [True, False, True]      |\n                   +----------------------------+\n                                |\n                                v (H\u00c0NH \u0110\u1ed8NG: L\u1ea5y trung b\u00ecnh, True=1, False=0)\n                                    np.mean()\n                                |\n                                v\n                         +-----------------------+\n                         |       accuracy        |\n                         | (M\u1ed9t s\u1ed1 v\u00f4 h\u01b0\u1edbng)      |\n                         |    0.666666...        |\n                         +-----------------------+\n\n\n================================================================================\n                           K\u1ebeT QU\u1ea2 CU\u1ed0I C\u00d9NG\n                +------------------+     +------------------+\n                |   average_loss   |     |     accuracy     |\n                |      0.385       |     |   0.666666...    |\n                +------------------+     +------------------+\n================================================================================\n</code></pre>"},{"location":"vi/Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/","title":"LOSS & ACCURACY","text":""},{"location":"vi/Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#tinh-toan-sai-so-mang-no-ron-voi-ham-mat-mat-loss-function","title":"T\u00ednh to\u00e1n Sai s\u1ed1 M\u1ea1ng n\u01a1-ron v\u1edbi H\u00e0m m\u1ea5t m\u00e1t (Loss Function)","text":"<p>Ch\u00e0o m\u1eebng c\u00e1c b\u1ea1n \u0111\u1ebfn v\u1edbi b\u00e0i h\u1ecdc v\u1ec1 c\u00e1ch ch\u00fang ta \"\u0111o l\u01b0\u1eddng s\u1ef1 sai l\u1ea7m\" c\u1ee7a m\u1ed9t m\u1ea1ng n\u01a1-ron. Trong c\u00e1c ch\u01b0\u01a1ng tr\u01b0\u1edbc, ch\u00fang ta \u0111\u00e3 x\u00e2y d\u1ef1ng \u0111\u01b0\u1ee3c m\u1ed9t m\u1ea1ng c\u00f3 kh\u1ea3 n\u0103ng nh\u1eadn \u0111\u1ea7u v\u00e0o v\u00e0 \u0111\u01b0a ra d\u1ef1 \u0111o\u00e1n. Nh\u01b0ng l\u00e0m th\u1ebf n\u00e0o \u0111\u1ec3 bi\u1ebft d\u1ef1 \u0111o\u00e1n \u0111\u00f3 \"t\u1ed1t\" hay \"t\u1ec7\" \u0111\u1ebfn m\u1ee9c n\u00e0o? V\u00e0 \"t\u1ec7\" h\u01a1n bao nhi\u00eau so v\u1edbi m\u1ed9t d\u1ef1 \u0111o\u00e1n kh\u00e1c? \u0110\u00e2y ch\u00ednh l\u00e0 l\u00fac H\u00e0m m\u1ea5t m\u00e1t (Loss Function) v\u00e0o cu\u1ed9c.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#1-tai-sao-o-chinh-xac-accuracy-la-khong-u","title":"1. T\u1ea1i sao \u0110\u1ed9 ch\u00ednh x\u00e1c (Accuracy) l\u00e0 kh\u00f4ng \u0111\u1ee7?","text":"<p>C\u00e2u h\u1ecfi \u0111\u1ea7u ti\u00ean b\u1ea1n c\u00f3 th\u1ec3 \u0111\u1eb7t ra l\u00e0: \"T\u1ea1i sao kh\u00f4ng d\u00f9ng lu\u00f4n \u0111\u1ed9 ch\u00ednh x\u00e1c \u0111\u1ec3 \u0111o l\u01b0\u1eddng? C\u1ee9 xem m\u1ea1ng d\u1ef1 \u0111o\u00e1n \u0111\u00fang bao nhi\u00eau ph\u1ea7n tr\u0103m l\u00e0 \u0111\u01b0\u1ee3c.\"</p> <p>H\u00e3y xem v\u00ed d\u1ee5 kinh \u0111i\u1ec3n trong s\u00e1ch: Gi\u1ea3 s\u1eed k\u1ebft qu\u1ea3 \u0111\u00fang l\u00e0 l\u1edbp \u1edf gi\u1eefa (index 1). Ch\u00fang ta c\u00f3 hai d\u1ef1 \u0111o\u00e1n t\u1eeb m\u00f4 h\u00ecnh:</p> <ul> <li>D\u1ef1 \u0111o\u00e1n A: <code>[0.22, 0.6, 0.18]</code></li> <li>D\u1ef1 \u0111o\u00e1n B: <code>[0.32, 0.36, 0.32]</code></li> </ul> <p>N\u1ebfu ch\u00fang ta ch\u1ec9 d\u00f9ng \u0111\u1ed9 ch\u00ednh x\u00e1c, ta s\u1ebd l\u00e0m nh\u01b0 sau: 1.  D\u00f9ng h\u00e0m <code>argmax</code> \u0111\u1ec3 t\u00ecm ch\u1ec9 s\u1ed1 c\u1ee7a gi\u00e1 tr\u1ecb l\u1edbn nh\u1ea5t. 2.  <code>argmax</code> c\u1ee7a c\u1ea3 A v\u00e0 B \u0111\u1ec1u l\u00e0 <code>1</code>. 3.  So s\u00e1nh v\u1edbi k\u1ebft qu\u1ea3 \u0111\u00fang l\u00e0 <code>1</code>. C\u1ea3 hai \u0111\u1ec1u \u0111\u00fang. \u0110\u1ed9 ch\u00ednh x\u00e1c l\u00e0 100% cho c\u1ea3 hai tr\u01b0\u1eddng h\u1ee3p.</p> <p>Nh\u01b0ng h\u00e3y nh\u00ecn k\u1ef9 h\u01a1n. \u0110\u1ea7u ra c\u1ee7a m\u1ea1ng n\u01a1-ron (sau l\u1edbp Softmax) th\u1ec3 hi\u1ec7n m\u1ee9c \u0111\u1ed9 t\u1ef1 tin (confidence). *   \u1ede d\u1ef1 \u0111o\u00e1n A, m\u00f4 h\u00ecnh r\u1ea5t t\u1ef1 tin (60%) r\u1eb1ng \u0111\u00f3 l\u00e0 l\u1edbp 1. *   \u1ede d\u1ef1 \u0111o\u00e1n B, m\u00f4 h\u00ecnh ch\u1ec9 h\u01a1i nh\u1ec9nh h\u01a1n m\u1ed9t ch\u00fat (36%) v\u00e0 kh\u00e1 ph\u00e2n v\u00e2n gi\u1eefa c\u1ea3 ba l\u1edbp.</p> <p>R\u00f5 r\u00e0ng, d\u1ef1 \u0111o\u00e1n A \"t\u1ed1t h\u01a1n\" nhi\u1ec1u so v\u1edbi d\u1ef1 \u0111o\u00e1n B. Ch\u00fang ta c\u1ea7n m\u1ed9t th\u01b0\u1edbc \u0111o c\u00f3 th\u1ec3 ph\u1ea3n \u00e1nh \u0111\u01b0\u1ee3c s\u1ef1 kh\u00e1c bi\u1ec7t v\u1ec1 \u0111\u1ed9 t\u1ef1 tin n\u00e0y. \u0110\u1ed9 ch\u00ednh x\u00e1c l\u00e0 m\u1ed9t th\u01b0\u1edbc \u0111o r\u1eddi r\u1ea1c (\u0111\u00fang ho\u1eb7c sai), trong khi ch\u00fang ta c\u1ea7n m\u1ed9t th\u01b0\u1edbc \u0111o li\u00ean t\u1ee5c, chi ti\u1ebft h\u01a1n.</p> <p>\u0110\u00e2y ch\u00ednh l\u00e0 m\u1ee5c \u0111\u00edch c\u1ee7a Loss (M\u1ea5t m\u00e1t). Loss l\u00e0 m\u1ed9t con s\u1ed1 duy nh\u1ea5t cho bi\u1ebft m\u00f4 h\u00ecnh \u0111\u00e3 sai l\u1ea7m nhi\u1ec1u nh\u01b0 th\u1ebf n\u00e0o. M\u1ee5c ti\u00eau c\u1ee7a vi\u1ec7c hu\u1ea5n luy\u1ec7n l\u00e0 \u0111i\u1ec1u ch\u1ec9nh tr\u1ecdng s\u1ed1 (weights) v\u00e0 \u0111\u1ed9 l\u1ec7ch (biases) \u0111\u1ec3 gi\u00e1 tr\u1ecb Loss n\u00e0y c\u00e0ng g\u1ea7n 0 c\u00e0ng t\u1ed1t.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#2-categorical-cross-entropy-ham-mat-mat-cho-bai-toan-phan-loai","title":"2. Categorical Cross-Entropy: H\u00e0m m\u1ea5t m\u00e1t cho b\u00e0i to\u00e1n ph\u00e2n lo\u1ea1i","text":"<p>Trong b\u00e0i to\u00e1n c\u1ee7a ch\u00fang ta (ph\u00e2n lo\u1ea1i d\u1eef li\u1ec7u xo\u1eafn \u1ed1c), l\u1edbp \u0111\u1ea7u ra s\u1eed d\u1ee5ng h\u00e0m k\u00edch ho\u1ea1t Softmax, bi\u1ebfn c\u00e1c con s\u1ed1 (logits) th\u00e0nh m\u1ed9t ph\u00e2n ph\u1ed1i x\u00e1c su\u1ea5t (t\u1ed5ng c\u00e1c gi\u00e1 tr\u1ecb b\u1eb1ng 1). \u0110\u1ec3 so s\u00e1nh hai ph\u00e2n ph\u1ed1i x\u00e1c su\u1ea5t (m\u1ed9t l\u00e0 d\u1ef1 \u0111o\u00e1n c\u1ee7a m\u00f4 h\u00ecnh, m\u1ed9t l\u00e0 s\u1ef1 th\u1eadt), ng\u01b0\u1eddi ta th\u01b0\u1eddng d\u00f9ng m\u1ed9t c\u00f4ng c\u1ee5 to\u00e1n h\u1ecdc g\u1ecdi l\u00e0 Cross-Entropy.</p> <p>Info</p> <p>Ki\u1ebfn th\u1ee9c b\u00ean l\u1ec1: Cross-Entropy l\u00e0 g\u00ec? Trong l\u00fd thuy\u1ebft th\u00f4ng tin, cross-entropy \u0111o l\u01b0\u1eddng s\u1ef1 kh\u00e1c bi\u1ec7t gi\u1eefa hai ph\u00e2n ph\u1ed1i x\u00e1c su\u1ea5t. N\u1ebfu b\u1ea1n d\u00f9ng m\u1ed9t b\u1ed9 m\u00e3 h\u00f3a t\u1ed1i \u01b0u cho ph\u00e2n ph\u1ed1i P \u0111\u1ec3 m\u00e3 h\u00f3a d\u1eef li\u1ec7u t\u1eeb ph\u00e2n ph\u1ed1i Q, cross-entropy H(Q, P) cho b\u1ea1n bi\u1ebft s\u1ed1 bit trung b\u00ecnh c\u1ea7n thi\u1ebft. Khi P v\u00e0 Q c\u00e0ng gi\u1ed1ng nhau, gi\u00e1 tr\u1ecb n\u00e0y c\u00e0ng nh\u1ecf. Trong Machine Learning, ch\u00fang ta coi \"s\u1ef1 th\u1eadt\" (ground-truth) l\u00e0 ph\u00e2n ph\u1ed1i P v\u00e0 \"d\u1ef1 \u0111o\u00e1n\" c\u1ee7a m\u00f4 h\u00ecnh (predictions) l\u00e0 ph\u00e2n ph\u1ed1i Q. M\u1ee5c ti\u00eau l\u00e0 l\u00e0m cho Q c\u00e0ng gi\u1ed1ng P c\u00e0ng t\u1ed1t, t\u1ee9c l\u00e0 gi\u1ea3m thi\u1ec3u cross-entropy. (Ngu\u1ed3n: Deep Learning Book, Chapter 3.13)</p> <p>Khi \u00e1p d\u1ee5ng cho b\u00e0i to\u00e1n ph\u00e2n lo\u1ea1i v\u1edbi nhi\u1ec1u l\u1edbp, n\u00f3 \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 Categorical Cross-Entropy.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#cong-thuc-toan-hoc","title":"C\u00f4ng th\u1ee9c to\u00e1n h\u1ecdc","text":"<p>C\u00f4ng th\u1ee9c \u0111\u1ea7y \u0111\u1ee7 \u0111\u1ec3 t\u00ednh loss cho m\u1ed9t m\u1eabu (sample) <code>i</code> l\u00e0:</p> <p><code>L_i = - \u03a3_j ( y_i,j * log(\u0177_i,j) )</code></p> <p>Trong \u0111\u00f3: *   <code>L_i</code>: Gi\u00e1 tr\u1ecb loss c\u1ee7a m\u1eabu th\u1ee9 <code>i</code>. *   <code>j</code>: Ch\u1ec9 s\u1ed1 c\u1ee7a t\u1eebng l\u1edbp \u0111\u1ea7u ra (v\u00ed d\u1ee5: ch\u00f3, m\u00e8o, ng\u01b0\u1eddi). *   <code>\u03a3_j</code>: K\u00fd hi\u1ec7u l\u1ea5y t\u1ed5ng theo t\u1ea5t c\u1ea3 c\u00e1c l\u1edbp <code>j</code>. *   <code>y_i,j</code>: S\u1ef1 th\u1eadt (ground-truth). L\u00e0 1 n\u1ebfu m\u1eabu <code>i</code> th\u1ef1c s\u1ef1 thu\u1ed9c l\u1edbp <code>j</code>, v\u00e0 l\u00e0 0 cho c\u00e1c l\u1edbp c\u00f2n l\u1ea1i. *   <code>\u0177_i,j</code> (\u0111\u1ecdc l\u00e0 \"y-hat\"): D\u1ef1 \u0111o\u00e1n c\u1ee7a m\u00f4 h\u00ecnh. L\u00e0 x\u00e1c su\u1ea5t m\u00e0 m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n m\u1eabu <code>i</code> thu\u1ed9c v\u1ec1 l\u1edbp <code>j</code> (gi\u00e1 tr\u1ecb t\u1eeb Softmax). *   <code>log</code>: L\u00e0 logarit t\u1ef1 nhi\u00ean (c\u01a1 s\u1ed1 e), trong Python l\u00e0 <code>math.log()</code> ho\u1eb7c <code>np.log()</code>.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#su-ky-dieu-cua-one-hot-encoding","title":"S\u1ef1 k\u1ef3 di\u1ec7u c\u1ee7a One-Hot Encoding","text":"<p>H\u00e3y xem c\u00f4ng th\u1ee9c n\u00e0y ho\u1ea1t \u0111\u1ed9ng nh\u01b0 th\u1ebf n\u00e0o trong th\u1ef1c t\u1ebf. Gi\u1ea3 s\u1eed ta c\u00f3 3 l\u1edbp v\u00e0 s\u1ef1 th\u1eadt l\u00e0 l\u1edbp \u0111\u1ea7u ti\u00ean (index 0). *   D\u1ef1 \u0111o\u00e1n c\u1ee7a m\u00f4 h\u00ecnh (\u0177): <code>[0.7, 0.1, 0.2]</code> *   S\u1ef1 th\u1eadt (y): <code>[1, 0, 0]</code></p> <p>Vector <code>[1, 0, 0]</code> \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 one-hot encoding. \"Hot\" l\u00e0 1, \"cold\" l\u00e0 0.</p> <p>\u00c1p d\u1ee5ng c\u00f4ng th\u1ee9c: <code>L = - ( y_0*log(\u0177_0) + y_1*log(\u0177_1) + y_2*log(\u0177_2) )</code> <code>L = - ( 1*log(0.7) + 0*log(0.1) + 0*log(0.2) )</code> <code>L = - ( 1*log(0.7) + 0 + 0 )</code> <code>L = -log(0.7)</code></p> <p>C\u00f4ng th\u1ee9c \u0111\u1ed3 s\u1ed9 ban \u0111\u1ea7u \u0111\u00e3 \u0111\u01b0\u1ee3c r\u00fat g\u1ecdn th\u00e0nh m\u1ed9t ph\u00e9p t\u00ednh v\u00f4 c\u00f9ng \u0111\u01a1n gi\u1ea3n: ch\u1ec9 c\u1ea7n l\u1ea5y logarit t\u1ef1 nhi\u00ean c\u1ee7a x\u00e1c su\u1ea5t d\u1ef1 \u0111o\u00e1n cho l\u1edbp \u0111\u00fang, r\u1ed3i \u0111\u1ed5i d\u1ea5u.</p> <p>\u0110\u00e2y l\u00e0 m\u1ed9t insight c\u1ef1c k\u1ef3 quan tr\u1ecdng! N\u00f3 gi\u00fap vi\u1ec7c t\u00ednh to\u00e1n tr\u1edf n\u00ean hi\u1ec7u qu\u1ea3 h\u01a1n r\u1ea5t nhi\u1ec1u.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#3-trien-khai-bang-python-step-by-step","title":"3. Tri\u1ec3n khai b\u1eb1ng Python (Step-by-Step)","text":""},{"location":"vi/Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#31-tinh-loss-cho-mot-batch","title":"3.1. T\u00ednh Loss cho m\u1ed9t Batch","text":"<p>Trong th\u1ef1c t\u1ebf, ch\u00fang ta kh\u00f4ng x\u1eed l\u00fd t\u1eebng m\u1eabu m\u1ed9t m\u00e0 x\u1eed l\u00fd theo batch (l\u00f4) \u0111\u1ec3 t\u0103ng t\u1ed1c \u0111\u1ed9.</p> Python<pre><code>import numpy as np\n\n# \u0110\u1ea7u ra Softmax cho m\u1ed9t batch 3 m\u1eabu, m\u1ed7i m\u1eabu 3 l\u1edbp\nsoftmax_outputs = np.array([[0.7, 0.1, 0.2],  # M\u1eabu 1\n                            [0.1, 0.5, 0.4],  # M\u1eabu 2\n                            [0.02, 0.9, 0.08]]) # M\u1eabu 3\n\n# Nh\u00e3n th\u1eadt (target labels). \u0110\u00e2y l\u00e0 d\u1ea1ng \"sparse\" (th\u01b0a)\n# M\u1eabu 1 \u0111\u00fang l\u00e0 l\u1edbp 0, m\u1eabu 2 l\u00e0 l\u1edbp 1, m\u1eabu 3 l\u00e0 l\u1edbp 1\nclass_targets = [0, 1, 1]\n</code></pre> <p>L\u00e0m th\u1ebf n\u00e0o \u0111\u1ec3 l\u1ea5y ra c\u00e1c x\u00e1c su\u1ea5t \u0111\u00fang t\u1eeb <code>softmax_outputs</code>? Ch\u00fang ta c\u1ea7n: *   T\u1eeb h\u00e0ng 0, l\u1ea5y ph\u1ea7n t\u1eed \u1edf c\u1ed9t 0 (<code>0.7</code>) *   T\u1eeb h\u00e0ng 1, l\u1ea5y ph\u1ea7n t\u1eed \u1edf c\u1ed9t 1 (<code>0.5</code>) *   T\u1eeb h\u00e0ng 2, l\u1ea5y ph\u1ea7n t\u1eed \u1edf c\u1ed9t 1 (<code>0.9</code>)</p> <p>NumPy cung c\u1ea5p m\u1ed9t c\u00e1ch r\u1ea5t thanh l\u1ecbch \u0111\u1ec3 l\u00e0m \u0111i\u1ec1u n\u00e0y, g\u1ecdi l\u00e0 advanced indexing:</p> Python<pre><code># L\u1ea5y ra c\u00e1c x\u00e1c su\u1ea5t c\u1ee7a c\u00e1c l\u1edbp \u0111\u00fang\ncorrect_confidences = softmax_outputs[\n    range(len(softmax_outputs)), # Ch\u1ec9 s\u1ed1 h\u00e0ng: [0, 1, 2]\n    class_targets              # Ch\u1ec9 s\u1ed1 c\u1ed9t: [0, 1, 1]\n]\n\nprint(correct_confidences)\n# K\u1ebft qu\u1ea3: [0.7 0.5 0.9]\n</code></pre> <p>B\u00e2y gi\u1edd, ch\u00fang ta ch\u1ec9 c\u1ea7n \u00e1p d\u1ee5ng c\u00f4ng th\u1ee9c <code>-log</code> cho t\u1eebng gi\u00e1 tr\u1ecb n\u00e0y:</p> Python<pre><code># T\u00ednh loss cho t\u1eebng m\u1eabu\nsample_losses = -np.log(correct_confidences)\nprint(sample_losses)\n# K\u1ebft qu\u1ea3: [0.35667494 0.69314718 0.10536052]\n</code></pre> <p>Cu\u1ed1i c\u00f9ng, loss c\u1ee7a c\u1ea3 batch th\u01b0\u1eddng \u0111\u01b0\u1ee3c t\u00ednh b\u1eb1ng trung b\u00ecnh c\u1ed9ng (mean) c\u1ee7a c\u00e1c loss ri\u00eang l\u1ebb:</p> Python<pre><code># T\u00ednh loss trung b\u00ecnh cho c\u1ea3 batch\naverage_loss = np.mean(sample_losses)\nprint(average_loss)\n# K\u1ebft qu\u1ea3: 0.38506088...\n</code></pre>"},{"location":"vi/Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#32-van-e-nan-giai-log0","title":"3.2. V\u1ea5n \u0111\u1ec1 nan gi\u1ea3i: <code>log(0)</code>","text":"<p>H\u00e0m <code>log(x)</code> ch\u1ec9 x\u00e1c \u0111\u1ecbnh khi <code>x &gt; 0</code>. Chuy\u1ec7n g\u00ec s\u1ebd x\u1ea3y ra n\u1ebfu m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n x\u00e1c su\u1ea5t c\u1ee7a l\u1edbp \u0111\u00fang l\u00e0 0?</p> <p><code>log(0)</code> l\u00e0 \u00e2m v\u00f4 c\u00f9ng. \u0110i\u1ec1u n\u00e0y s\u1ebd t\u1ea1o ra gi\u00e1 tr\u1ecb loss l\u00e0 <code>inf</code> (v\u00f4 c\u00f9ng) trong Python, v\u00e0 m\u1ed9t khi <code>inf</code> xu\u1ea5t hi\u1ec7n, n\u00f3 s\u1ebd \"l\u00e2y nhi\u1ec5m\" cho c\u00e1c ph\u00e9p t\u00ednh sau \u0111\u00f3 (v\u00ed d\u1ee5, trung b\u00ecnh c\u1ee7a m\u1ed9t d\u00e3y s\u1ed1 c\u00f3 <code>inf</code> c\u0169ng l\u00e0 <code>inf</code>). \u0110i\u1ec1u n\u00e0y s\u1ebd l\u00e0m h\u1ecfng to\u00e0n b\u1ed9 qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n.</p> <p>T\u1ec7 h\u01a1n n\u1eefa, n\u1ebfu m\u00f4 h\u00ecnh qu\u00e1 t\u1ef1 tin v\u00e0 d\u1ef1 \u0111o\u00e1n x\u00e1c su\u1ea5t l\u00e0 1.0 cho l\u1edbp \u0111\u00fang, th\u00ec <code>log(1.0) = 0</code>, loss s\u1ebd b\u1eb1ng 0. Nh\u01b0ng do sai s\u1ed1 l\u00e0m tr\u00f2n c\u1ee7a s\u1ed1 th\u1ef1c trong m\u00e1y t\u00ednh, gi\u00e1 tr\u1ecb c\u00f3 th\u1ec3 l\u00e0 <code>1.0000001</code>. <code>log(1.0000001)</code> l\u00e0 m\u1ed9t s\u1ed1 d\u01b0\u01a1ng r\u1ea5t nh\u1ecf, khi\u1ebfn loss tr\u1edf th\u00e0nh m\u1ed9t s\u1ed1 \u00e2m r\u1ea5t nh\u1ecf. Loss \u00e2m l\u00e0 \u0111i\u1ec1u v\u00f4 l\u00fd.</p> <p>Gi\u1ea3i ph\u00e1p: Clipping (C\u1eaft x\u00e9n)</p> <p>\u0110\u1ec3 gi\u1ea3i quy\u1ebft tri\u1ec7t \u0111\u1ec3, ch\u00fang ta s\u1ebd \"c\u1eaft x\u00e9n\" c\u00e1c gi\u00e1 tr\u1ecb d\u1ef1 \u0111o\u00e1n \u0111\u1ec3 ch\u00fang kh\u00f4ng bao gi\u1edd ch\u1ea1m t\u1edbi 0 ho\u1eb7c 1. Ta s\u1ebd \u00e9p ch\u00fang v\u00e0o m\u1ed9t kho\u1ea3ng r\u1ea5t nh\u1ecf, v\u00ed d\u1ee5 <code>[1e-7, 1 - 1e-7]</code>.</p> <ul> <li><code>1e-7</code> l\u00e0 c\u00e1ch vi\u1ebft khoa h\u1ecdc c\u1ee7a <code>0.0000001</code>.</li> <li>B\u1ea5t k\u1ef3 gi\u00e1 tr\u1ecb n\u00e0o nh\u1ecf h\u01a1n <code>1e-7</code> s\u1ebd \u0111\u01b0\u1ee3c g\u00e1n b\u1eb1ng <code>1e-7</code>.</li> <li>B\u1ea5t k\u1ef3 gi\u00e1 tr\u1ecb n\u00e0o l\u1edbn h\u01a1n <code>1 - 1e-7</code> s\u1ebd \u0111\u01b0\u1ee3c g\u00e1n b\u1eb1ng <code>1 - 1e-7</code>.</li> </ul> <p>Trong NumPy, ta d\u00f9ng h\u00e0m <code>np.clip()</code>:</p> <p>Python<pre><code># y_pred l\u00e0 m\u1ea3ng c\u00e1c x\u00e1c su\u1ea5t d\u1ef1 \u0111o\u00e1n\ny_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n</code></pre> Thao t\u00e1c n\u00e0y \u0111\u1ea3m b\u1ea3o r\u1eb1ng ch\u00fang ta s\u1ebd kh\u00f4ng bao gi\u1edd ph\u1ea3i t\u00ednh <code>log</code> c\u1ee7a 0 ho\u1eb7c c\u1ee7a m\u1ed9t s\u1ed1 l\u1edbn h\u01a1n 1, gi\u00fap qu\u00e1 tr\u00ecnh t\u00ednh to\u00e1n \u1ed5n \u0111\u1ecbnh.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#33-xu-ly-ca-hai-inh-dang-nhan-one-hot-va-sparse","title":"3.3. X\u1eed l\u00fd c\u1ea3 hai \u0111\u1ecbnh d\u1ea1ng Nh\u00e3n (One-Hot v\u00e0 Sparse)","text":"<p>Nh\u00e3n th\u1eadt (targets) c\u00f3 th\u1ec3 \u1edf hai d\u1ea1ng: 1.  Sparse (th\u01b0a): M\u1ed9t m\u1ea3ng 1 chi\u1ec1u ch\u1ee9a c\u00e1c ch\u1ec9 s\u1ed1 c\u1ee7a l\u1edbp \u0111\u00fang. V\u00ed d\u1ee5: <code>[0, 1, 1]</code>. 2.  One-Hot: M\u1ed9t m\u1ea3ng 2 chi\u1ec1u. V\u00ed d\u1ee5: <code>[[1,0,0], [0,1,0], [0,1,0]]</code>.</p> <p>Ta c\u00f3 th\u1ec3 ki\u1ec3m tra s\u1ed1 chi\u1ec1u c\u1ee7a m\u1ea3ng nh\u00e3n (<code>len(y_true.shape)</code>) \u0111\u1ec3 bi\u1ebft n\u00f3 thu\u1ed9c d\u1ea1ng n\u00e0o v\u00e0 x\u1eed l\u00fd t\u01b0\u01a1ng \u1ee9ng.</p> <ul> <li>N\u1ebfu l\u00e0 Sparse (shape=1): D\u00f9ng ph\u01b0\u01a1ng ph\u00e1p indexing nh\u01b0 \u0111\u00e3 l\u00e0m \u1edf tr\u00ean.</li> <li>N\u1ebfu l\u00e0 One-Hot (shape=2): Ta quay l\u1ea1i c\u00f4ng th\u1ee9c g\u1ed1c <code>\u03a3 (y * log(\u0177))</code>. Do <code>y</code> l\u00e0 one-hot, ph\u00e9p nh\u00e2n <code>y * log(\u0177)</code> s\u1ebd bi\u1ebfn t\u1ea5t c\u1ea3 c\u00e1c gi\u00e1 tr\u1ecb c\u1ee7a l\u1edbp sai th\u00e0nh 0, ch\u1ec9 gi\u1eef l\u1ea1i gi\u00e1 tr\u1ecb c\u1ee7a l\u1edbp \u0111\u00fang. Sau \u0111\u00f3 ta ch\u1ec9 c\u1ea7n l\u1ea5y t\u1ed5ng theo h\u00e0ng (<code>axis=1</code>).</li> </ul> <p>Python<pre><code># Gi\u1ea3 s\u1eed y_pred_clipped \u0111\u00e3 \u0111\u01b0\u1ee3c t\u00ednh\n# V\u00e0 y_true l\u00e0 nh\u00e3n one-hot\nif len(y_true.shape) == 2:\n    correct_confidences = np.sum(\n        y_pred_clipped * y_true,\n        axis=1\n    )\n</code></pre> C\u00e1ch n\u00e0y cho k\u1ebft qu\u1ea3 t\u01b0\u01a1ng \u0111\u01b0\u01a1ng v\u1edbi c\u00e1ch indexing nh\u01b0ng t\u1ed5ng qu\u00e1t h\u01a1n.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#4-xay-dung-class-loss-hoan-chinh","title":"4. X\u00e2y d\u1ef1ng Class Loss ho\u00e0n ch\u1ec9nh","text":"<p>\u0110\u1ec3 m\u00e3 ngu\u1ed3n c\u00f3 t\u1ed5 ch\u1ee9c v\u00e0 d\u1ec5 t\u00e1i s\u1eed d\u1ee5ng, ch\u00fang ta s\u1ebd t\u1ea1o c\u00e1c class cho Loss.</p>"},{"location":"vi/Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#class-loss-co-so","title":"Class <code>Loss</code> c\u01a1 s\u1edf","text":"<p>\u0110\u00e2y l\u00e0 m\u1ed9t class cha tr\u1eebu t\u01b0\u1ee3ng. N\u00f3 c\u00f3 m\u1ed9t ph\u01b0\u01a1ng th\u1ee9c <code>calculate</code> chung cho t\u1ea5t c\u1ea3 c\u00e1c lo\u1ea1i loss: t\u00ednh loss c\u1ee7a t\u1eebng m\u1eabu (qua h\u00e0m <code>forward</code>) r\u1ed3i l\u1ea5y trung b\u00ecnh.</p> Python<pre><code># Common loss class\nclass Loss:\n    # T\u00ednh to\u00e1n loss d\u1eef li\u1ec7u\n    def calculate(self, output, y):\n        # T\u00ednh loss c\u1ee7a t\u1eebng m\u1eabu\n        sample_losses = self.forward(output, y)\n\n        # T\u00ednh loss trung b\u00ecnh\n        data_loss = np.mean(sample_losses)\n\n        return data_loss\n</code></pre>"},{"location":"vi/Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#class-loss_categoricalcrossentropy","title":"Class <code>Loss_CategoricalCrossentropy</code>","text":"<p>Class n\u00e0y k\u1ebf th\u1eeba t\u1eeb <code>Loss</code> v\u00e0 tri\u1ec3n khai logic t\u00ednh to\u00e1n c\u1ee5 th\u1ec3 cho Categorical Cross-Entropy.</p> Python<pre><code># Cross-entropy loss\nclass Loss_CategoricalCrossentropy(Loss):\n\n    # Ph\u01b0\u01a1ng th\u1ee9c forward\n    def forward(self, y_pred, y_true):\n        samples = len(y_pred)\n\n        # 1. Clipping d\u1eef li\u1ec7u \u0111\u1ec3 tr\u00e1nh log(0)\n        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n\n        # 2. Ki\u1ec3m tra \u0111\u1ecbnh d\u1ea1ng nh\u00e3n v\u00e0 t\u00ednh to\u00e1n\n        # N\u1ebfu l\u00e0 nh\u00e3n sparse [0, 1, 1]\n        if len(y_true.shape) == 1:\n            correct_confidences = y_pred_clipped[\n                range(samples),\n                y_true\n            ]\n        # N\u1ebfu l\u00e0 nh\u00e3n one-hot [[1,0,0], [0,1,0], ...]\n        elif len(y_true.shape) == 2:\n            correct_confidences = np.sum(\n                y_pred_clipped * y_true,\n                axis=1\n            )\n\n        # 3. T\u00ednh negative log likelihoods (loss cho t\u1eebng m\u1eabu)\n        negative_log_likelihoods = -np.log(correct_confidences)\n        return negative_log_likelihoods\n</code></pre>"},{"location":"vi/Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#5-tinh-them-o-chinh-xac-accuracy","title":"5. T\u00ednh th\u00eam \u0110\u1ed9 ch\u00ednh x\u00e1c (Accuracy)","text":"<p>M\u1eb7c d\u00f9 kh\u00f4ng d\u00f9ng \u0111\u1ec3 t\u1ed1i \u01b0u h\u00f3a, \u0111\u1ed9 ch\u00ednh x\u00e1c v\u1eabn l\u00e0 m\u1ed9t th\u01b0\u1edbc \u0111o r\u1ea5t tr\u1ef1c quan \u0111\u1ec3 con ng\u01b0\u1eddi \u0111\u00e1nh gi\u00e1 hi\u1ec7u su\u1ea5t c\u1ee7a m\u00f4 h\u00ecnh. Ch\u00fang ta s\u1ebd t\u00ednh n\u00f3 song song v\u1edbi loss.</p> <p>C\u00e1ch t\u00ednh accuracy r\u1ea5t \u0111\u01a1n gi\u1ea3n: 1.  D\u00f9ng <code>np.argmax(softmax_outputs, axis=1)</code> \u0111\u1ec3 t\u00ecm ra l\u1edbp \u0111\u01b0\u1ee3c d\u1ef1 \u0111o\u00e1n c\u00f3 x\u00e1c su\u1ea5t cao nh\u1ea5t cho m\u1ed7i m\u1eabu. <code>axis=1</code> ngh\u0129a l\u00e0 t\u00ecm argmax tr\u00ean m\u1ed7i h\u00e0ng (m\u1ed7i m\u1eabu). 2.  So s\u00e1nh m\u1ea3ng d\u1ef1 \u0111o\u00e1n n\u00e0y v\u1edbi m\u1ea3ng nh\u00e3n th\u1eadt. Ph\u00e9p so s\u00e1nh <code>predictions == class_targets</code> s\u1ebd tr\u1ea3 v\u1ec1 m\u1ed9t m\u1ea3ng c\u00e1c gi\u00e1 tr\u1ecb <code>True</code> (n\u1ebfu \u0111\u00fang) v\u00e0 <code>False</code> (n\u1ebfu sai). 3.  L\u1ea5y trung b\u00ecnh c\u1ee7a m\u1ea3ng boolean n\u00e0y. <code>np.mean()</code> s\u1ebd t\u1ef1 \u0111\u1ed9ng coi <code>True</code> l\u00e0 1 v\u00e0 <code>False</code> l\u00e0 0.</p> Python<pre><code># L\u1ea5y d\u1ef1 \u0111o\u00e1n t\u1eeb \u0111\u1ea7u ra softmax\npredictions = np.argmax(softmax_outputs, axis=1)\n\n# N\u1ebfu nh\u00e3n l\u00e0 one-hot, chuy\u1ec3n v\u1ec1 sparse\nif len(class_targets.shape) == 2:\n    class_targets = np.argmax(class_targets, axis=1)\n\n# So s\u00e1nh v\u00e0 t\u00ednh trung b\u00ecnh\naccuracy = np.mean(predictions == class_targets)\nprint('acc:', accuracy)\n</code></pre>"},{"location":"vi/Explain%20Documents/Ch4pt3r.04_L0SS/04-loss/#tong-ket","title":"T\u1ed5ng k\u1ebft","text":"<p>Qua ch\u01b0\u01a1ng n\u00e0y, ch\u00fang ta \u0111\u00e3 h\u1ecdc \u0111\u01b0\u1ee3c nh\u1eefng \u0111i\u1ec1u c\u1ed1t l\u00f5i:</p> <ol> <li>T\u1ea1i sao c\u1ea7n Loss: Loss l\u00e0 m\u1ed9t th\u01b0\u1edbc \u0111o chi ti\u1ebft, li\u00ean t\u1ee5c v\u1ec1 \"m\u1ee9c \u0111\u1ed9 sai l\u1ea7m\" c\u1ee7a m\u00f4 h\u00ecnh, t\u1ed1t h\u01a1n nhi\u1ec1u so v\u1edbi \u0111\u1ed9 ch\u00ednh x\u00e1c (Accuracy) cho vi\u1ec7c hu\u1ea5n luy\u1ec7n.</li> <li>Categorical Cross-Entropy: L\u00e0 h\u00e0m m\u1ea5t m\u00e1t ti\u00eau chu\u1ea9n cho b\u00e0i to\u00e1n ph\u00e2n lo\u1ea1i \u0111a l\u1edbp, \u0111o l\u01b0\u1eddng s\u1ef1 kh\u00e1c bi\u1ec7t gi\u1eefa ph\u00e2n ph\u1ed1i x\u00e1c su\u1ea5t d\u1ef1 \u0111o\u00e1n v\u00e0 ph\u00e2n ph\u1ed1i x\u00e1c su\u1ea5t th\u1eadt.</li> <li>C\u00f4ng th\u1ee9c r\u00fat g\u1ecdn: Khi nh\u00e3n l\u00e0 one-hot, c\u00f4ng th\u1ee9c ph\u1ee9c t\u1ea1p \u0111\u01b0\u1ee3c r\u00fat g\u1ecdn th\u00e0nh <code>-log(x\u00e1c su\u1ea5t c\u1ee7a l\u1edbp \u0111\u00fang)</code>.</li> <li>Tri\u1ec3n khai th\u1ef1c t\u1ebf:<ul> <li>S\u1eed d\u1ee5ng NumPy indexing \u0111\u1ec3 t\u00ednh to\u00e1n hi\u1ec7u qu\u1ea3 tr\u00ean c\u1ea3 batch.</li> <li>Gi\u1ea3i quy\u1ebft v\u1ea5n \u0111\u1ec1 <code>log(0)</code> b\u1eb1ng k\u1ef9 thu\u1eadt clipping.</li> <li>X\u00e2y d\u1ef1ng m\u00e3 ngu\u1ed3n theo h\u01b0\u1edbng \u0111\u1ed1i t\u01b0\u1ee3ng (OOP) v\u1edbi c\u00e1c class <code>Loss</code> \u0111\u1ec3 d\u1ec5 qu\u1ea3n l\u00fd v\u00e0 m\u1edf r\u1ed9ng.</li> </ul> </li> <li>Accuracy: V\u1eabn l\u00e0 m\u1ed9t th\u01b0\u1edbc \u0111o quan tr\u1ecdng \u0111\u1ec3 con ng\u01b0\u1eddi theo d\u00f5i v\u00e0 \u0111\u01b0\u1ee3c t\u00ednh to\u00e1n song song v\u1edbi loss.</li> </ol> <p>V\u1edbi vi\u1ec7c t\u00ednh \u0111\u01b0\u1ee3c Loss, ch\u00fang ta \u0111\u00e3 c\u00f3 m\u1ed9t \"t\u00edn hi\u1ec7u\" \u0111\u1ec3 bi\u1ebft m\u00f4 h\u00ecnh c\u1ea7n c\u1ea3i thi\u1ec7n \u1edf \u0111\u00e2u v\u00e0 nh\u01b0 th\u1ebf n\u00e0o. Ch\u01b0\u01a1ng ti\u1ebfp theo s\u1ebd ch\u1ec9 cho ch\u00fang ta c\u00e1ch s\u1eed d\u1ee5ng t\u00edn hi\u1ec7u n\u00e0y \u0111\u1ec3 th\u1ef1c s\u1ef1 \"d\u1ea1y\" cho m\u1ea1ng n\u01a1-ron th\u00f4ng qua qu\u00e1 tr\u00ecnh t\u1ed1i \u01b0u h\u00f3a v\u00e0 lan truy\u1ec1n ng\u01b0\u1ee3c (backpropagation).</p>"}]}